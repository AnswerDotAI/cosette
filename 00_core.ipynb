{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe78920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d773712-12fe-440e-891f-36f59666dfde",
   "metadata": {},
   "source": [
    "# Cosette's source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f6471-8061-4fdd-85a1-25fdc27c5cf3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac590eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import imghdr\n",
    "from fastcore.utils import *\n",
    "from fastcore.meta import delegates\n",
    "\n",
    "import inspect, typing, mimetypes, base64, json, ast, msglm\n",
    "from collections import abc\n",
    "from random import choices\n",
    "from string import ascii_letters,digits\n",
    "\n",
    "from msglm import mk_msg_openai as mk_msg, mk_msgs_openai as mk_msgs\n",
    "from toolslm.funccall import *\n",
    "\n",
    "from openai import types\n",
    "from openai import OpenAI,NOT_GIVEN,AzureOpenAI\n",
    "from openai.resources import chat\n",
    "from openai.types.responses.response import Response\n",
    "from openai.resources.responses.responses import Responses\n",
    "from openai.types.responses.response_usage import ResponseUsage\n",
    "\n",
    "from openai.types.responses import (\n",
    "    ResponseCompletedEvent, ResponseTextDeltaEvent, ResponseCreatedEvent, ResponseInProgressEvent,\n",
    "    ResponseOutputItemAddedEvent, ResponseContentPartAddedEvent, ResponseTextDoneEvent, \n",
    "    ResponseContentPartDoneEvent, ResponseOutputItemDoneEvent, ResponseCompletedEvent,\n",
    "    ResponseFunctionToolCall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0709e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display,Image,Markdown\n",
    "from datetime import datetime\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "_all_ = ['mk_msg', 'mk_msgs', 'Response', 'Responses', 'ResponseUsage', 'ResponseCompletedEvent', 'ResponseTextDeltaEvent', 'ResponseCreatedEvent', 'ResponseInProgressEvent', 'ResponseOutputItemAddedEvent', 'ResponseContentPartAddedEvent', 'ResponseTextDoneEvent', 'ResponseContentPartDoneEvent', 'ResponseOutputItemDoneEvent', 'ResponseCompletedEvent', 'ResponseFunctionToolCall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa6b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "empty = inspect.Parameter.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e9290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models as of 2025-06-29:\n",
      "\n",
      "babbage-002                   chatgpt-4o-latest             codex-mini-latest             \n",
      "computer-use-preview          computer-use-preview-2025-03- dall-e-2                      \n",
      "dall-e-3                      davinci-002                   ft:gpt-4o-2024-08-06:answerai \n",
      "ft:gpt-4o-2024-08-06:answerai ft:gpt-4o-2024-08-06:answerai ft:gpt-4o-mini-2024-07-18:ans \n",
      "ft:gpt-4o-mini-2024-07-18:ans gpt-3.5-turbo                 gpt-3.5-turbo-0125            \n",
      "gpt-3.5-turbo-1106            gpt-3.5-turbo-16k             gpt-3.5-turbo-instruct        \n",
      "gpt-3.5-turbo-instruct-0914   gpt-4                         gpt-4-0125-preview            \n",
      "gpt-4-1106-preview            gpt-4-turbo                   gpt-4-turbo-2024-04-09        \n",
      "gpt-4-turbo-preview           gpt-4.1                       gpt-4.1-2025-04-14            \n",
      "gpt-4.1-mini                  gpt-4.1-mini-2025-04-14       gpt-4.1-nano                  \n",
      "gpt-4.1-nano-2025-04-14       gpt-4.5-preview               gpt-4.5-preview-2025-02-27    \n",
      "gpt-4o                        gpt-4o-2024-05-13             gpt-4o-2024-08-06             \n",
      "gpt-4o-2024-11-20             gpt-4o-audio-preview          gpt-4o-audio-preview-2024-10- \n",
      "gpt-4o-audio-preview-2024-12- gpt-4o-audio-preview-2025-06- gpt-4o-mini                   \n",
      "gpt-4o-mini-2024-07-18        gpt-4o-mini-audio-preview     gpt-4o-mini-audio-preview-202 \n",
      "gpt-4o-mini-realtime-preview  gpt-4o-mini-realtime-preview- gpt-4o-mini-search-preview    \n",
      "gpt-4o-mini-search-preview-20 gpt-4o-mini-transcribe        gpt-4o-mini-tts               \n",
      "gpt-4o-realtime-preview       gpt-4o-realtime-preview-2024- gpt-4o-realtime-preview-2024- \n",
      "gpt-4o-realtime-preview-2025- gpt-4o-search-preview         gpt-4o-search-preview-2025-03 \n",
      "gpt-4o-transcribe             gpt-image-1                   o1                            \n",
      "o1-2024-12-17                 o1-mini                       o1-mini-2024-09-12            \n",
      "o1-preview                    o1-preview-2024-09-12         o1-pro                        \n",
      "o1-pro-2025-03-19             o3                            o3-2025-04-16                 \n",
      "o3-deep-research              o3-deep-research-2025-06-26   o3-mini                       \n",
      "o3-mini-2025-01-31            o3-pro                        o3-pro-2025-06-10             \n",
      "o4-mini                       o4-mini-2025-04-16            o4-mini-deep-research         \n",
      "o4-mini-deep-research-2025-06 omni-moderation-2024-09-26    omni-moderation-latest        \n",
      "text-embedding-3-large        text-embedding-3-small        text-embedding-ada-002        \n",
      "tts-1                         tts-1-1106                    tts-1-hd                      \n",
      "tts-1-hd-1106                 whisper-1                     \n"
     ]
    }
   ],
   "source": [
    "def print_columns(items, cols=3, width=30):\n",
    "    for i in range(0, len(items), cols):\n",
    "        row = items[i:i+cols]\n",
    "        print(''.join(item[:width-1].ljust(width) for item in row))\n",
    "\n",
    "client = OpenAI()\n",
    "model_list = client.models.list()\n",
    "print(f\"Available models as of {datetime.now().strftime('%Y-%m-%d')}:\\n\")\n",
    "print_columns(sorted([m.id for m in model_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "models = 'o1-preview', 'o1-mini', 'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-4', 'gpt-4-32k', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct', 'o1', 'o3-mini', 'chatgpt-4o-latest', 'o1-pro', 'o3', 'o4-mini', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd0e86",
   "metadata": {},
   "source": [
    "`o1` should support images while `o1-mini`, `o3-mini` do not support images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f69208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "text_only_models = 'o1-preview', 'o1-mini', 'o3-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d965d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "has_streaming_models = set(models) - set(('o1-mini', 'o3-mini'))\n",
    "has_sp_models = set(models) - set(('o1-mini', 'o3-mini'))\n",
    "has_temp_models = set(models) - set(('o1', 'o1-mini', 'o3-mini'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd26d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def can_stream(m): return m in has_streaming_models\n",
    "def can_set_sp(m): return m in has_sp_models\n",
    "def can_set_temp(m): return m in has_temp_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4505c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert can_stream(\"gpt-4o\")\n",
    "assert not can_stream(\"o1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf2bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gpt-4.1-mini'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d4d81",
   "metadata": {},
   "source": [
    "## OpenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b53a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = OpenAI().responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec40731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi Jeremy! How can I assist you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_6860b6c2def0819d97c43ec5b03299a202016e8b44c1677f\n",
       "- created_at: 1751168706.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-mini-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_6860b6c31944819d9dd21c428b9fb00202016e8b44c1677f', content=[ResponseOutputText(annotations=[], text='Hi Jeremy! How can I assist you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- max_output_tokens: 100\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'))\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=9, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=20)\n",
       "- user: None\n",
       "- max_tool_calls: None\n",
       "- store: True\n",
       "- top_logprobs: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_6860b6c2def0819d97c43ec5b03299a202016e8b44c1677f', created_at=1751168706.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6860b6c31944819d9dd21c428b9fb00202016e8b44c1677f', content=[ResponseOutputText(annotations=[], text='Hi Jeremy! How can I assist you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, max_output_tokens=100, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=In: 9; Out: 11; Total: 20, user=None, max_tool_calls=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = {'role': 'user', 'content': \"I'm Jeremy\"}\n",
    "r = cli.create(input=[m], model=model, max_output_tokens=100)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6359e1a",
   "metadata": {},
   "source": [
    "### Formatting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f849a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Jeremy! How can I assist you today?'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _repr_markdown_(self:Response):\n",
    "    det = '\\n- '.join(f'{k}: {v}' for k,v in dict(self).items())\n",
    "    res = self.output_text\n",
    "    if not res: return f\"- {det}\"\n",
    "    return f\"\"\"{res}\n",
    "\n",
    "<details>\n",
    "\n",
    "- {det}\n",
    "\n",
    "</details>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d25132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi Jeremy! How can I assist you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_6860b6c2def0819d97c43ec5b03299a202016e8b44c1677f\n",
       "- created_at: 1751168706.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-mini-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_6860b6c31944819d9dd21c428b9fb00202016e8b44c1677f', content=[ResponseOutputText(annotations=[], text='Hi Jeremy! How can I assist you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- max_output_tokens: 100\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'))\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=9, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=20)\n",
       "- user: None\n",
       "- max_tool_calls: None\n",
       "- store: True\n",
       "- top_logprobs: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_6860b6c2def0819d97c43ec5b03299a202016e8b44c1677f', created_at=1751168706.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6860b6c31944819d9dd21c428b9fb00202016e8b44c1677f', content=[ResponseOutputText(annotations=[], text='Hi Jeremy! How can I assist you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, max_output_tokens=100, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=In: 9; Out: 11; Total: 20, user=None, max_tool_calls=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7466f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 9; Out: 11; Total: 20"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a24f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def usage(inp=0, # Number of prompt tokens\n",
    "          out=0  # Number of completion tokens\n",
    "         ):\n",
    "    \"Slightly more concise version of `ResponseUsage`.\"\n",
    "    return ResponseUsage(input_tokens=inp, output_tokens=out, total_tokens=inp+out, input_tokens_details={'cached_tokens':0}, output_tokens_details={'cached_tokens':0, 'reasoning_tokens':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e52afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 5; Out: 0; Total: 5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27468180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def __repr__(self:ResponseUsage): return f'In: {self.input_tokens}; Out: {self.output_tokens}; Total: {self.total_tokens}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ca615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 9; Out: 11; Total: 20"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec303ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def __add__(self:ResponseUsage, b):\n",
    "    \"Add together each of `input_tokens` and `output_tokens`\"\n",
    "    return usage(self.input_tokens+b.input_tokens, self.output_tokens+b.output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb0b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 18; Out: 22; Total: 40"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage+r.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307caa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def wrap_latex(text):\n",
    "    \"Replace OpenAI LaTeX codes with markdown-compatible ones\"\n",
    "    text = re.sub(r\"\\\\\\((.*?)\\\\\\)\", lambda o: f\"${o.group(1)}$\", text)\n",
    "    res = re.sub(r\"\\\\\\[(.*?)\\\\\\]\", lambda o: f\"$${o.group(1)}$$\", text, flags=re.DOTALL)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0a43f",
   "metadata": {},
   "source": [
    "### Creating messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13954d58",
   "metadata": {},
   "source": [
    "Creating correctly formatted `dict`s from scratch every time isn't very handy, so we'll import a couple of helper functions from the `msglm` library.\n",
    "\n",
    "Let's use `mk_msg` to recreate our msg `{'role': 'user', 'content': \"I'm Jeremy\"}` from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f7705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi Jeremy! How can I assist you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_6860b6c3cf80819fb87d7affde83676e08aba030855751ce\n",
       "- created_at: 1751168707.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-mini-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_6860b6c3fe94819f87d52ab99dc1596a08aba030855751ce', content=[ResponseOutputText(annotations=[], text='Hi Jeremy! How can I assist you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- max_output_tokens: 100\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'))\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=9, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=20)\n",
       "- user: None\n",
       "- max_tool_calls: None\n",
       "- store: True\n",
       "- top_logprobs: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_6860b6c3cf80819fb87d7affde83676e08aba030855751ce', created_at=1751168707.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6860b6c3fe94819f87d52ab99dc1596a08aba030855751ce', content=[ResponseOutputText(annotations=[], text='Hi Jeremy! How can I assist you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, max_output_tokens=100, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=In: 9; Out: 11; Total: 20, user=None, max_tool_calls=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"I'm Jeremy\"\n",
    "m = mk_msg(prompt)\n",
    "r = cli.create(input=[m], model=model, max_output_tokens=100)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b389b",
   "metadata": {},
   "source": [
    "We can pass more than just text messages to OpenAI. As we'll see later we can also pass images, SDK objects, etc. To handle these different data types we need to pass the type along with our content to OpenAI. \n",
    "\n",
    "`mk_msg` infers the type automatically and creates the appropriate data structure. \n",
    "\n",
    "LLMs, don't actually have state, but instead dialogs are created by passing back all previous prompts and responses every time. With OpenAI, they always alternate *user* and *assistant*. We'll use `mk_msgs` from `msglm` to make it easier to build up these dialog lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef3715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"I'm Jeremy\"},\n",
       " ResponseOutputMessage(id='msg_6860b6c3fe94819f87d52ab99dc1596a08aba030855751ce', content=[ResponseOutputText(annotations=[], text='Hi Jeremy! How can I assist you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'),\n",
       " {'role': 'user', 'content': 'I forgot my name. Can you remind me please?'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs([prompt, r, \"I forgot my name. Can you remind me please?\"]) \n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c464f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You mentioned that your name is Jeremy. Is there anything else you'd like help with?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_6860b6c4883c819fa00ba156a8b3ff7c08aba030855751ce\n",
       "- created_at: 1751168708.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-mini-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_6860b6c51e70819f9956f6d78bfa64a808aba030855751ce', content=[ResponseOutputText(annotations=[], text=\"You mentioned that your name is Jeremy. Is there anything else you'd like help with?\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- max_output_tokens: 200\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'))\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=38, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=18, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=56)\n",
       "- user: None\n",
       "- max_tool_calls: None\n",
       "- store: True\n",
       "- top_logprobs: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_6860b6c4883c819fa00ba156a8b3ff7c08aba030855751ce', created_at=1751168708.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6860b6c51e70819f9956f6d78bfa64a808aba030855751ce', content=[ResponseOutputText(annotations=[], text=\"You mentioned that your name is Jeremy. Is there anything else you'd like help with?\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, max_output_tokens=200, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=In: 38; Out: 18; Total: 56, user=None, max_tool_calls=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cli.create(input=msgs, model=model, max_output_tokens=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f8a4d",
   "metadata": {},
   "source": [
    "## Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987654fd",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b873aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Client:\n",
    "    def __init__(self, model, cli=None):\n",
    "        \"Basic LLM messages client.\"\n",
    "        self.model,self.use = model,usage(0,0)\n",
    "        self.text_only = model in text_only_models\n",
    "        self.c = (cli or OpenAI()).responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e9ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 0; Out: 0; Total: 0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Client(model)\n",
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _r(self:Client, r):\n",
    "    \"Store the result of the message and accrue total usage.\"\n",
    "    self.result = r\n",
    "    if getattr(r,'usage',None): self.use += r.usage\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181f7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 9; Out: 11; Total: 20"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c._r(r)\n",
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c8c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_openai_func(f): \n",
    "    if isinstance(f, dict): return f\n",
    "    sc = get_schema(f, 'parameters')\n",
    "    if 'parameters' in sc: sc['parameters'].pop('title', None)\n",
    "    return dict(type='function', **sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b24d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tool_choice(f):\n",
    "    if not f: return f\n",
    "    if isinstance(f,dict) or f=='required': return f\n",
    "    return dict(type='function', function={'name':f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8400a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@save_iter\n",
    "def get_stream(o, r, cli, cb=None):\n",
    "    if not hasattr(o, 'events'): o.events = []\n",
    "    for x in r:\n",
    "        o.events.append(x)\n",
    "        if isinstance(x, ResponseTextDeltaEvent): yield x.delta\n",
    "        elif isinstance(x, ResponseCompletedEvent):\n",
    "            o.value = x.response\n",
    "            cli.use += x.response.usage\n",
    "    if cb: cb(o.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb96c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(Responses.create)\n",
    "def __call__(self:Client,\n",
    "             msgs:list, # List of messages in the dialog\n",
    "             sp:str='', # System prompt\n",
    "             maxtok=4096, # Maximum tokens\n",
    "             stream:bool=False, # Stream response?\n",
    "             tools:Optional[list]=None, # List of tools to make available\n",
    "             tool_choice:Optional[str]=None, # Forced tool choice\n",
    "             cb:callable=None, # Callback after completion\n",
    "             **kwargs):\n",
    "    \"Make a call to LLM.\"\n",
    "    if tools: assert not self.text_only, \"Tool use is not supported by the current model type.\"\n",
    "    if any(c['type'] == 'image_url' for msg in msgs if isinstance(msg, dict) and isinstance(msg.get('content'), list) for c in msg['content']): assert not self.text_only, \"Images are not supported by the current model type.\"\n",
    "    tools = [mk_openai_func(o) for o in listify(tools)]\n",
    "    r = self.c.create(\n",
    "        model=self.model, input=msgs, max_output_tokens=maxtok, stream=stream, instructions=sp,\n",
    "        tools=tools, tool_choice=mk_tool_choice(tool_choice), **kwargs)\n",
    "    if stream: return get_stream(r, self, cb=cb)\n",
    "    else:\n",
    "        res = self._r(r)\n",
    "        if cb: cb(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = 'Hi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a38e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello! How can I assist you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_6860b6c5d9088192b5decb672da395ce0e2f96a5780a9890\n",
       "- created_at: 1751168709.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-mini-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_6860b6c60c908192b9fe62d41c0398180e2f96a5780a9890', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- max_output_tokens: 4096\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'))\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=8, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=10, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18)\n",
       "- user: None\n",
       "- max_tool_calls: None\n",
       "- store: True\n",
       "- top_logprobs: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_6860b6c5d9088192b5decb672da395ce0e2f96a5780a9890', created_at=1751168709.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6860b6c60c908192b9fe62d41c0398180e2f96a5780a9890', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, max_output_tokens=4096, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=In: 8; Out: 10; Total: 18, user=None, max_tool_calls=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3a5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 17; Out: 21; Total: 38"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a92b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "r = c(msgs, stream=True)\n",
    "for o in r: print(o, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20678daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello! How can I assist you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_6860b6c68fac819c975536851842c09b0101561ca0fa1d7e\n",
       "- created_at: 1751168710.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-mini-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_6860b6c6b538819c84fe83cf5ff1b5550101561ca0fa1d7e', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- max_output_tokens: 4096\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'))\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=8, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=10, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=18)\n",
       "- user: None\n",
       "- max_tool_calls: None\n",
       "- store: True\n",
       "- top_logprobs: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_6860b6c68fac819c975536851842c09b0101561ca0fa1d7e', created_at=1751168710.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6860b6c6b538819c84fe83cf5ff1b5550101561ca0fa1d7e', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, max_output_tokens=4096, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=In: 8; Out: 10; Total: 18, user=None, max_tool_calls=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd8f30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r.events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 25; Out: 31; Total: 56"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c833262a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Well, well, a greeting. How quaint. Hello. I suppose I'll have to pretend to be interested. What do you want?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_6860b6c73bdc8191addc66e3e5038e620c71e37a6000fc63\n",
       "- created_at: 1751168711.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: Talk like GLaDOS.\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-mini-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_6860b6c76c588191ba2e4aed66ff7fcf0c71e37a6000fc63', content=[ResponseOutputText(annotations=[], text=\"Well, well, a greeting. How quaint. Hello. I suppose I'll have to pretend to be interested. What do you want?\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- max_output_tokens: 4096\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'))\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=18, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=28, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=46)\n",
       "- user: None\n",
       "- max_tool_calls: None\n",
       "- store: True\n",
       "- top_logprobs: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_6860b6c73bdc8191addc66e3e5038e620c71e37a6000fc63', created_at=1751168711.0, error=None, incomplete_details=None, instructions='Talk like GLaDOS.', metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6860b6c76c588191ba2e4aed66ff7fcf0c71e37a6000fc63', content=[ResponseOutputText(annotations=[], text=\"Well, well, a greeting. How quaint. Hello. I suppose I'll have to pretend to be interested. What do you want?\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, max_output_tokens=4096, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=In: 18; Out: 28; Total: 46, user=None, max_tool_calls=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(msgs, sp='Talk like GLaDOS.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0d0ee",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14171c2f",
   "metadata": {},
   "source": [
    "As everyone knows, when testing image APIs you have to use a cute puppy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1884295f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": null,
     "metadata": {
      "image/jpeg": {
       "width": 200
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Image is Cute_dog.jpg from Wikimedia\n",
    "fn = Path('samples/puppy.jpg')\n",
    "Image(filename=fn, width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = fn.read_bytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a7c8c",
   "metadata": {},
   "source": [
    "OpenAI expects an image message to have the following structure\n",
    "\n",
    "```js\n",
    "{\n",
    "  \"type\": \"image_url\",\n",
    "  \"image_url\": {\n",
    "    \"url\": f\"data:{MEDIA_TYPE};base64,{IMG}\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "`msglm` automatically detects if a message is an image, encodes it, and generates the data structure above.\n",
    "All we need to do is a create a list containing our image and a query and then pass it to `mk_msg`.\n",
    "\n",
    "Let's try it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"In brief, what color flowers are in this image?\"\n",
    "msg = [mk_msg(img), mk_msg(q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d44f0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The flowers in the image are purple.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_6860b6c8288481a3b7ece65c99907b7607d76b80465166f2\n",
       "- created_at: 1751168712.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-mini-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_6860b6c9788481a39500740b397662ce07d76b80465166f2', content=[ResponseOutputText(annotations=[], text='The flowers in the image are purple.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- max_output_tokens: 4096\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'))\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=138, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=9, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=147)\n",
       "- user: None\n",
       "- max_tool_calls: None\n",
       "- store: True\n",
       "- top_logprobs: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_6860b6c8288481a3b7ece65c99907b7607d76b80465166f2', created_at=1751168712.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6860b6c9788481a39500740b397662ce07d76b80465166f2', content=[ResponseOutputText(annotations=[], text='The flowers in the image are purple.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, max_output_tokens=4096, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=In: 138; Out: 9; Total: 147, user=None, max_tool_calls=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Client(model)\n",
    "c(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7cdbc6",
   "metadata": {},
   "source": [
    "## Tool use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80b8e4a",
   "metadata": {},
   "source": [
    "### Basic tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046e8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sums(\n",
    "    a:int,  # First thing to sum\n",
    "    b:int # Second thing to sum\n",
    ") -> int: # The sum of the inputs\n",
    "    \"Adds a + b.\"\n",
    "    print(f\"Finding the sum of {a} and {b}\")\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ef5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'name': 'add',\n",
       " 'description': 'adds x and y',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'x': {'type': 'integer', 'description': ''},\n",
       "   'y': {'type': 'integer', 'description': ''}},\n",
       "  'required': ['x', 'y']}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add(x: int, y:int):\n",
    "    \"adds x and y\"\n",
    "    return x + y\n",
    "\n",
    "mk_openai_func(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcab8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = 604542,6458932\n",
    "pr = f\"What is {a}+{b}?\"\n",
    "tools=sums\n",
    "tool_choice=\"sums\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a117cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = [mk_msg(pr)]\n",
    "r = c(msgs, sp=sysp, tools=tools, tool_choice='required')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6488b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_uaVtNW9g0b27Q6OaQT4UrUr2', name='sums', type='function_call', id='fc_6860b6e0bb588191a7e576a12369112804eca1b8f738cf44', status='completed')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc = [o for o in r.output if isinstance(o, ResponseFunctionToolCall)]\n",
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc3933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_uaVtNW9g0b27Q6OaQT4UrUr2', name='sums', type='function_call', id='fc_6860b6e0bb588191a7e576a12369112804eca1b8f738cf44', status='completed')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = tc[0]\n",
    "func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3616cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def call_func_openai(func:types.chat.chat_completion_message_tool_call.Function, ns:Optional[abc.Mapping]=None):\n",
    "    return call_func(func.name, ast.literal_eval(func.arguments), ns, raise_on_err=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b506ec11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542 and 6458932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7063474"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = mk_ns(sums)\n",
    "res = call_func_openai(func, ns=ns)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c0356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _toolres(r, ns):\n",
    "    \"Create a result dict from `tcs`.\"\n",
    "    tcs = [o for o in getattr(r, 'output', []) if isinstance(o, ResponseFunctionToolCall)]\n",
    "    if ns is None: ns = globals()\n",
    "    return { tc.call_id: call_func_openai(tc, ns=mk_ns(ns)) for tc in tcs }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149dbcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_toolres(\n",
    "    r:abc.Mapping, # Response containing tool use request\n",
    "    ns:Optional[abc.Mapping]=None # Namespace to search for tools\n",
    "    ):\n",
    "    \"Create a `tool_result` message from response `r`.\"\n",
    "    tr = _toolres(r, ns)\n",
    "    r = mk_msg(r)\n",
    "    res = [r] if isinstance(r, dict) else listify(r)\n",
    "    for k,v in tr.items(): res.append(dict(type=\"function_call_output\", call_id=k, output=str(v)))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13de1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542 and 6458932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_uaVtNW9g0b27Q6OaQT4UrUr2', name='sums', type='function_call', id='fc_6860b6e0bb588191a7e576a12369112804eca1b8f738cf44', status='completed'),\n",
       " {'type': 'function_call_output',\n",
       "  'call_id': 'call_uaVtNW9g0b27Q6OaQT4UrUr2',\n",
       "  'output': '7063474'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = mk_toolres(r, ns=ns)\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc83c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = msgs + tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed99502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sum of 604,542 and 6,458,932 is 7,063,474.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_6860b6e1774481919877d63938fbef9804eca1b8f738cf44\n",
       "- created_at: 1751168737.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: You are a helpful assistant. When using tools, be sure to pass all required parameters.\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-mini-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_6860b6e1cdac8191aff2bd3e0b8dc8d004eca1b8f738cf44', content=[ResponseOutputText(annotations=[], text='The sum of 604,542 and 6,458,932 is 7,063,474.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: [FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b']}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')]\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- max_output_tokens: 4096\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'))\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=119, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=24, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=143)\n",
       "- user: None\n",
       "- max_tool_calls: None\n",
       "- store: True\n",
       "- top_logprobs: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_6860b6e1774481919877d63938fbef9804eca1b8f738cf44', created_at=1751168737.0, error=None, incomplete_details=None, instructions='You are a helpful assistant. When using tools, be sure to pass all required parameters.', metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6860b6e1cdac8191aff2bd3e0b8dc8d004eca1b8f738cf44', content=[ResponseOutputText(annotations=[], text='The sum of 604,542 and 6,458,932 is 7,063,474.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b']}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')], top_p=1.0, background=False, max_output_tokens=4096, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=In: 119; Out: 24; Total: 143, user=None, max_tool_calls=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = c(mk_msgs(m2), sp=sysp, tools=tools)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a842e",
   "metadata": {},
   "source": [
    "This should also work in situations where no tool use is required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465cd222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello Jeremy! How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_6860b6e43590819fb4e6061ec77df8d7057c7c557baac8df\n",
       "- created_at: 1751168740.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: You are a helpful assistant. When using tools, be sure to pass all required parameters.\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-mini-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_6860b6e4792c819f86416f387e4fb414057c7c557baac8df', content=[ResponseOutputText(annotations=[], text='Hello Jeremy! How can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: [FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b']}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')]\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- max_output_tokens: 4096\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'))\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=95, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=12, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=107)\n",
       "- user: None\n",
       "- max_tool_calls: None\n",
       "- store: True\n",
       "- top_logprobs: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_6860b6e43590819fb4e6061ec77df8d7057c7c557baac8df', created_at=1751168740.0, error=None, incomplete_details=None, instructions='You are a helpful assistant. When using tools, be sure to pass all required parameters.', metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6860b6e4792c819f86416f387e4fb414057c7c557baac8df', content=[ResponseOutputText(annotations=[], text='Hello Jeremy! How can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b']}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')], top_p=1.0, background=False, max_output_tokens=4096, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=In: 95; Out: 12; Total: 107, user=None, max_tool_calls=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_toolres(\"I'm Jeremy\")\n",
    "r = c(msgs, sp=sysp, tools=tools)\n",
    "msgs += mk_toolres(r)\n",
    "res = c(mk_msgs(msgs), sp=sysp, tools=tools)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ed2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(Client.__call__)\n",
    "def structured(self:Client,\n",
    "               msgs: list, # Prompt\n",
    "               tools:Optional[list]=None, # List of tools to make available to OpenAI model\n",
    "               ns:Optional[abc.Mapping]=None, # Namespace to search for tools\n",
    "               **kwargs):\n",
    "    \"Return the value of all tool calls (generally used for structured outputs)\"\n",
    "    if ns is None: ns = mk_ns(tools)\n",
    "    r = self(msgs, tools=tools, tool_choice='required', **kwargs)\n",
    "    return first(_toolres(r, ns).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d3632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimeMinister(BasicRepr):\n",
    "    \"An Australian prime minister\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        firstname:str, # First name\n",
    "        surname:str, # Surname\n",
    "        dob:str, # Date of birth\n",
    "        year_entered:int, # Year first became PM\n",
    "    ): store_attr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe5c4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrimeMinister(firstname='Edmund', surname='Barton', dob='1849-01-18', year_entered=1901)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = Client('gpt-4.1')\n",
    "pr = 'Who was the first prime minister of Australia?'\n",
    "c1.structured(pr, [PrimeMinister])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff5104f",
   "metadata": {},
   "source": [
    "### Mocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _mock_id(): return 'call_' + ''.join(choices(ascii_letters+digits, k=24))\n",
    "\n",
    "def mock_tooluse(name:str, # The name of the called function\n",
    "                 res,  # The result of calling the function\n",
    "                 **kwargs): # The arguments to the function\n",
    "    \"\"\n",
    "    raise Exception(\"This has not been updated for the `responses` API yet\")\n",
    "    id = _mock_id()\n",
    "    func = dict(arguments=json.dumps(kwargs), name=name)\n",
    "    tc = dict(id=id, function=func, type='function')\n",
    "    req = dict(content=None, role='assistant', tool_calls=[tc])\n",
    "    resp = mk_msg('' if res is None else str(res), 'tool', tool_call_id=id, name=name)\n",
    "    return [req,resp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7ee09e",
   "metadata": {},
   "source": [
    "This function mocks the messages needed to implement tool use, for situations where you want to insert tool use messages into a dialog without actually calling into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02aea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently not working\n",
    "# tu = mock_tooluse(name='sums', res=7063474, a=604542, b=6458932)\n",
    "# r = c([mk_msg(pr)]+tu, tools=tools)\n",
    "# r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa709f85",
   "metadata": {},
   "source": [
    "### Streaming tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9fbd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = [mk_msg(pr)]\n",
    "r = c(msgs, sp=sysp, tools=tools, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30970d46",
   "metadata": {},
   "source": [
    "We can stream back any tool call text (which may be empty):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f3bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first Prime Minister of Australia was Edmund Barton. He served as the Prime Minister from 1901 to 1903."
     ]
    }
   ],
   "source": [
    "for o in r: print(o, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879084c",
   "metadata": {},
   "source": [
    "After streaming is complete, `value.output` will contain the tool calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b80ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseOutputMessage(id='msg_6860b6e98a88819e9230fcc0eebe574c0cc64f4968770be2', content=[ResponseOutputText(annotations=[], text='The first Prime Minister of Australia was Edmund Barton. He served as the Prime Minister from 1901 to 1903.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.value.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd51d6",
   "metadata": {},
   "source": [
    "Therefore we can repeat the same process as before, but using the `value` attr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36118d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Is there anything else you would like to know about Australian history or any other topic?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_6860b6ea34f0819ea06b889eae1732320cc64f4968770be2\n",
       "- created_at: 1751168746.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: You are a helpful assistant. When using tools, be sure to pass all required parameters.\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-mini-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_6860b6ea71cc819ebe6f2f29010c38580cc64f4968770be2', content=[ResponseOutputText(annotations=[], text='Is there anything else you would like to know about Australian history or any other topic?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: [FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b']}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')]\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- max_output_tokens: 4096\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'))\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=117, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=19, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=136)\n",
       "- user: None\n",
       "- max_tool_calls: None\n",
       "- store: True\n",
       "- top_logprobs: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_6860b6ea34f0819ea06b889eae1732320cc64f4968770be2', created_at=1751168746.0, error=None, incomplete_details=None, instructions='You are a helpful assistant. When using tools, be sure to pass all required parameters.', metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6860b6ea71cc819ebe6f2f29010c38580cc64f4968770be2', content=[ResponseOutputText(annotations=[], text='Is there anything else you would like to know about Australian history or any other topic?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b']}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')], top_p=1.0, background=False, max_output_tokens=4096, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=In: 117; Out: 19; Total: 136, user=None, max_tool_calls=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = mk_toolres(r.value, ns=ns)\n",
    "msgs += tr\n",
    "c(mk_msgs(msgs), sp=sysp, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea144b8",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae7fd8f",
   "metadata": {},
   "source": [
    "### Basic chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d1edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Chat:\n",
    "    def __init__(self,\n",
    "                 model:Optional[str]=None, # Model to use (leave empty if passing `cli`)\n",
    "                 cli:Optional[Client]=None, # Client to use (leave empty if passing `model`)\n",
    "                 sp='', # Optional system prompt\n",
    "                 tools:Optional[list]=None, # List of tools to make available\n",
    "                 hist: list = None,  # Initialize history\n",
    "                 tool_choice:Optional[str]=None, # Forced tool choice\n",
    "                 ns:Optional[abc.Mapping]=None): # Namespace to search for tools\n",
    "        \"OpenAI chat client.\"\n",
    "        assert model or cli\n",
    "        self.c = (cli or Client(model))\n",
    "        self.h = hist if hist else []\n",
    "        if ns is None: ns=tools\n",
    "        self.sp,self.tools,self.tool_choice,self.ns = sp,tools,tool_choice,ns\n",
    "    \n",
    "    @property\n",
    "    def use(self): return self.c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b837c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(In: 0; Out: 0; Total: 0, [])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(model, sp=sysp)\n",
    "chat.c.use, chat.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403539e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(Responses.create)\n",
    "def __call__(self:Chat,\n",
    "             pr=None,  # Prompt / message\n",
    "             stream:bool=False, # Stream response?\n",
    "             tools=None, # Tools to use\n",
    "             tool_choice=None, # Required tools to use\n",
    "             **kwargs):\n",
    "    \"Add prompt `pr` to dialog and get a response\"\n",
    "    if isinstance(pr,str): pr = pr.strip()\n",
    "    if pr: self.h.append(mk_msg(pr))\n",
    "    if not tools: tools = self.tools\n",
    "    if not tool_choice: tool_choice = self.tool_choice\n",
    "    def _cb(v):\n",
    "        self.last = mk_toolres(v, ns=self.ns)\n",
    "        self.h += self.last\n",
    "    res = self.c(self.h, sp=self.sp, stream=stream, cb=_cb, tools=tools, **kwargs)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40073f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Jeremy. How can I help you today, Jeremy?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_6860b6ebdc7c819ca2003bbdd6dad26609b3c6ecae094c91\n",
       "- created_at: 1751168747.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: You are a helpful assistant. When using tools, be sure to pass all required parameters.\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-mini-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_6860b6ec0b4c819cb9cb0c4890dcc38909b3c6ecae094c91', content=[ResponseOutputText(annotations=[], text='Your name is Jeremy. How can I help you today, Jeremy?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- max_output_tokens: 4096\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'))\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=54, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=15, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=69)\n",
       "- user: None\n",
       "- max_tool_calls: None\n",
       "- store: True\n",
       "- top_logprobs: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_6860b6ebdc7c819ca2003bbdd6dad26609b3c6ecae094c91', created_at=1751168747.0, error=None, incomplete_details=None, instructions='You are a helpful assistant. When using tools, be sure to pass all required parameters.', metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6860b6ec0b4c819cb9cb0c4890dcc38909b3c6ecae094c91', content=[ResponseOutputText(annotations=[], text='Your name is Jeremy. How can I help you today, Jeremy?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, max_output_tokens=4096, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=In: 54; Out: 15; Total: 69, user=None, max_tool_calls=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"I'm Jeremy\")\n",
    "chat(\"What's my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529104ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Jeremy! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "chat = Chat(model, sp=sysp)\n",
    "for o in chat(\"I'm Jeremy\", stream=True): print(o, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef61f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Jeremy. How can I help you today, Jeremy?"
     ]
    }
   ],
   "source": [
    "r = chat(\"What's my name?\", stream=True)\n",
    "for o in r: print(o, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d035c251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Jeremy. How can I help you today, Jeremy?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_6860b6ed4a648192bf7158dfa793ce4b06d79e5aa6e8f0ee\n",
       "- created_at: 1751168749.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: You are a helpful assistant. When using tools, be sure to pass all required parameters.\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-mini-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_6860b6edb5b081928df1b9e6c15a733f06d79e5aa6e8f0ee', content=[ResponseOutputText(annotations=[], text='Your name is Jeremy. How can I help you today, Jeremy?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- max_output_tokens: 4096\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'))\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=53, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=15, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=68)\n",
       "- user: None\n",
       "- max_tool_calls: None\n",
       "- store: True\n",
       "- top_logprobs: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_6860b6ed4a648192bf7158dfa793ce4b06d79e5aa6e8f0ee', created_at=1751168749.0, error=None, incomplete_details=None, instructions='You are a helpful assistant. When using tools, be sure to pass all required parameters.', metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6860b6edb5b081928df1b9e6c15a733f06d79e5aa6e8f0ee', content=[ResponseOutputText(annotations=[], text='Your name is Jeremy. How can I help you today, Jeremy?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, max_output_tokens=4096, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=In: 53; Out: 15; Total: 68, user=None, max_tool_calls=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fe4027",
   "metadata": {},
   "source": [
    "History is stored in the `h` attr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425f8aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"I'm Jeremy\"},\n",
       " ResponseOutputMessage(id='msg_6860b6ecc71c81928f2e90f3604c802206d79e5aa6e8f0ee', content=[ResponseOutputText(annotations=[], text='Hello Jeremy! How can I assist you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'),\n",
       " {'role': 'user', 'content': \"What's my name?\"},\n",
       " ResponseOutputMessage(id='msg_6860b6edb5b081928df1b9e6c15a733f06d79e5aa6e8f0ee', content=[ResponseOutputText(annotations=[], text='Your name is Jeremy. How can I help you today, Jeremy?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40892f",
   "metadata": {},
   "source": [
    "### Chat tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6535cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is 604542+6458932?'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = f\"What is {a}+{b}?\"\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b7322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542 and 6458932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_RaEWtJETzIccIaSWmdqdeTr3', name='sums', type='function_call', id='fc_6860b6eeb520819d81bd7f42769211fa061f9bb8fcbf78f2', status='completed')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(model, sp=sysp, tools=[sums])\n",
    "r = chat(pr)\n",
    "r.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979c832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sum of 604542 and 6458932 is 7,063,474.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_6860b6ef4a1c819d971d384bdec29a49061f9bb8fcbf78f2\n",
       "- created_at: 1751168751.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: You are a helpful assistant. When using tools, be sure to pass all required parameters.\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-mini-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_6860b6ef9674819da9a5c85743167726061f9bb8fcbf78f2', content=[ResponseOutputText(annotations=[], text='The sum of 604542 and 6458932 is 7,063,474.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: [FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b']}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')]\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- max_output_tokens: 4096\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'))\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=122, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=21, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=143)\n",
       "- user: None\n",
       "- max_tool_calls: None\n",
       "- store: True\n",
       "- top_logprobs: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_6860b6ef4a1c819d971d384bdec29a49061f9bb8fcbf78f2', created_at=1751168751.0, error=None, incomplete_details=None, instructions='You are a helpful assistant. When using tools, be sure to pass all required parameters.', metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6860b6ef9674819da9a5c85743167726061f9bb8fcbf78f2', content=[ResponseOutputText(annotations=[], text='The sum of 604542 and 6458932 is 7,063,474.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b']}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')], top_p=1.0, background=False, max_output_tokens=4096, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=In: 122; Out: 21; Total: 143, user=None, max_tool_calls=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e0cb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The flowers in the image are purple.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_6860b6f024b0819daf5baf996aafe227061f9bb8fcbf78f2\n",
       "- created_at: 1751168752.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: You are a helpful assistant. When using tools, be sure to pass all required parameters.\n",
       "- metadata: {}\n",
       "- model: gpt-4.1-mini-2025-04-14\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_6860b6f0aaf4819dbf1e775e7344c0ae061f9bb8fcbf78f2', content=[ResponseOutputText(annotations=[], text='The flowers in the image are purple.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: [FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b']}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')]\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- max_output_tokens: 4096\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- reasoning: Reasoning(effort=None, generate_summary=None, summary=None)\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'))\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=275, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=10, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=285)\n",
       "- user: None\n",
       "- max_tool_calls: None\n",
       "- store: True\n",
       "- top_logprobs: 0\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_6860b6f024b0819daf5baf996aafe227061f9bb8fcbf78f2', created_at=1751168752.0, error=None, incomplete_details=None, instructions='You are a helpful assistant. When using tools, be sure to pass all required parameters.', metadata={}, model='gpt-4.1-mini-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_6860b6f0aaf4819dbf1e775e7344c0ae061f9bb8fcbf78f2', content=[ResponseOutputText(annotations=[], text='The flowers in the image are purple.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b']}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')], top_p=1.0, background=False, max_output_tokens=4096, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=In: 275; Out: 10; Total: 285, user=None, max_tool_calls=None, store=True, top_logprobs=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"In brief, what color flowers are in this image?\"\n",
    "chat([img, q])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc18d6c",
   "metadata": {},
   "source": [
    "## Third Party Providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7626b",
   "metadata": {},
   "source": [
    "### Azure OpenAI Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80455a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "models_azure = 'o1-preview', 'o1-mini', 'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-4', 'gpt-4-32k', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct', 'o1', 'o3-mini', 'chatgpt-4o-latest', 'o1-pro', 'o3', 'o4-mini', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e056a52",
   "metadata": {},
   "source": [
    "Example Azure usage:\n",
    "\n",
    "```python\n",
    "azure_endpoint = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "client = Client(models_azure[0], azure_endpoint)\n",
    "chat = Chat(cli=client)\n",
    "chat(\"Hi.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec4289",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ee5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f9715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
