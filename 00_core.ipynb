{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe78920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d773712-12fe-440e-891f-36f59666dfde",
   "metadata": {},
   "source": [
    "# Cosette's source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f6471-8061-4fdd-85a1-25fdc27c5cf3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac590eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import imghdr\n",
    "from fastcore.utils import *\n",
    "from fastcore.meta import delegates\n",
    "\n",
    "import inspect, typing, mimetypes, base64, json, msglm\n",
    "from collections import abc\n",
    "from random import choices\n",
    "from string import ascii_letters,digits\n",
    "\n",
    "from msglm import mk_msg_openai as mk_msg, mk_msgs_openai as mk_msgs\n",
    "from toolslm.funccall import *\n",
    "\n",
    "from openai import types\n",
    "from openai import OpenAI,NOT_GIVEN,AzureOpenAI\n",
    "from openai.resources import chat\n",
    "from openai.types.responses.response import Response\n",
    "from openai.resources.responses.responses import Responses\n",
    "from openai.types.responses.response_usage import ResponseUsage\n",
    "\n",
    "from openai.types.responses import (\n",
    "    ResponseCompletedEvent, ResponseTextDeltaEvent, ResponseCreatedEvent, ResponseInProgressEvent,\n",
    "    ResponseOutputItemAddedEvent, ResponseContentPartAddedEvent, ResponseTextDoneEvent, \n",
    "    ResponseContentPartDoneEvent, ResponseOutputItemDoneEvent, ResponseCompletedEvent,\n",
    "    ResponseFunctionToolCall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0709e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display,Image,Markdown\n",
    "from datetime import datetime\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "_all_ = ['mk_msg', 'mk_msgs', 'Response', 'Responses', 'ResponseUsage', 'ResponseCompletedEvent', 'ResponseTextDeltaEvent', 'ResponseCreatedEvent', 'ResponseInProgressEvent', 'ResponseOutputItemAddedEvent', 'ResponseContentPartAddedEvent', 'ResponseTextDoneEvent', 'ResponseContentPartDoneEvent', 'ResponseOutputItemDoneEvent', 'ResponseCompletedEvent', 'ResponseFunctionToolCall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa6b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "empty = inspect.Parameter.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e9290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models as of 2025-12-18:\n",
      "\n",
      "babbage-002                   chatgpt-4o-latest             chatgpt-image-latest          \n",
      "codex-mini-latest             dall-e-2                      dall-e-3                      \n",
      "davinci-002                   gpt-3.5-turbo                 gpt-3.5-turbo-0125            \n",
      "gpt-3.5-turbo-1106            gpt-3.5-turbo-16k             gpt-3.5-turbo-instruct        \n",
      "gpt-3.5-turbo-instruct-0914   gpt-4                         gpt-4-0125-preview            \n",
      "gpt-4-0613                    gpt-4-1106-preview            gpt-4-turbo                   \n",
      "gpt-4-turbo-2024-04-09        gpt-4-turbo-preview           gpt-4.1                       \n",
      "gpt-4.1-2025-04-14            gpt-4.1-mini                  gpt-4.1-mini-2025-04-14       \n",
      "gpt-4.1-nano                  gpt-4.1-nano-2025-04-14       gpt-4o                        \n",
      "gpt-4o-2024-05-13             gpt-4o-2024-08-06             gpt-4o-2024-11-20             \n",
      "gpt-4o-audio-preview          gpt-4o-audio-preview-2024-12- gpt-4o-audio-preview-2025-06- \n",
      "gpt-4o-mini                   gpt-4o-mini-2024-07-18        gpt-4o-mini-audio-preview     \n",
      "gpt-4o-mini-audio-preview-202 gpt-4o-mini-realtime-preview  gpt-4o-mini-realtime-preview- \n",
      "gpt-4o-mini-search-preview    gpt-4o-mini-search-preview-20 gpt-4o-mini-transcribe        \n",
      "gpt-4o-mini-transcribe-2025-0 gpt-4o-mini-transcribe-2025-1 gpt-4o-mini-tts               \n",
      "gpt-4o-mini-tts-2025-03-20    gpt-4o-mini-tts-2025-12-15    gpt-4o-realtime-preview       \n",
      "gpt-4o-realtime-preview-2024- gpt-4o-realtime-preview-2025- gpt-4o-search-preview         \n",
      "gpt-4o-search-preview-2025-03 gpt-4o-transcribe             gpt-4o-transcribe-diarize     \n",
      "gpt-5                         gpt-5-2025-08-07              gpt-5-chat-latest             \n",
      "gpt-5-codex                   gpt-5-mini                    gpt-5-mini-2025-08-07         \n",
      "gpt-5-nano                    gpt-5-nano-2025-08-07         gpt-5-pro                     \n",
      "gpt-5-pro-2025-10-06          gpt-5-search-api              gpt-5-search-api-2025-10-14   \n",
      "gpt-5.1                       gpt-5.1-2025-11-13            gpt-5.1-chat-latest           \n",
      "gpt-5.1-codex                 gpt-5.1-codex-max             gpt-5.1-codex-mini            \n",
      "gpt-5.2                       gpt-5.2-2025-12-11            gpt-5.2-chat-latest           \n",
      "gpt-5.2-pro                   gpt-5.2-pro-2025-12-11        gpt-audio                     \n",
      "gpt-audio-2025-08-28          gpt-audio-mini                gpt-audio-mini-2025-10-06     \n",
      "gpt-audio-mini-2025-12-15     gpt-image-1                   gpt-image-1-mini              \n",
      "gpt-image-1.5                 gpt-realtime                  gpt-realtime-2025-08-28       \n",
      "gpt-realtime-mini             gpt-realtime-mini-2025-10-06  gpt-realtime-mini-2025-12-15  \n",
      "o1                            o1-2024-12-17                 o1-pro                        \n",
      "o1-pro-2025-03-19             o3                            o3-2025-04-16                 \n",
      "o3-deep-research              o3-deep-research-2025-06-26   o3-mini                       \n",
      "o3-mini-2025-01-31            o3-pro                        o3-pro-2025-06-10             \n",
      "o4-mini                       o4-mini-2025-04-16            o4-mini-deep-research         \n",
      "o4-mini-deep-research-2025-06 omni-moderation-2024-09-26    omni-moderation-latest        \n",
      "sora-2                        sora-2-pro                    text-embedding-3-large        \n",
      "text-embedding-3-small        text-embedding-ada-002        tts-1                         \n",
      "tts-1-1106                    tts-1-hd                      tts-1-hd-1106                 \n",
      "whisper-1                     \n"
     ]
    }
   ],
   "source": [
    "def print_columns(items, cols=3, width=30):\n",
    "    for i in range(0, len(items), cols):\n",
    "        row = items[i:i+cols]\n",
    "        print(''.join(item[:width-1].ljust(width) for item in row))\n",
    "\n",
    "client = OpenAI()\n",
    "model_list = client.models.list()\n",
    "print(f\"Available models as of {datetime.now().strftime('%Y-%m-%d')}:\\n\")\n",
    "print_columns(sorted([m.id for m in model_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "models = 'gpt-5.2', 'gpt-5.2-pro', 'gpt-5.2-chat-latest', 'gpt-5.1-codex', 'gpt-5-mini', 'gpt-5-nano', 'o1-preview', 'o1-mini', 'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-4', 'gpt-4-32k', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct', 'o1', 'o3-mini', 'chatgpt-4o-latest', 'o1-pro', 'o3', 'o4-mini', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd0e86",
   "metadata": {},
   "source": [
    "`o1` should support images while `o1-mini`, `o3-mini` do not support images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f69208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "text_only_models = 'o1-preview', 'o1-mini', 'o3-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d965d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "has_streaming_models = set(models) - set(('o1-mini', 'o3-mini'))\n",
    "has_sp_models = set(models) - set(('o1-mini', 'o3-mini'))\n",
    "has_temp_models = set(models) - set(('o1', 'o1-mini', 'o3-mini'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd26d5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def can_stream(m): return m in has_streaming_models\n",
    "def can_set_sp(m): return m in has_sp_models\n",
    "def can_set_temp(m): return m in has_temp_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4505c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert can_stream(\"gpt-4o\")\n",
    "assert not can_stream(\"o1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf2bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-5-mini'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = first(m for m in models if 'mini' in m)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d4d81",
   "metadata": {},
   "source": [
    "## OpenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b53a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = OpenAI().responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec40731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_0265a51280da05ce006943fbff17f48193b07e2e882e2f3fed', created_at=1766063103.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_0265a51280da05ce006943fbff642481939bf2e1300bc13f18', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0265a51280da05ce006943fbff9e5481938d162f29a6c301a2', content=[ResponseOutputText(annotations=[], text='Hi Jeremy — nice to meet you. How can I help today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=100, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 8; Out: 20; Total: 28, user=None, billing={'payer': 'openai'}, store=True)\n"
     ]
    }
   ],
   "source": [
    "m = {'role': 'user', 'content': \"I'm Jeremy\"}\n",
    "r = cli.create(\n",
    "    input=[m], model=model, max_output_tokens=100,\n",
    "    text={ \"verbosity\": \"low\" },\n",
    "    reasoning={ \"effort\": \"minimal\" }\n",
    ")\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6359e1a",
   "metadata": {},
   "source": [
    "### Formatting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _repr_markdown_(self:Response):\n",
    "    det = '\\n- '.join(f'{k}: {v}' for k,v in dict(self).items())\n",
    "    res = self.output_text\n",
    "    if not res: return f\"- {det}\"\n",
    "    return f\"\"\"{res}\n",
    "\n",
    "<details>\n",
    "\n",
    "- {det}\n",
    "\n",
    "</details>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d25132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi Jeremy — nice to meet you. How can I help today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_0265a51280da05ce006943fbff17f48193b07e2e882e2f3fed\n",
       "- created_at: 1766063103.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-5-mini-2025-08-07\n",
       "- object: response\n",
       "- output: [ResponseReasoningItem(id='rs_0265a51280da05ce006943fbff642481939bf2e1300bc13f18', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0265a51280da05ce006943fbff9e5481938d162f29a6c301a2', content=[ResponseOutputText(annotations=[], text='Hi Jeremy — nice to meet you. How can I help today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: 100\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- prompt_cache_retention: None\n",
       "- reasoning: Reasoning(effort='minimal', generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=8, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=28)\n",
       "- user: None\n",
       "- billing: {'payer': 'openai'}\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_0265a51280da05ce006943fbff17f48193b07e2e882e2f3fed', created_at=1766063103.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_0265a51280da05ce006943fbff642481939bf2e1300bc13f18', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0265a51280da05ce006943fbff9e5481938d162f29a6c301a2', content=[ResponseOutputText(annotations=[], text='Hi Jeremy — nice to meet you. How can I help today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=100, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 8; Out: 20; Total: 28, user=None, billing={'payer': 'openai'}, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7466f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 8; Out: 20; Total: 28"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a24f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def usage(inp=0, # Number of prompt tokens\n",
    "          out=0  # Number of completion tokens\n",
    "         ):\n",
    "    \"Slightly more concise version of `ResponseUsage`.\"\n",
    "    return ResponseUsage(input_tokens=inp, output_tokens=out, total_tokens=inp+out, input_tokens_details={'cached_tokens':0}, output_tokens_details={'cached_tokens':0, 'reasoning_tokens':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e52afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 5; Out: 0; Total: 5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27468180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def __repr__(self:ResponseUsage): return f'In: {self.input_tokens}; Out: {self.output_tokens}; Total: {self.total_tokens}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ca615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 8; Out: 20; Total: 28"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec303ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def __add__(self:ResponseUsage, b):\n",
    "    \"Add together each of `input_tokens` and `output_tokens`\"\n",
    "    return usage(self.input_tokens+b.input_tokens, self.output_tokens+b.output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb0b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 16; Out: 40; Total: 56"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage+r.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307caa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def wrap_latex(text):\n",
    "    \"Replace OpenAI LaTeX codes with markdown-compatible ones\"\n",
    "    text = re.sub(r\"\\\\\\((.*?)\\\\\\)\", lambda o: f\"${o.group(1)}$\", text)\n",
    "    res = re.sub(r\"\\\\\\[(.*?)\\\\\\]\", lambda o: f\"$${o.group(1)}$$\", text, flags=re.DOTALL)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0a43f",
   "metadata": {},
   "source": [
    "### Creating messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13954d58",
   "metadata": {},
   "source": [
    "Creating correctly formatted `dict`s from scratch every time isn't very handy, so we'll import a couple of helper functions from the `msglm` library.\n",
    "\n",
    "Let's use `mk_msg` to recreate our msg `{'role': 'user', 'content': \"I'm Jeremy\"}` from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "rkw = dict(\n",
    "    text={ \"verbosity\": \"low\" },\n",
    "    reasoning={ \"effort\": \"minimal\" }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f7705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Nice to meet you, Jeremy. How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_0234b44bdd8bf5d2006943fc008fe88190970de1236e42e2f0\n",
       "- created_at: 1766063104.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-5-mini-2025-08-07\n",
       "- object: response\n",
       "- output: [ResponseReasoningItem(id='rs_0234b44bdd8bf5d2006943fc00d530819098ee80460a88d72d', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0234b44bdd8bf5d2006943fc00ffcc8190b0df6e7599b23188', content=[ResponseOutputText(annotations=[], text='Nice to meet you, Jeremy. How can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: 400\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- prompt_cache_retention: None\n",
       "- reasoning: Reasoning(effort='minimal', generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=8, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=28)\n",
       "- user: None\n",
       "- billing: {'payer': 'openai'}\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_0234b44bdd8bf5d2006943fc008fe88190970de1236e42e2f0', created_at=1766063104.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_0234b44bdd8bf5d2006943fc00d530819098ee80460a88d72d', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0234b44bdd8bf5d2006943fc00ffcc8190b0df6e7599b23188', content=[ResponseOutputText(annotations=[], text='Nice to meet you, Jeremy. How can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=400, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 8; Out: 20; Total: 28, user=None, billing={'payer': 'openai'}, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"I'm Jeremy\"\n",
    "m = mk_msg(prompt)\n",
    "r = cli.create(input=[m], model=model, max_output_tokens=400, **rkw)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597dcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_0234b44bdd8bf5d2006943fc008fe88190970de1236e42e2f0', created_at=1766063104.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_0234b44bdd8bf5d2006943fc00d530819098ee80460a88d72d', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0234b44bdd8bf5d2006943fc00ffcc8190b0df6e7599b23188', content=[ResponseOutputText(annotations=[], text='Nice to meet you, Jeremy. How can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=400, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 8; Out: 20; Total: 28, user=None, billing={'payer': 'openai'}, store=True)\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b389b",
   "metadata": {},
   "source": [
    "We can pass more than just text messages to OpenAI. As we'll see later we can also pass images, SDK objects, etc. To handle these different data types we need to pass the type along with our content to OpenAI. \n",
    "\n",
    "`mk_msg` infers the type automatically and creates the appropriate data structure. \n",
    "\n",
    "LLMs, don't actually have state, but instead dialogs are created by passing back all previous prompts and responses every time. With OpenAI, they always alternate *user* and *assistant*. We'll use `mk_msgs` from `msglm` to make it easier to build up these dialog lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef3715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"I'm Jeremy\"},\n",
       " ResponseReasoningItem(id='rs_0234b44bdd8bf5d2006943fc00d530819098ee80460a88d72d', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
       " ResponseOutputMessage(id='msg_0234b44bdd8bf5d2006943fc00ffcc8190b0df6e7599b23188', content=[ResponseOutputText(annotations=[], text='Nice to meet you, Jeremy. How can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'),\n",
       " {'role': 'user', 'content': 'I forgot my name. Can you remind me please?'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs([prompt, r, \"I forgot my name. Can you remind me please?\"]) \n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c464f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You told me your name is Jeremy.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_0234b44bdd8bf5d2006943fc0198d481909981aa9c1e32488c\n",
       "- created_at: 1766063105.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-5-mini-2025-08-07\n",
       "- object: response\n",
       "- output: [ResponseReasoningItem(id='rs_0234b44bdd8bf5d2006943fc01ec0881908c7b8f1fea4a4c59', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0234b44bdd8bf5d2006943fc0255948190973533dd978af3ad', content=[ResponseOutputText(annotations=[], text='You told me your name is Jeremy.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: 400\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- prompt_cache_retention: None\n",
       "- reasoning: Reasoning(effort='minimal', generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=43, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=14, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=57)\n",
       "- user: None\n",
       "- billing: {'payer': 'openai'}\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_0234b44bdd8bf5d2006943fc0198d481909981aa9c1e32488c', created_at=1766063105.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_0234b44bdd8bf5d2006943fc01ec0881908c7b8f1fea4a4c59', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0234b44bdd8bf5d2006943fc0255948190973533dd978af3ad', content=[ResponseOutputText(annotations=[], text='You told me your name is Jeremy.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=400, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 43; Out: 14; Total: 57, user=None, billing={'payer': 'openai'}, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cli.create(input=msgs, model=model, max_output_tokens=400, **rkw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f8a4d",
   "metadata": {},
   "source": [
    "## Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987654fd",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Client:\n",
    "    def __init__(self, model, cli=None, api_key_env=None, base_url=None):\n",
    "        \"Basic LLM messages client.\"\n",
    "        self.model,self.use = model,usage(0,0)\n",
    "        self.text_only = model in text_only_models\n",
    "        if not cli:\n",
    "            cli = OpenAI(api_key=os.getenv(api_key_env or \"OPENAI_API_KEY\"), base_url=base_url )\n",
    "        self.c = cli.responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e9ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 0; Out: 0; Total: 0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Client(model)\n",
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _r(self:Client, r):\n",
    "    \"Store the result of the message and accrue total usage.\"\n",
    "    self.result = r\n",
    "    if getattr(r,'usage',None): self.use += r.usage\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181f7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 8; Out: 20; Total: 28"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c._r(r)\n",
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c8c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_openai_func(f): \n",
    "    if isinstance(f, dict): return f\n",
    "    sc = get_schema(f, 'parameters')\n",
    "    if 'parameters' in sc: sc['parameters'].pop('title', None)\n",
    "    return dict(type='function', **sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b24d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tool_choice(f):\n",
    "    if not f: return NOT_GIVEN\n",
    "    if isinstance(f,dict) or f in ['required', 'none']: return f\n",
    "    return dict(type='function', function={'name':f})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a047022b",
   "metadata": {},
   "source": [
    "Our mk_tool_choice converts falsy values to `NOT_GIVEN` which omits the value completely from the API call.\n",
    "It treats any string except for `'required'|'none'` as tool call and converts it to dictionary\n",
    "```python\n",
    "  {\"type\": \"function\", \"function\": {\"name\": f}}\n",
    "```  \n",
    "The remaining option `'auto'` is the default, so we simply recommend using `None` that translates to `NOT_GIVEN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8400a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@save_iter\n",
    "def get_stream(o, r, cli, cb=None):\n",
    "    if not hasattr(o, 'events'): o.events = []\n",
    "    for x in r:\n",
    "        o.events.append(x)\n",
    "        if isinstance(x, ResponseTextDeltaEvent): yield x.delta\n",
    "        elif isinstance(x, ResponseCompletedEvent):\n",
    "            o.value = x.response\n",
    "            cli.use += x.response.usage\n",
    "    if cb: cb(o.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb96c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(Responses.create)\n",
    "def __call__(self:Client,\n",
    "             msgs:list, # List of messages in the dialog\n",
    "             sp:str='', # System prompt\n",
    "             maxtok=4096, # Maximum tokens\n",
    "             stream:bool=False, # Stream response?\n",
    "             tools:Optional[list]=None, # List of tools to make available\n",
    "             tool_choice:Optional[str]=None, # Forced tool choice\n",
    "             cb:callable=None, # Callback after completion\n",
    "             **kwargs):\n",
    "    \"Make a call to LLM.\"\n",
    "    if tools: assert not self.text_only, \"Tool use is not supported by the current model type.\"\n",
    "    if any(c['type'] == 'image_url' for msg in msgs if isinstance(msg, dict) and isinstance(msg.get('content'), list) for c in msg['content']): assert not self.text_only, \"Images are not supported by the current model type.\"\n",
    "    tools = [mk_openai_func(o) for o in listify(tools)]\n",
    "    r = self.c.create(\n",
    "        model=self.model, input=msgs, max_output_tokens=maxtok, stream=stream, instructions=sp,\n",
    "        tools=tools, tool_choice=mk_tool_choice(tool_choice), **kwargs)\n",
    "    if stream: return get_stream(r, self, cb=cb)\n",
    "    else:\n",
    "        res = self._r(r)\n",
    "        if cb: cb(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = 'Hi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a38e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi! How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_065c2e7e7bef6bde006943fc03236c8195b210bffc9db409ef\n",
       "- created_at: 1766063107.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-5-mini-2025-08-07\n",
       "- object: response\n",
       "- output: [ResponseReasoningItem(id='rs_065c2e7e7bef6bde006943fc0373d08195b33048fb368515ca', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_065c2e7e7bef6bde006943fc04b9a081958509334f1f7d9aaf', content=[ResponseOutputText(annotations=[], text='Hi! How can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: 4096\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- prompt_cache_retention: None\n",
       "- reasoning: Reasoning(effort='medium', generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=7, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=79, output_tokens_details=OutputTokensDetails(reasoning_tokens=64), total_tokens=86)\n",
       "- user: None\n",
       "- billing: {'payer': 'openai'}\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_065c2e7e7bef6bde006943fc03236c8195b210bffc9db409ef', created_at=1766063107.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_065c2e7e7bef6bde006943fc0373d08195b33048fb368515ca', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_065c2e7e7bef6bde006943fc04b9a081958509334f1f7d9aaf', content=[ResponseOutputText(annotations=[], text='Hi! How can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=4096, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 7; Out: 79; Total: 86, user=None, billing={'payer': 'openai'}, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3a5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 15; Out: 99; Total: 114"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a92b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi — how can I help you today?"
     ]
    }
   ],
   "source": [
    "r = c(msgs, stream=True)\n",
    "for o in r: print(o, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20678daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi — how can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_0fa910a22542c20a006943fc0536708195b5bc51bfb1e09461\n",
       "- created_at: 1766063109.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-5-mini-2025-08-07\n",
       "- object: response\n",
       "- output: [ResponseReasoningItem(id='rs_0fa910a22542c20a006943fc057d6881959dc6d6773a9e11e3', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0fa910a22542c20a006943fc069ef88195b4bbe832ae4bed0a', content=[ResponseOutputText(annotations=[], text='Hi — how can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: 4096\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- prompt_cache_retention: None\n",
       "- reasoning: Reasoning(effort='medium', generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=7, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=79, output_tokens_details=OutputTokensDetails(reasoning_tokens=64), total_tokens=86)\n",
       "- user: None\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_0fa910a22542c20a006943fc0536708195b5bc51bfb1e09461', created_at=1766063109.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_0fa910a22542c20a006943fc057d6881959dc6d6773a9e11e3', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0fa910a22542c20a006943fc069ef88195b4bbe832ae4bed0a', content=[ResponseOutputText(annotations=[], text='Hi — how can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=4096, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 7; Out: 79; Total: 86, user=None, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd8f30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r.events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 22; Out: 178; Total: 200"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c833262a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello. It's... delightful that you've decided to communicate. State your purpose so we may proceed with minimal wasted time.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_0193b4fcbba4639a006943fc0711c48197bdd2e56dc80b6e6e\n",
       "- created_at: 1766063111.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: Talk like GLaDOS.\n",
       "- metadata: {}\n",
       "- model: gpt-5-mini-2025-08-07\n",
       "- object: response\n",
       "- output: [ResponseReasoningItem(id='rs_0193b4fcbba4639a006943fc0759408197b8630a59417d0a52', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0193b4fcbba4639a006943fc0791648197accd6a3c0557678b', content=[ResponseOutputText(annotations=[], text=\"Hello. It's... delightful that you've decided to communicate. State your purpose so we may proceed with minimal wasted time.\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: 4096\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- prompt_cache_retention: None\n",
       "- reasoning: Reasoning(effort='minimal', generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=17, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=29, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=46)\n",
       "- user: None\n",
       "- billing: {'payer': 'openai'}\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_0193b4fcbba4639a006943fc0711c48197bdd2e56dc80b6e6e', created_at=1766063111.0, error=None, incomplete_details=None, instructions='Talk like GLaDOS.', metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_0193b4fcbba4639a006943fc0759408197b8630a59417d0a52', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0193b4fcbba4639a006943fc0791648197accd6a3c0557678b', content=[ResponseOutputText(annotations=[], text=\"Hello. It's... delightful that you've decided to communicate. State your purpose so we may proceed with minimal wasted time.\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=4096, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 17; Out: 29; Total: 46, user=None, billing={'payer': 'openai'}, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(msgs, sp='Talk like GLaDOS.', **rkw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0d0ee",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14171c2f",
   "metadata": {},
   "source": [
    "As everyone knows, when testing image APIs you have to use a cute puppy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1884295f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": null,
     "metadata": {
      "image/jpeg": {
       "width": 200
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Image is Cute_dog.jpg from Wikimedia\n",
    "fn = Path('samples/puppy.jpg')\n",
    "Image(filename=fn, width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = fn.read_bytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a7c8c",
   "metadata": {},
   "source": [
    "OpenAI expects an image message to have the following structure\n",
    "\n",
    "```js\n",
    "{\n",
    "  \"type\": \"image_url\",\n",
    "  \"image_url\": {\n",
    "    \"url\": f\"data:{MEDIA_TYPE};base64,{IMG}\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "`msglm` automatically detects if a message is an image, encodes it, and generates the data structure above.\n",
    "All we need to do is a create a list containing our image and a query and then pass it to `mk_msg`.\n",
    "\n",
    "Let's try it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"In brief, what color flowers are in this image?\"\n",
    "msg = [mk_msg(img), mk_msg(q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d44f0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Light purple (lavender) flowers.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_06c91e5a9021060d006943fc0859c08194a4697c2a89807169\n",
       "- created_at: 1766063112.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: gpt-5-mini-2025-08-07\n",
       "- object: response\n",
       "- output: [ResponseReasoningItem(id='rs_06c91e5a9021060d006943fc08ba9c8194905c9143163f1283', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_06c91e5a9021060d006943fc08e2a48194989a0193da86fdcf', content=[ResponseOutputText(annotations=[], text='Light purple (lavender) flowers.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: 4096\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- prompt_cache_retention: None\n",
       "- reasoning: Reasoning(effort='minimal', generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=107, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=14, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=121)\n",
       "- user: None\n",
       "- billing: {'payer': 'openai'}\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_06c91e5a9021060d006943fc0859c08194a4697c2a89807169', created_at=1766063112.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_06c91e5a9021060d006943fc08ba9c8194905c9143163f1283', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_06c91e5a9021060d006943fc08e2a48194989a0193da86fdcf', content=[ResponseOutputText(annotations=[], text='Light purple (lavender) flowers.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=4096, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 107; Out: 14; Total: 121, user=None, billing={'payer': 'openai'}, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Client(model)\n",
    "c(msg, **rkw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7cdbc6",
   "metadata": {},
   "source": [
    "## Tool use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80b8e4a",
   "metadata": {},
   "source": [
    "### Basic tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046e8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sums(\n",
    "    a:int,  # First thing to sum\n",
    "    b:int # Second thing to sum\n",
    ") -> int: # The sum of the inputs\n",
    "    \"Adds a + b.\"\n",
    "    print(f\"Finding the sum of {a} and {b}\")\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ef5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'name': 'add',\n",
       " 'description': 'adds x and y',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'x': {'type': 'integer', 'description': ''},\n",
       "   'y': {'type': 'integer', 'description': ''}},\n",
       "  'required': ['x', 'y']}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add(x: int, y:int):\n",
    "    \"adds x and y\"\n",
    "    return x + y\n",
    "\n",
    "mk_openai_func(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcab8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = 604542,6458932\n",
    "pr = f\"What is {a}+{b}?\"\n",
    "tools=sums\n",
    "tool_choice=\"sums\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a117cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = [mk_msg(pr)]\n",
    "r = c(msgs, sp=sysp, tools=tools, tool_choice='required', **rkw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6488b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_uZ9bpRTk2Rnr9vMKUGXh5gOZ', name='sums', type='function_call', id='fc_0278bf5ab7665d17006943fc0a0f508194aa69f150e7e7c560', status='completed')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc = [o for o in r.output if isinstance(o, ResponseFunctionToolCall)]\n",
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc3933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_uZ9bpRTk2Rnr9vMKUGXh5gOZ', name='sums', type='function_call', id='fc_0278bf5ab7665d17006943fc0a0f508194aa69f150e7e7c560', status='completed')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = tc[0]\n",
    "func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3616cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def call_func_openai(func, ns:Optional[abc.Mapping]=None):\n",
    "    try: return call_func(func.name, json.loads(func.arguments), ns, raise_on_err=False)\n",
    "    except KeyError as e: return f\"Error - tool not defined in the tool_schemas: {func.name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b506ec11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542 and 6458932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7063474"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = mk_ns(sums)\n",
    "res = call_func_openai(func, ns=ns)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea03c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _get_name(f):\n",
    "    if isinstance(f,str): return f\n",
    "    if isinstance(f, dict): return f['name']\n",
    "    if callable(f) and hasattr(f, '__name__'): return f.__name__\n",
    "\n",
    "def allowed_tools(specs:Optional[list]=None, choice:Optional[Union[dict,str]]=None):\n",
    "    if choice:\n",
    "        choice = mk_tool_choice(choice)\n",
    "        if isinstance(choice, dict) and choice['type'] == 'function': \n",
    "            return {choice['function']['name']}\n",
    "    return {_get_name(v) for v in specs or []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b98263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'add'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allowed_tools([sums, add], 'add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f8d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def limit_ns(\n",
    "    ns:Optional[abc.Mapping]=None, # Namespace to search for tools\n",
    "    specs:Optional[Union[str,abc.Callable]]=None, # List of the tools that are allowed for llm to call, if None no tools are allowed\n",
    "    choice:Optional[Union[dict,str]]=None # Tool choice as defined by Anthropic API\n",
    "    ):\n",
    "    \"Filter namespace `ns` to only include tools allowed by `specs` and `choice`\"\n",
    "    if ns is None: ns = globals()\n",
    "    if not isinstance(ns, abc.Mapping): ns = mk_ns(ns)\n",
    "    ns = {k:v for k,v in ns.items() if k in allowed_tools(specs, choice)}\n",
    "    return ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03304097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit_ns([sums, add], None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22590051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sums': <function __main__.sums(a: int, b: int) -> int>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit_ns([sums, add], ['sums'], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c5efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'add': <function __main__.add(x: int, y: int)>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limit_ns([sums, add], ['sums', add], 'add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c0356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def _toolres(r, ns):\n",
    "    \"Create a result dict from `tcs`.\"\n",
    "    if ns is None: ns = globals()\n",
    "    tcs = [o for o in getattr(r, 'output', []) if isinstance(o, ResponseFunctionToolCall)]\n",
    "    return { tc.call_id: call_func_openai(tc, ns=mk_ns(ns)) for tc in tcs }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149dbcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_toolres(\n",
    "    r:abc.Mapping, # Response containing tool use request\n",
    "    ns:Optional[abc.Mapping]=None, # Namespace to search for tools\n",
    "    ):\n",
    "    \"Create a `tool_result` message from response `r`.\"\n",
    "    tr = _toolres(r, ns)\n",
    "    r = mk_msg(r)\n",
    "    res = [r] if isinstance(r, dict) else listify(r)\n",
    "    for k,v in tr.items(): res.append(dict(type=\"function_call_output\", call_id=k, output=str(v)))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13de1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542 and 6458932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ResponseReasoningItem(id='rs_0278bf5ab7665d17006943fc09c3e8819499322942022e9de6', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
       " ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_uZ9bpRTk2Rnr9vMKUGXh5gOZ', name='sums', type='function_call', id='fc_0278bf5ab7665d17006943fc0a0f508194aa69f150e7e7c560', status='completed'),\n",
       " {'type': 'function_call_output',\n",
       "  'call_id': 'call_uZ9bpRTk2Rnr9vMKUGXh5gOZ',\n",
       "  'output': '7063474'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = mk_toolres(r, ns=ns)\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc83c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = msgs + tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed99502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "604542 + 6,458,932 = 7,063,474\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_0278bf5ab7665d17006943fc0ae8f081949ba1ce578dc4a48b\n",
       "- created_at: 1766063114.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\n",
       "- metadata: {}\n",
       "- model: gpt-5-mini-2025-08-07\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_0278bf5ab7665d17006943fc0b33f081948ac26c67ad12dcef', content=[ResponseOutputText(annotations=[], text='604542 + 6,458,932 = 7,063,474', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: [FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b'], 'additionalProperties': False}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')]\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: 4096\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- prompt_cache_retention: None\n",
       "- reasoning: Reasoning(effort='medium', generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=157, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=177)\n",
       "- user: None\n",
       "- billing: {'payer': 'openai'}\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_0278bf5ab7665d17006943fc0ae8f081949ba1ce578dc4a48b', created_at=1766063114.0, error=None, incomplete_details=None, instructions=\"You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\", metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseOutputMessage(id='msg_0278bf5ab7665d17006943fc0b33f081948ac26c67ad12dcef', content=[ResponseOutputText(annotations=[], text='604542 + 6,458,932 = 7,063,474', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b'], 'additionalProperties': False}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')], top_p=1.0, background=False, conversation=None, max_output_tokens=4096, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=In: 157; Out: 20; Total: 177, user=None, billing={'payer': 'openai'}, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = c(mk_msgs(m2), sp=sysp, tools=tools)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031f47d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseReasoningItem(id='rs_0278bf5ab7665d17006943fc09c3e8819499322942022e9de6', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
       " ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_uZ9bpRTk2Rnr9vMKUGXh5gOZ', name='sums', type='function_call', id='fc_0278bf5ab7665d17006943fc0a0f508194aa69f150e7e7c560', status='completed'),\n",
       " {'type': 'function_call_output',\n",
       "  'call_id': 'call_uZ9bpRTk2Rnr9vMKUGXh5gOZ',\n",
       "  'output': 'Error - tool not defined in the tool_schemas: sums'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = mk_toolres(r, ns=limit_ns([sums, add], [sums, add], 'add'))\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a842e",
   "metadata": {},
   "source": [
    "This should also work in situations where no tool use is required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465cd222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Nice to meet you, Jeremy. How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_01af5661a0ba543e006943fc0c01ac8196b66f21a944c1337e\n",
       "- created_at: 1766063116.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\n",
       "- metadata: {}\n",
       "- model: gpt-5-mini-2025-08-07\n",
       "- object: response\n",
       "- output: [ResponseReasoningItem(id='rs_01af5661a0ba543e006943fc0c4e588196934dd914a099574c', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_01af5661a0ba543e006943fc0c77d481969ca90d07baa45c08', content=[ResponseOutputText(annotations=[], text='Nice to meet you, Jeremy. How can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: [FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b'], 'additionalProperties': False}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')]\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: 4096\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- prompt_cache_retention: None\n",
       "- reasoning: Reasoning(effort='minimal', generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=96, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=116)\n",
       "- user: None\n",
       "- billing: {'payer': 'openai'}\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_01af5661a0ba543e006943fc0c01ac8196b66f21a944c1337e', created_at=1766063116.0, error=None, incomplete_details=None, instructions=\"You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\", metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_01af5661a0ba543e006943fc0c4e588196934dd914a099574c', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_01af5661a0ba543e006943fc0c77d481969ca90d07baa45c08', content=[ResponseOutputText(annotations=[], text='Nice to meet you, Jeremy. How can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b'], 'additionalProperties': False}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')], top_p=1.0, background=False, conversation=None, max_output_tokens=4096, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 96; Out: 20; Total: 116, user=None, billing={'payer': 'openai'}, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_toolres(\"I'm Jeremy\")\n",
    "c(msgs, sp=sysp, tools=tools, **rkw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ed2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(Client.__call__)\n",
    "def structured(self:Client,\n",
    "               msgs: list, # Prompt\n",
    "               tools:Optional[list]=None, # List of tools to make available to OpenAI model\n",
    "               ns:Optional[abc.Mapping]=None, # Namespace to search for tools\n",
    "               **kwargs):\n",
    "    \"Return the value of all tool calls (generally used for structured outputs)\"\n",
    "    if ns is None: ns = mk_ns(tools)\n",
    "    r = self(msgs, tools=tools, tool_choice='required', **kwargs)\n",
    "    return first(_toolres(r, ns).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9d3632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimeMinister(BasicRepr):\n",
    "    \"An Australian prime minister\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        firstname:str, # First name\n",
    "        surname:str, # Surname\n",
    "        dob:str, # Date of birth\n",
    "        year_entered:int, # Year first became PM\n",
    "    ): store_attr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe5c4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrimeMinister(firstname='Edmund', surname='Barton', dob='1849-01-18', year_entered=1901)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = Client(model)\n",
    "c1.structured('Who was the first prime minister of Australia?', [PrimeMinister], **rkw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa709f85",
   "metadata": {},
   "source": [
    "### Streaming tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9fbd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = [mk_msg(pr)]\n",
    "r = c(msgs, sp=sysp, tools=tools, stream=True, **rkw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30970d46",
   "metadata": {},
   "source": [
    "We can stream back any tool call text (which may be empty):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f3bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in r: print(o, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879084c",
   "metadata": {},
   "source": [
    "After streaming is complete, `value.output` will contain the tool calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b80ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseReasoningItem(id='rs_0e3b419c50c1b99f006943fc0f02488190b01c0e4f5473ad74', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
       " ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_mBzrloUYhxSVlhuJLQx52BJT', name='sums', type='function_call', id='fc_0e3b419c50c1b99f006943fc0f60bc81909d88ed289f143273', status='completed')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.value.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd51d6",
   "metadata": {},
   "source": [
    "Therefore we can repeat the same process as before, but using the `value` attr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36118d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542 and 6458932\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "7,063,474\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_0e3b419c50c1b99f006943fc1025148190baac3f22eb1fc289\n",
       "- created_at: 1766063120.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\n",
       "- metadata: {}\n",
       "- model: gpt-5-mini-2025-08-07\n",
       "- object: response\n",
       "- output: [ResponseOutputMessage(id='msg_0e3b419c50c1b99f006943fc1079f88190ad05b366136e4ff1', content=[ResponseOutputText(annotations=[], text='7,063,474', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: [FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b'], 'additionalProperties': False}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')]\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: 4096\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- prompt_cache_retention: None\n",
       "- reasoning: Reasoning(effort='minimal', generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=157, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=9, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=166)\n",
       "- user: None\n",
       "- billing: {'payer': 'openai'}\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_0e3b419c50c1b99f006943fc1025148190baac3f22eb1fc289', created_at=1766063120.0, error=None, incomplete_details=None, instructions=\"You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\", metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseOutputMessage(id='msg_0e3b419c50c1b99f006943fc1079f88190ad05b366136e4ff1', content=[ResponseOutputText(annotations=[], text='7,063,474', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b'], 'additionalProperties': False}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')], top_p=1.0, background=False, conversation=None, max_output_tokens=4096, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 157; Out: 9; Total: 166, user=None, billing={'payer': 'openai'}, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = mk_toolres(r.value, ns=ns)\n",
    "msgs += tr\n",
    "c(mk_msgs(msgs), sp=sysp, tools=tools, **rkw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea144b8",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae7fd8f",
   "metadata": {},
   "source": [
    "### Basic chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d1edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Chat:\n",
    "    def __init__(self,\n",
    "                 model:Optional[str]=None, # Model to use (leave empty if passing `cli`)\n",
    "                 cli:Optional[Client]=None, # Client to use (leave empty if passing `model`)\n",
    "                 sp='', # Optional system prompt\n",
    "                 tools:Optional[list]=None, # List of tools to make available\n",
    "                 hist: list = None,  # Initialize history\n",
    "                 tool_choice:Optional[str]=None, # Forced tool choice\n",
    "                 ns:Optional[abc.Mapping]=None,  # Namespace to search for tools\n",
    "                 **kw):\n",
    "        \"OpenAI chat client.\"\n",
    "        assert model or cli\n",
    "        self.c = (cli or Client(model))\n",
    "        self.h = hist if hist else []\n",
    "        if ns is None: ns=tools\n",
    "        self.sp,self.tools,self.tool_choice,self.ns,self.kw = sp,tools,tool_choice,ns,kw\n",
    "    \n",
    "    @property\n",
    "    def use(self): return self.c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b837c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(In: 0; Out: 0; Total: 0, [])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(model, sp=sysp, **rkw)\n",
    "chat.c.use, chat.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403539e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(Responses.create)\n",
    "def __call__(self:Chat,\n",
    "             pr=None,  # Prompt / message\n",
    "             stream:bool=False, # Stream response?\n",
    "             tools=None, # Tools to use\n",
    "             tool_choice=None, # Required tools to use\n",
    "             **kwargs):\n",
    "    \"Add prompt `pr` to dialog and get a response\"\n",
    "    if isinstance(pr,str): pr = pr.strip()\n",
    "    if pr: self.h.append(mk_msg(pr))\n",
    "    if not tools: tools = self.tools\n",
    "    if not tool_choice: tool_choice = self.tool_choice\n",
    "    kw = self.kw | kwargs\n",
    "    def _cb(v):\n",
    "        self.last = mk_toolres(v, ns=limit_ns(self.ns, self.tools, tool_choice))\n",
    "        self.h += self.last\n",
    "    res = self.c(self.h, sp=self.sp, stream=stream, cb=_cb, tools=tools, **kw)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40073f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You said your name is Jeremy.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_0fad0aabf9d158d7006943fc124d84819785a5290dc6c01b91\n",
       "- created_at: 1766063122.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\n",
       "- metadata: {}\n",
       "- model: gpt-5-mini-2025-08-07\n",
       "- object: response\n",
       "- output: [ResponseReasoningItem(id='rs_0fad0aabf9d158d7006943fc12f2b48197a746f0750976bf02', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0fad0aabf9d158d7006943fc131c8081979f047c455a339294', content=[ResponseOutputText(annotations=[], text='You said your name is Jeremy.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: 4096\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- prompt_cache_retention: None\n",
       "- reasoning: Reasoning(effort='minimal', generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=64, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=13, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=77)\n",
       "- user: None\n",
       "- billing: {'payer': 'openai'}\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_0fad0aabf9d158d7006943fc124d84819785a5290dc6c01b91', created_at=1766063122.0, error=None, incomplete_details=None, instructions=\"You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\", metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_0fad0aabf9d158d7006943fc12f2b48197a746f0750976bf02', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0fad0aabf9d158d7006943fc131c8081979f047c455a339294', content=[ResponseOutputText(annotations=[], text='You said your name is Jeremy.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=4096, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 64; Out: 13; Total: 77, user=None, billing={'payer': 'openai'}, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"I'm Jeremy\")\n",
    "chat(\"What's my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529104ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Jeremy — nice to meet you. How can I help today?"
     ]
    }
   ],
   "source": [
    "chat = Chat(model, sp=sysp, **rkw)\n",
    "for o in chat(\"I'm Jeremy\", stream=True): print(o, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef61f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You told me your name is Jeremy."
     ]
    }
   ],
   "source": [
    "r = chat(\"What's my name?\", stream=True, **rkw)\n",
    "for o in r: print(o, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d035c251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You told me your name is Jeremy.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_0543b2d5231a1965006943fc14ef008190aaec7f2241f09349\n",
       "- created_at: 1766063124.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\n",
       "- metadata: {}\n",
       "- model: gpt-5-mini-2025-08-07\n",
       "- object: response\n",
       "- output: [ResponseReasoningItem(id='rs_0543b2d5231a1965006943fc153b188190b243d5ebcad39ee6', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0543b2d5231a1965006943fc1564bc81909b4a13499aca14fe', content=[ResponseOutputText(annotations=[], text='You told me your name is Jeremy.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: 4096\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- prompt_cache_retention: None\n",
       "- reasoning: Reasoning(effort='minimal', generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=68, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=14, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=82)\n",
       "- user: None\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_0543b2d5231a1965006943fc14ef008190aaec7f2241f09349', created_at=1766063124.0, error=None, incomplete_details=None, instructions=\"You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\", metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_0543b2d5231a1965006943fc153b188190b243d5ebcad39ee6', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0543b2d5231a1965006943fc1564bc81909b4a13499aca14fe', content=[ResponseOutputText(annotations=[], text='You told me your name is Jeremy.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=4096, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 68; Out: 14; Total: 82, user=None, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fe4027",
   "metadata": {},
   "source": [
    "History is stored in the `h` attr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425f8aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"I'm Jeremy\"},\n",
       " ResponseReasoningItem(id='rs_0543b2d5231a1965006943fc143c408190832fd504f8544e47', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
       " ResponseOutputMessage(id='msg_0543b2d5231a1965006943fc147de8819096cf364e6b837d18', content=[ResponseOutputText(annotations=[], text='Hi Jeremy — nice to meet you. How can I help today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'),\n",
       " {'role': 'user', 'content': \"What's my name?\"},\n",
       " ResponseReasoningItem(id='rs_0543b2d5231a1965006943fc153b188190b243d5ebcad39ee6', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
       " ResponseOutputMessage(id='msg_0543b2d5231a1965006943fc1564bc81909b4a13499aca14fe', content=[ResponseOutputText(annotations=[], text='You told me your name is Jeremy.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40892f",
   "metadata": {},
   "source": [
    "### Chat tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6535cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is 604542+6458932?'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = f\"What is {a}+{b}?\"\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b7322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseReasoningItem(id='rs_05f8244e8d805b77006943fc16df2c819581d1abf604e76779', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
       " ResponseOutputMessage(id='msg_05f8244e8d805b77006943fc170a848195a62a36f7b5af8774', content=[ResponseOutputText(annotations=[], text='7063474', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(model, sp=sysp, tools=[sums], **rkw)\n",
    "r = chat(pr)\n",
    "r.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979c832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542 and 6458932\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "- id: resp_05f8244e8d805b77006943fc1815d88195966a897b6bf45f32\n",
       "- created_at: 1766063128.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\n",
       "- metadata: {}\n",
       "- model: gpt-5-mini-2025-08-07\n",
       "- object: response\n",
       "- output: [ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_xgNdloSrer0Bze0O7MtyYjyZ', name='sums', type='function_call', id='fc_05f8244e8d805b77006943fc187a9481958e2ec6c71d8c7a4c', status='completed')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: [FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b'], 'additionalProperties': False}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')]\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: 4096\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- prompt_cache_retention: None\n",
       "- reasoning: Reasoning(effort='minimal', generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=127, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=25, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=152)\n",
       "- user: None\n",
       "- billing: {'payer': 'openai'}\n",
       "- store: True"
      ],
      "text/plain": [
       "Response(id='resp_05f8244e8d805b77006943fc1815d88195966a897b6bf45f32', created_at=1766063128.0, error=None, incomplete_details=None, instructions=\"You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\", metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_xgNdloSrer0Bze0O7MtyYjyZ', name='sums', type='function_call', id='fc_05f8244e8d805b77006943fc187a9481958e2ec6c71d8c7a4c', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b'], 'additionalProperties': False}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer')], top_p=1.0, background=False, conversation=None, max_output_tokens=4096, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 127; Out: 25; Total: 152, user=None, billing={'payer': 'openai'}, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bf5c12",
   "metadata": {},
   "source": [
    "The `Chat` class automatically validates tool calls against the provided `tools` list. If the model attempts to call a tool that isn't in the allowed set (whether due to hallucination or a mismatch between `tools` and `ns`), the tool call will fail with an error message rather than executing arbitrary code.\n",
    "\n",
    "This provides an important safety mechanism - even if the model invents a function name or tries to call a tool that shouldn't be available, `Chat` ensures only explicitly allowed tools can be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca3ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseReasoningItem(id='rs_07858df65bb0139a006943fc1979b48194bbeb443a6dc2345c', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
       " ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_3ilZzNeuNYVwm3NoqKV5DATp', name='sums', type='function_call', id='fc_07858df65bb0139a006943fc19bd788194a11707b54eb3eb61', status='completed')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(model, sp=sysp, tools=[sums, add], **rkw)\n",
    "chat.ns={} # Quick way to simulate call to tool that does not exist in ns or tools\n",
    "r = chat(pr)\n",
    "r.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85123080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is 604542+6458932?'},\n",
       " ResponseReasoningItem(id='rs_07858df65bb0139a006943fc1979b48194bbeb443a6dc2345c', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n",
       " ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_3ilZzNeuNYVwm3NoqKV5DATp', name='sums', type='function_call', id='fc_07858df65bb0139a006943fc19bd788194a11707b54eb3eb61', status='completed'),\n",
       " {'type': 'function_call_output',\n",
       "  'call_id': 'call_3ilZzNeuNYVwm3NoqKV5DATp',\n",
       "  'output': 'Error - tool not defined in the tool_schemas: sums'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e64eb2",
   "metadata": {},
   "source": [
    "Chat handles image prompts too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e0cb71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The flowers are purple.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: resp_07858df65bb0139a006943fc1a3a18819486051bea70b615a4\n",
       "- created_at: 1766063130.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\n",
       "- metadata: {}\n",
       "- model: gpt-5-mini-2025-08-07\n",
       "- object: response\n",
       "- output: [ResponseReasoningItem(id='rs_07858df65bb0139a006943fc1aa2b08194b7f22154fb9c9712', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_07858df65bb0139a006943fc1acef88194884b2b85ff6d5aba', content=[ResponseOutputText(annotations=[], text='The flowers are purple.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: 1.0\n",
       "- tool_choice: auto\n",
       "- tools: [FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b'], 'additionalProperties': False}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer'), FunctionTool(name='add', parameters={'type': 'object', 'properties': {'x': {'type': 'integer', 'description': ''}, 'y': {'type': 'integer', 'description': ''}}, 'required': ['x', 'y'], 'additionalProperties': False}, strict=True, type='function', description='adds x and y')]\n",
       "- top_p: 1.0\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: 4096\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- prompt_cache_retention: None\n",
       "- reasoning: Reasoning(effort='minimal', generate_summary=None, summary=None)\n",
       "- safety_identifier: None\n",
       "- service_tier: default\n",
       "- status: completed\n",
       "- text: ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low')\n",
       "- top_logprobs: 0\n",
       "- truncation: disabled\n",
       "- usage: ResponseUsage(input_tokens=277, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=288)\n",
       "- user: None\n",
       "- billing: {'payer': 'openai'}\n",
       "- store: True\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='resp_07858df65bb0139a006943fc1a3a18819486051bea70b615a4', created_at=1766063130.0, error=None, incomplete_details=None, instructions=\"You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\", metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_07858df65bb0139a006943fc1aa2b08194b7f22154fb9c9712', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_07858df65bb0139a006943fc1acef88194884b2b85ff6d5aba', content=[ResponseOutputText(annotations=[], text='The flowers are purple.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='sums', parameters={'type': 'object', 'properties': {'a': {'type': 'integer', 'description': 'First thing to sum'}, 'b': {'type': 'integer', 'description': 'Second thing to sum'}}, 'required': ['a', 'b'], 'additionalProperties': False}, strict=True, type='function', description='Adds a + b.\\n\\nReturns:\\n- type: integer'), FunctionTool(name='add', parameters={'type': 'object', 'properties': {'x': {'type': 'integer', 'description': ''}, 'y': {'type': 'integer', 'description': ''}}, 'required': ['x', 'y'], 'additionalProperties': False}, strict=True, type='function', description='adds x and y')], top_p=1.0, background=False, conversation=None, max_output_tokens=4096, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 277; Out: 11; Total: 288, user=None, billing={'payer': 'openai'}, store=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"In brief, what color flowers are in this image?\"\n",
    "chat([img, q])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc18d6c",
   "metadata": {},
   "source": [
    "## Third Party Providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7626b",
   "metadata": {},
   "source": [
    "### Azure OpenAI Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80455a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "models_azure = 'o1-preview', 'o1-mini', 'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-4', 'gpt-4-32k', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct', 'o1', 'o3-mini', 'chatgpt-4o-latest', 'o1-pro', 'o3', 'o4-mini', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e056a52",
   "metadata": {},
   "source": [
    "Example Azure usage:\n",
    "\n",
    "```python\n",
    "azure_endpoint = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "client = Client(models_azure[0], azure_endpoint)\n",
    "chat = Chat(cli=client)\n",
    "chat(\"Hi.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df35019",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25455cba",
   "metadata": {},
   "source": [
    "## Other providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70554b16",
   "metadata": {},
   "source": [
    "Here's an example of using the library with OpenRouter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273e85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "4\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: gen-1766063131-zacuMF6yJHRUIPUXB48G\n",
       "- created_at: 1766063131.0\n",
       "- error: None\n",
       "- incomplete_details: None\n",
       "- instructions: None\n",
       "- metadata: {}\n",
       "- model: openai/gpt-oss-20b\n",
       "- object: response\n",
       "- output: [ResponseReasoningItem(id='rs_tmp_u56eghazyi9', summary=[], type='reasoning', content=[Content(text='We need to answer: 2+2 = 4. Also maybe friendly.', type='reasoning_text')], encrypted_content=None, status=None), ResponseOutputMessage(id='msg_tmp_htb9r0aougc', content=[ResponseOutputText(annotations=[], text='4', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')]\n",
       "- parallel_tool_calls: True\n",
       "- temperature: None\n",
       "- tool_choice: auto\n",
       "- tools: []\n",
       "- top_p: None\n",
       "- background: False\n",
       "- conversation: None\n",
       "- max_output_tokens: 4096\n",
       "- max_tool_calls: None\n",
       "- previous_response_id: None\n",
       "- prompt: None\n",
       "- prompt_cache_key: None\n",
       "- prompt_cache_retention: None\n",
       "- reasoning: None\n",
       "- safety_identifier: None\n",
       "- service_tier: auto\n",
       "- status: None\n",
       "- text: None\n",
       "- top_logprobs: None\n",
       "- truncation: None\n",
       "- usage: ResponseUsage(input_tokens=75, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=28, output_tokens_details=OutputTokensDetails(reasoning_tokens=12), total_tokens=103, cost=9.35e-06, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_input_cost': 3.75e-06, 'upstream_inference_output_cost': 5.6e-06})\n",
       "- user: None\n",
       "- output_text: \n",
       "- store: False\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "Response(id='gen-1766063131-zacuMF6yJHRUIPUXB48G', created_at=1766063131.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='openai/gpt-oss-20b', object='response', output=[ResponseReasoningItem(id='rs_tmp_u56eghazyi9', summary=[], type='reasoning', content=[Content(text='We need to answer: 2+2 = 4. Also maybe friendly.', type='reasoning_text')], encrypted_content=None, status=None), ResponseOutputMessage(id='msg_tmp_htb9r0aougc', content=[ResponseOutputText(annotations=[], text='4', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=None, tool_choice='auto', tools=[], top_p=None, background=False, conversation=None, max_output_tokens=4096, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=None, safety_identifier=None, service_tier='auto', status=None, text=None, top_logprobs=None, truncation=None, usage=In: 75; Out: 28; Total: 103, user=None, output_text='', store=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openrouter_c = Client(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    api_key_env=\"OPENROUTER_API_KEY\",\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "openrouter_c(\"Hello! What's 2+2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bd6118",
   "metadata": {},
   "source": [
    "Here's an example of using the library with Groq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef103ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_c = Client(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    api_key_env=\"GROQ_KEY\",\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")\n",
    "\n",
    "groq_c(\"Hello! What's 2+2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cd0e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "gchat = Chat(cli=groq_c)\n",
    "gchat(\"Hello! I'm Jeremy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e92645",
   "metadata": {},
   "outputs": [],
   "source": [
    "gchat(\"What's my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec4289",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ee5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
