{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe78920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d773712-12fe-440e-891f-36f59666dfde",
   "metadata": {},
   "source": [
    "# Cosette's source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f6471-8061-4fdd-85a1-25fdc27c5cf3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore import imghdr\n",
    "from fastcore.utils import *\n",
    "from fastcore.meta import delegates\n",
    "\n",
    "import inspect, typing, mimetypes, base64, json, ast\n",
    "from collections import abc\n",
    "from random import choices\n",
    "from string import ascii_letters,digits\n",
    "\n",
    "from msglm import mk_msg_openai as mk_msg, mk_msgs_openai as mk_msgs\n",
    "\n",
    "from openai import types\n",
    "from openai import Completion,OpenAI,NOT_GIVEN,AzureOpenAI\n",
    "from openai.resources import chat\n",
    "from openai.resources.chat import Completions\n",
    "from openai.types.chat.chat_completion import ChatCompletion, ChatCompletionMessage\n",
    "from openai.types.completion_usage import CompletionUsage\n",
    "\n",
    "from toolslm.funccall import *\n",
    "\n",
    "try: from IPython import display\n",
    "except: display=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13866a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "_all_ = ['mk_msg', 'mk_msgs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa6b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "empty = inspect.Parameter.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e9290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models as of 2025-01-31:\n",
      "\n",
      "babbage-002                   chatgpt-4o-latest             dall-e-2                      \n",
      "dall-e-3                      davinci-002                   ft:gpt-4o-2024-08-06:answerai \n",
      "ft:gpt-4o-2024-08-06:answerai ft:gpt-4o-2024-08-06:answerai ft:gpt-4o-mini-2024-07-18:ans \n",
      "ft:gpt-4o-mini-2024-07-18:ans gpt-3.5-turbo                 gpt-3.5-turbo-0125            \n",
      "gpt-3.5-turbo-1106            gpt-3.5-turbo-16k             gpt-3.5-turbo-instruct        \n",
      "gpt-3.5-turbo-instruct-0914   gpt-4                         gpt-4-0125-preview            \n",
      "gpt-4-1106-preview            gpt-4-turbo                   gpt-4-turbo-2024-04-09        \n",
      "gpt-4-turbo-preview           gpt-4o                        gpt-4o-2024-05-13             \n",
      "gpt-4o-2024-08-06             gpt-4o-2024-11-20             gpt-4o-audio-preview          \n",
      "gpt-4o-audio-preview-2024-10- gpt-4o-audio-preview-2024-12- gpt-4o-mini                   \n",
      "gpt-4o-mini-2024-07-18        gpt-4o-mini-audio-preview     gpt-4o-mini-audio-preview-202 \n",
      "gpt-4o-mini-realtime-preview  gpt-4o-mini-realtime-preview- gpt-4o-realtime-preview       \n",
      "gpt-4o-realtime-preview-2024- gpt-4o-realtime-preview-2024- o1                            \n",
      "o1-2024-12-17                 o1-mini                       o1-mini-2024-09-12            \n",
      "o1-preview                    o1-preview-2024-09-12         o3-mini                       \n",
      "o3-mini-2025-01-31            omni-moderation-2024-09-26    omni-moderation-latest        \n",
      "text-embedding-3-large        text-embedding-3-small        text-embedding-ada-002        \n",
      "tts-1                         tts-1-1106                    tts-1-hd                      \n",
      "tts-1-hd-1106                 whisper-1                     \n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "\n",
    "def print_columns(items, cols=3, width=30):\n",
    "    for i in range(0, len(items), cols):\n",
    "        row = items[i:i+cols]\n",
    "        print(''.join(item[:width-1].ljust(width) for item in row))\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "models = client.models.list()\n",
    "print(f\"Available models as of {datetime.now().strftime('%Y-%m-%d')}:\\n\")\n",
    "print_columns(sorted([m.id for m in models]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a311e8d9",
   "metadata": {},
   "source": [
    "*NB* Since index into models is often hardcoded in consuming code, *always append newer entries to the end of the list* to avoid breaking code that consumes this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fff8869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "models = 'o1-preview', 'o1-mini', 'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-4', 'gpt-4-32k', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct', 'o1', 'o3-mini', 'chatgpt-4o-latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd0e86",
   "metadata": {},
   "source": [
    "`o1` should support images while `o1-preview`, `o1-mini`, `o3-mini` do not support images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f69208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "text_only_models = 'o1-preview', 'o1-mini', 'o3-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d965d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "has_streaming_models = set(models) - set(('o1', 'o1-mini', 'o3-mini'))\n",
    "has_system_prompt_models = set(models) - set(('o1-mini', 'o3-mini'))\n",
    "has_temperature_models = set(models) - set(('o1', 'o1-mini', 'o3-mini'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf2bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4o'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models[2]\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71985a9c",
   "metadata": {},
   "source": [
    "For examples, we'll use GPT-4o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d4d81",
   "metadata": {},
   "source": [
    "## OpenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b53a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli = OpenAI().chat.completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec40731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello, Jeremy! How can I assist you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: chatcmpl-AvrnGqI9TV3oTJuCLAL1ThXpqJptc\n",
       "- choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello, Jeremy! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))]\n",
       "- created: 1738354842\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: chat.completion\n",
       "- service_tier: default\n",
       "- system_fingerprint: fp_4691090a87\n",
       "- usage: CompletionUsage(completion_tokens=12, prompt_tokens=9, total_tokens=21, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
       "- _request_id: req_10a1b2a0e683163dfe746c6caecf97c9\n",
       "- __exclude_fields__: {'__exclude_fields__', '_request_id'}\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AvrnGqI9TV3oTJuCLAL1ThXpqJptc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello, Jeremy! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738354842, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_4691090a87', usage=In: 9; Out: 12; Total: 21)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = {'role': 'user', 'content': \"I'm Jeremy\"}\n",
    "r = cli.create(messages=[m], model=model, max_completion_tokens=100)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6359e1a",
   "metadata": {},
   "source": [
    "### Formatting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba620ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def find_block(r:abc.Mapping, # The message to look in\n",
    "              ):\n",
    "    \"Find the message in `r`.\"\n",
    "    m = nested_idx(r, 'choices', 0)\n",
    "    if not m: return m\n",
    "    if hasattr(m, 'message'): return m.message\n",
    "    return m.delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def contents(r):\n",
    "    \"Helper to get the contents from response `r`.\"\n",
    "    blk = find_block(r)\n",
    "    if not blk: return r\n",
    "    if hasattr(blk, 'content'): return getattr(blk,'content')\n",
    "    return blk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5f2107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, Jeremy! How can I assist you today?'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17924d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _repr_markdown_(self:ChatCompletion):\n",
    "    det = '\\n- '.join(f'{k}: {v}' for k,v in dict(self).items())\n",
    "    res = contents(self)\n",
    "    if not res: return f\"- {det}\"\n",
    "    return f\"\"\"{contents(self)}\n",
    "\n",
    "<details>\n",
    "\n",
    "- {det}\n",
    "\n",
    "</details>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd85eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello, Jeremy! How can I assist you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: chatcmpl-AvrnGqI9TV3oTJuCLAL1ThXpqJptc\n",
       "- choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello, Jeremy! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))]\n",
       "- created: 1738354842\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: chat.completion\n",
       "- service_tier: default\n",
       "- system_fingerprint: fp_4691090a87\n",
       "- usage: CompletionUsage(completion_tokens=12, prompt_tokens=9, total_tokens=21, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
       "- _request_id: req_10a1b2a0e683163dfe746c6caecf97c9\n",
       "- __exclude_fields__: {'__exclude_fields__', '_request_id'}\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AvrnGqI9TV3oTJuCLAL1ThXpqJptc', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello, Jeremy! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738354842, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_4691090a87', usage=In: 9; Out: 12; Total: 21)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7466f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 9; Out: 12; Total: 21"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a24f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def usage(inp=0, # Number of prompt tokens\n",
    "          out=0  # Number of completion tokens\n",
    "         ):\n",
    "    \"Slightly more concise version of `CompletionUsage`.\"\n",
    "    return CompletionUsage(prompt_tokens=inp, completion_tokens=out, total_tokens=inp+out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e52afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 5; Out: 0; Total: 5"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usage(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27468180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def __repr__(self:CompletionUsage): return f'In: {self.prompt_tokens}; Out: {self.completion_tokens}; Total: {self.total_tokens}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ca615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 9; Out: 12; Total: 21"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec303ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def __add__(self:CompletionUsage, b):\n",
    "    \"Add together each of `input_tokens` and `output_tokens`\"\n",
    "    return usage(self.prompt_tokens+b.prompt_tokens, self.completion_tokens+b.completion_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb0b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 18; Out: 24; Total: 42"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.usage+r.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307caa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def wrap_latex(text, md=True):\n",
    "    \"Replace OpenAI LaTeX codes with markdown-compatible ones\"\n",
    "    text = re.sub(r\"\\\\\\((.*?)\\\\\\)\", lambda o: f\"${o.group(1)}$\", text)\n",
    "    res = re.sub(r\"\\\\\\[(.*?)\\\\\\]\", lambda o: f\"$${o.group(1)}$$\", text, flags=re.DOTALL)\n",
    "    if md: res = display.Markdown(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0a43f",
   "metadata": {},
   "source": [
    "### Creating messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13954d58",
   "metadata": {},
   "source": [
    "Creating correctly formatted `dict`s from scratch every time isn't very handy, so we'll import a couple of helper functions from the `msglm` library.\n",
    "\n",
    "Let's use `mk_msg` to recreate our msg `{'role': 'user', 'content': \"I'm Jeremy\"}` from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f7705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi Jeremy! How can I assist you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: chatcmpl-AvrnIZ76kdH1gVP0KPGlpqaSPKzDS\n",
       "- choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hi Jeremy! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))]\n",
       "- created: 1738354844\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: chat.completion\n",
       "- service_tier: default\n",
       "- system_fingerprint: fp_4691090a87\n",
       "- usage: CompletionUsage(completion_tokens=11, prompt_tokens=9, total_tokens=20, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
       "- _request_id: req_671e207121e113e9adb369c57ca55d7c\n",
       "- __exclude_fields__: {'__exclude_fields__', '_request_id'}\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AvrnIZ76kdH1gVP0KPGlpqaSPKzDS', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hi Jeremy! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738354844, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_4691090a87', usage=In: 9; Out: 11; Total: 20)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"I'm Jeremy\"\n",
    "m = mk_msg(prompt)\n",
    "r = cli.create(messages=[m], model=model, max_completion_tokens=100)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7b389b",
   "metadata": {},
   "source": [
    "We can pass more than just text messages to OpenAI. As we'll see later we can also pass images, SDK objects, etc. To handle these different data types we need to pass the type along with our content to OpenAI. \n",
    "\n",
    "Here's an example of a multimodal message containing text and images. \n",
    "\n",
    "```json\n",
    "{\n",
    "    'role': 'user', \n",
    "    'content': [\n",
    "        {'type': 'text', 'text': 'What is in the image?'},\n",
    "        {'type': 'image_url', 'image_url': {'url': f'data:{MEDIA_TYPE};base64,{IMG}'}}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "`mk_msg` infers the type automatically and creates the appropriate data structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33194f",
   "metadata": {},
   "source": [
    "LLMs, don't actually have state, but instead dialogs are created by passing back all previous prompts and responses every time. With OpenAI, they always alternate *user* and *assistant*. We'll use `mk_msgs` from `msglm` to make it easier to build up these dialog lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef3715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"I'm Jeremy\"},\n",
       " ChatCompletionMessage(content='Hi Jeremy! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None),\n",
       " {'role': 'user', 'content': 'I forgot my name. Can you remind me please?'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs([prompt, r, \"I forgot my name. Can you remind me please?\"]) \n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c464f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Jeremy. How can I help you further?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: chatcmpl-AvrnJUxXCbN2g1vGiOGJgmSy4Pa4G\n",
       "- choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Your name is Jeremy. How can I help you further?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))]\n",
       "- created: 1738354845\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: chat.completion\n",
       "- service_tier: default\n",
       "- system_fingerprint: fp_4691090a87\n",
       "- usage: CompletionUsage(completion_tokens=13, prompt_tokens=38, total_tokens=51, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
       "- _request_id: req_2ff97a96015e781b8f0f116bd60defb7\n",
       "- __exclude_fields__: {'__exclude_fields__', '_request_id'}\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AvrnJUxXCbN2g1vGiOGJgmSy4Pa4G', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Your name is Jeremy. How can I help you further?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738354845, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_4691090a87', usage=In: 38; Out: 13; Total: 51)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cli.create(messages=msgs, model=model, max_completion_tokens=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f8a4d",
   "metadata": {},
   "source": [
    "## Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b873aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Client:\n",
    "    def __init__(self, model, cli=None):\n",
    "        \"Basic LLM messages client.\"\n",
    "        self.model,self.use = model,usage(0,0)\n",
    "        self.text_only = model in text_only_models\n",
    "        self.c = (cli or OpenAI()).chat.completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e9ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 0; Out: 0; Total: 0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Client(model)\n",
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "def _r(self:Client, r:ChatCompletion):\n",
    "    \"Store the result of the message and accrue total usage.\"\n",
    "    self.result = r\n",
    "    if getattr(r,'usage',None): self.use += r.usage\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181f7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 9; Out: 11; Total: 20"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c._r(r)\n",
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36029ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_stream(r):\n",
    "    for o in r:\n",
    "        o = contents(o)\n",
    "        if o and isinstance(o, str): yield(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb96c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(Completions.create)\n",
    "def __call__(self:Client,\n",
    "             msgs:list, # List of messages in the dialog\n",
    "             sp:str='', # System prompt\n",
    "             maxtok=4096, # Maximum tokens\n",
    "             stream:bool=False, # Stream response?\n",
    "             **kwargs):\n",
    "    \"Make a call to LLM.\"\n",
    "    if 'tools' in kwargs: assert not self.text_only, \"Tool use is not supported by the current model type.\"\n",
    "    if any(c['type'] == 'image_url' for msg in msgs if isinstance(msg, dict) and isinstance(msg.get('content'), list) for c in msg['content']): assert not self.text_only, \"Images are not supported by the current model type.\"\n",
    "    if stream: kwargs['stream_options'] = {\"include_usage\": True}\n",
    "    if self.model in has_system_prompt_models:\n",
    "        msgs = [mk_msg(sp, 'system')] + list(msgs)\n",
    "\n",
    "    r = self.c.create(\n",
    "        model=self.model, messages=msgs, max_completion_tokens=maxtok, stream=stream, **kwargs)\n",
    "    if not stream: return self._r(r)\n",
    "    else: return get_stream(map(self._r, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47195b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = [mk_msg('Hi')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a38e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello! How can I assist you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: chatcmpl-AvrnLik2SaNaHQM1IXrStnVFXWVEI\n",
       "- choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))]\n",
       "- created: 1738354847\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: chat.completion\n",
       "- service_tier: default\n",
       "- system_fingerprint: fp_50cad350e4\n",
       "- usage: CompletionUsage(completion_tokens=10, prompt_tokens=8, total_tokens=18, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
       "- _request_id: req_f4302a778667c07f4044baecfb5f2a1c\n",
       "- __exclude_fields__: {'__exclude_fields__', '_request_id'}\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AvrnLik2SaNaHQM1IXrStnVFXWVEI', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738354847, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_50cad350e4', usage=In: 8; Out: 10; Total: 18)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3a5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 17; Out: 21; Total: 38"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a92b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "for o in c(msgs, stream=True): print(o, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "In: 25; Out: 31; Total: 56"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7cdbc6",
   "metadata": {},
   "source": [
    "## Tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046e8cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sums(\n",
    "    a:int,  # First thing to sum\n",
    "    b:int # Second thing to sum\n",
    ") -> int: # The sum of the inputs\n",
    "    \"Adds a + b.\"\n",
    "    print(f\"Finding the sum of {a} and {b}\")\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88420769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_openai_func(f): \n",
    "    sc = get_schema(f, 'parameters')\n",
    "    sc['parameters'].pop('title', None)\n",
    "    return dict(type='function', function=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tool_choice(f): return dict(type='function', function={'name':f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcab8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sysp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters, at minimum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = 604542,6458932\n",
    "pr = f\"What is {a}+{b}?\"\n",
    "tools=[mk_openai_func(sums)]\n",
    "tool_choice=mk_tool_choice(\"sums\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cad8f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "- id: chatcmpl-AvrnNSGqHjn1SA17vdOi1nN56coHr\n",
       "- choices: [Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_J1rMfaAWXvNuT5pcsuPuwVNo', function=Function(arguments='{\"a\":604542,\"b\":6458932}', name='sums'), type='function')]))]\n",
       "- created: 1738354849\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: chat.completion\n",
       "- service_tier: default\n",
       "- system_fingerprint: fp_50cad350e4\n",
       "- usage: CompletionUsage(completion_tokens=22, prompt_tokens=94, total_tokens=116, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
       "- _request_id: req_2f7d17c5b61a00698bc40f9c4cb91107\n",
       "- __exclude_fields__: {'__exclude_fields__', '_request_id'}"
      ],
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AvrnNSGqHjn1SA17vdOi1nN56coHr', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_J1rMfaAWXvNuT5pcsuPuwVNo', function=Function(arguments='{\"a\":604542,\"b\":6458932}', name='sums'), type='function')]))], created=1738354849, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_50cad350e4', usage=In: 94; Out: 22; Total: 116)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = [mk_msg(pr)]\n",
    "r = c(msgs, sp=sysp, tools=tools)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9394b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_J1rMfaAWXvNuT5pcsuPuwVNo', function=Function(arguments='{\"a\":604542,\"b\":6458932}', name='sums'), type='function')])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = find_block(r)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6488b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='call_J1rMfaAWXvNuT5pcsuPuwVNo', function=Function(arguments='{\"a\":604542,\"b\":6458932}', name='sums'), type='function')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc = m.tool_calls\n",
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc3933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Function(arguments='{\"a\":604542,\"b\":6458932}', name='sums')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = tc[0].function\n",
    "func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3616cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def call_func_openai(func:types.chat.chat_completion_message_tool_call.Function, ns:Optional[abc.Mapping]=None):\n",
    "    return call_func(func.name, ast.literal_eval(func.arguments), ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b506ec11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542 and 6458932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7063474"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = mk_ns(sums)\n",
    "res = call_func_openai(func, ns=ns)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d475922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def mk_toolres(\n",
    "    r:abc.Mapping, # Tool use request response\n",
    "    ns:Optional[abc.Mapping]=None, # Namespace to search for tools\n",
    "    obj:Optional=None # Class to search for tools\n",
    "    ):\n",
    "    \"Create a `tool_result` message from response `r`.\"\n",
    "    r = mk_msg(r)\n",
    "    tcs = getattr(r, 'tool_calls', [])\n",
    "    res = [r]\n",
    "    if ns is None: ns = globals()\n",
    "    if obj is not None: ns = mk_ns(obj)\n",
    "    for tc in (tcs or []):\n",
    "        func = tc.function\n",
    "        cts = str(call_func_openai(func, ns=ns))\n",
    "        res.append(mk_msg(str(cts), 'tool', tool_call_id=tc.id, name=func.name))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13de1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542 and 6458932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_J1rMfaAWXvNuT5pcsuPuwVNo', function=Function(arguments='{\"a\":604542,\"b\":6458932}', name='sums'), type='function')]),\n",
       " {'role': 'tool',\n",
       "  'content': '7063474',\n",
       "  'tool_call_id': 'call_J1rMfaAWXvNuT5pcsuPuwVNo',\n",
       "  'name': 'sums'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = mk_toolres(r, ns=ns)\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc83c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs += tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed99502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sum of 604542 and 6458932 is 7063474.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: chatcmpl-AvrnPGK3mzUlVD2xh6FKquLbpx7cG\n",
       "- choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The sum of 604542 and 6458932 is 7063474.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))]\n",
       "- created: 1738354851\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: chat.completion\n",
       "- service_tier: default\n",
       "- system_fingerprint: fp_50cad350e4\n",
       "- usage: CompletionUsage(completion_tokens=19, prompt_tokens=126, total_tokens=145, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
       "- _request_id: req_c5bf38bf41fa485320cd6c2ac1db9491\n",
       "- __exclude_fields__: {'__exclude_fields__', '_request_id'}\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AvrnPGK3mzUlVD2xh6FKquLbpx7cG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The sum of 604542 and 6458932 is 7063474.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738354851, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_50cad350e4', usage=In: 126; Out: 19; Total: 145)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = c(msgs, sp=sysp, tools=tools)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a53a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummy:\n",
    "    def sums(\n",
    "        self,\n",
    "        a:int,  # First thing to sum\n",
    "        b:int=1 # Second thing to sum\n",
    "    ) -> int: # The sum of the inputs\n",
    "        \"Adds a + b.\"\n",
    "        print(f\"Finding the sum of {a} and {b}\")\n",
    "        return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff70d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello Jeremy! How can I assist you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: chatcmpl-AvrnQAnJhaoLpWrn4GpryGwO1awrD\n",
       "- choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello Jeremy! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))]\n",
       "- created: 1738354852\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: chat.completion\n",
       "- service_tier: default\n",
       "- system_fingerprint: fp_50cad350e4\n",
       "- usage: CompletionUsage(completion_tokens=12, prompt_tokens=106, total_tokens=118, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
       "- _request_id: req_0bb1f5835ad93ec9a36f7fb458238cca\n",
       "- __exclude_fields__: {'__exclude_fields__', '_request_id'}\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AvrnQAnJhaoLpWrn4GpryGwO1awrD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello Jeremy! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738354852, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_50cad350e4', usage=In: 106; Out: 12; Total: 118)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [mk_openai_func(Dummy.sums)]\n",
    "\n",
    "o = Dummy()\n",
    "msgs = mk_toolres(\"I'm Jeremy\")\n",
    "r = c(msgs, sp=sysp, tools=tools)\n",
    "msgs += mk_toolres(r, obj=o)\n",
    "res = c(msgs, sp=sysp, tools=tools)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93b087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"I'm Jeremy\"},\n",
       " ChatCompletionMessage(content='Hello Jeremy! How can I assist you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e6290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542 and 6458932\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The sum of 604,542 and 6,458,932 is 7,063,474.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: chatcmpl-AvrnScUsY8LTk9Iib4DckwqXPq0EM\n",
       "- choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The sum of 604,542 and 6,458,932 is 7,063,474.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))]\n",
       "- created: 1738354854\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: chat.completion\n",
       "- service_tier: default\n",
       "- system_fingerprint: fp_50cad350e4\n",
       "- usage: CompletionUsage(completion_tokens=24, prompt_tokens=132, total_tokens=156, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
       "- _request_id: req_0f6083e22a500b5c90c6571ae31991bb\n",
       "- __exclude_fields__: {'__exclude_fields__', '_request_id'}\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AvrnScUsY8LTk9Iib4DckwqXPq0EM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The sum of 604,542 and 6,458,932 is 7,063,474.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738354854, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_50cad350e4', usage=In: 132; Out: 24; Total: 156)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [mk_openai_func(Dummy.sums)]\n",
    "\n",
    "o = Dummy()\n",
    "msgs = mk_toolres(pr)\n",
    "r = c(msgs, sp=sysp, tools=tools)\n",
    "msgs += mk_toolres(r, obj=o)\n",
    "res = c(msgs, sp=sysp, tools=tools)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b9b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def _mock_id(): return 'call_' + ''.join(choices(ascii_letters+digits, k=24))\n",
    "\n",
    "def mock_tooluse(name:str, # The name of the called function\n",
    "                 res,  # The result of calling the function\n",
    "                 **kwargs): # The arguments to the function\n",
    "    \"\"\n",
    "    id = _mock_id()\n",
    "    func = dict(arguments=json.dumps(kwargs), name=name)\n",
    "    tc = dict(id=id, function=func, type='function')\n",
    "    req = dict(content=None, role='assistant', tool_calls=[tc])\n",
    "    resp = mk_msg('' if res is None else str(res), 'tool', tool_call_id=id, name=name)\n",
    "    return [req,resp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f693c",
   "metadata": {},
   "source": [
    "This function mocks the messages needed to implement tool use, for situations where you want to insert tool use messages into a dialog without actually calling into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcee006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sum of 604542 and 6458932 is 7063474.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: chatcmpl-AvrnUvxuwOdBQNSuSX0z168YaraXz\n",
       "- choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The sum of 604542 and 6458932 is 7063474.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))]\n",
       "- created: 1738354856\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: chat.completion\n",
       "- service_tier: default\n",
       "- system_fingerprint: fp_50cad350e4\n",
       "- usage: CompletionUsage(completion_tokens=19, prompt_tokens=111, total_tokens=130, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
       "- _request_id: req_b043700b108987819f58c52aa8dcaeb4\n",
       "- __exclude_fields__: {'__exclude_fields__', '_request_id'}\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AvrnUvxuwOdBQNSuSX0z168YaraXz', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The sum of 604542 and 6458932 is 7063474.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738354856, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_50cad350e4', usage=In: 111; Out: 19; Total: 130)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tu = mock_tooluse(name='sums', res=7063474, a=604542, b=6458932)\n",
    "r = c([mk_msg(pr)]+tu, tools=tools)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffaa969",
   "metadata": {},
   "source": [
    "Structured outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3679879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(Client.__call__)\n",
    "def structured(self:Client,\n",
    "               msgs: list, # Prompt\n",
    "               tools:Optional[list]=None, # List of tools to make available to OpenAI model\n",
    "               obj:Optional=None, # Class to search for tools\n",
    "               ns:Optional[abc.Mapping]=None, # Namespace to search for tools\n",
    "               **kwargs):\n",
    "    \"Return the value of all tool calls (generally used for structured outputs)\"\n",
    "    tools = listify(tools)\n",
    "    if ns is None: ns=mk_ns(*tools)\n",
    "    tools = [mk_openai_func(o) for o in tools]\n",
    "    if obj is not None: ns = mk_ns(obj)\n",
    "    res = self(msgs, tools=tools, tool_choice='required', **kwargs)\n",
    "    cts = getattr(res, 'choices', [])\n",
    "    tcs = [call_func_openai(t.function, ns=ns) for o in cts for t in (o.message.tool_calls or [])]\n",
    "    return tcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1648f208",
   "metadata": {},
   "source": [
    "OpenAI's API doesn't natively support response formats, so we introduce a `structured` method to handle tool calling for this purpose. In this setup, the tool's result is sent directly to the user without being passed back to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f40c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the sum of 604542 and 6458932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7063474]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.structured(mk_msgs(pr), tools=[sums])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea144b8",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d1edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Chat:\n",
    "    def __init__(self,\n",
    "                 model:Optional[str]=None, # Model to use (leave empty if passing `cli`)\n",
    "                 cli:Optional[Client]=None, # Client to use (leave empty if passing `model`)\n",
    "                 sp='', # Optional system prompt\n",
    "                 tools:Optional[list]=None,  # List of tools to make available\n",
    "                 tool_choice:Optional[str]=None): # Forced tool choice\n",
    "        \"OpenAI chat client.\"\n",
    "        assert model or cli\n",
    "        self.c = (cli or Client(model))\n",
    "        self.h,self.sp,self.tools,self.tool_choice = [],sp,tools,tool_choice\n",
    "    \n",
    "    @property\n",
    "    def use(self): return self.c.use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b837c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(In: 0; Out: 0; Total: 0, [])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = \"Never mention what tools you use.\"\n",
    "chat = Chat(model, sp=sp)\n",
    "chat.c.use, chat.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403539e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "@patch\n",
    "@delegates(Completions.create)\n",
    "def __call__(self:Chat,\n",
    "             pr=None,  # Prompt / message\n",
    "             stream:bool=False, # Stream response?\n",
    "             **kwargs):\n",
    "    \"Add prompt `pr` to dialog and get a response\"\n",
    "    if isinstance(pr,str): pr = pr.strip()\n",
    "    if pr: self.h.append(mk_msg(pr))\n",
    "    if self.tools: kwargs['tools'] = [mk_openai_func(o) for o in self.tools]\n",
    "    if self.tool_choice: kwargs['tool_choice'] = mk_tool_choice(self.tool_choice)\n",
    "    res = self.c(self.h, sp=self.sp, stream=stream, **kwargs)\n",
    "    self.h += mk_toolres(res, ns=self.tools)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40073f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Jeremy. How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: chatcmpl-AvrnXAdhUOt5M7h2gEJUsUMf50riN\n",
       "- choices: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Your name is Jeremy. How can I help you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))]\n",
       "- created: 1738354859\n",
       "- model: gpt-4o-2024-08-06\n",
       "- object: chat.completion\n",
       "- service_tier: default\n",
       "- system_fingerprint: fp_4691090a87\n",
       "- usage: CompletionUsage(completion_tokens=13, prompt_tokens=42, total_tokens=55, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
       "- _request_id: req_5cbcbd38bb3471e07a7d4f0e27df587c\n",
       "- __exclude_fields__: {'__exclude_fields__', '_request_id'}\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ChatCompletion(id='chatcmpl-AvrnXAdhUOt5M7h2gEJUsUMf50riN', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Your name is Jeremy. How can I help you today?', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1738354859, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_4691090a87', usage=In: 42; Out: 13; Total: 55)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"I'm Jeremy\")\n",
    "chat(\"What's my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529104ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Jeremy! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "chat = Chat(model, sp=sp)\n",
    "for o in chat(\"I'm Jeremy\", stream=True):\n",
    "    o = contents(o)\n",
    "    if o and isinstance(o, str): print(o, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78a53a",
   "metadata": {},
   "source": [
    "Check that the o1 reasoning model works and compare 4o default to o1 behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9828dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Answer:\n",
      "1233 * 4297 = 5298201\n",
      "\n",
      "gpt-4o Answer:\n",
      "1233 multiplied by 4297 equals 5,295,801.\n",
      "\n",
      "o-1 Answer:\n",
      "5,298,201\n"
     ]
    }
   ],
   "source": [
    "chat = Chat(model, sp=sp)\n",
    "chat_o1 = Chat(\"o1\", sp=sp)\n",
    "problem = \"1233 * 4297\"\n",
    "print(f\"Correct Answer:\\n{problem} = {eval(problem)}\")\n",
    "\n",
    "print(\"\\ngpt-4o Answer:\")\n",
    "r = chat(f\"what is {problem}?\")\n",
    "print(contents(r))\n",
    "\n",
    "print(\"\\no-1 Answer:\")\n",
    "r = chat_o1(f\"what is {problem}?\")\n",
    "print(contents(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b40892f",
   "metadata": {},
   "source": [
    "### Chat tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6535cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is 604542+6458932?'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = f\"What is {a}+{b}?\"\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797741c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(model, sp=sp, tools=[sums])\n",
    "r = chat(pr)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1dd4c3",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf741dbc",
   "metadata": {},
   "source": [
    "As everyone knows, when testing image APIs you have to use a cute puppy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image is Cute_dog.jpg from Wikimedia\n",
    "fn = Path('samples/puppy.jpg')\n",
    "display.Image(filename=fn, width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d120d8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = fn.read_bytes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18ee71",
   "metadata": {},
   "source": [
    "OpenAI expects an image message to have the following structure\n",
    "\n",
    "```js\n",
    "{\n",
    "  \"type\": \"image_url\",\n",
    "  \"image_url\": {\n",
    "    \"url\": f\"data:{MEDIA_TYPE};base64,{IMG}\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "`msglm` automatically detects if a message is an image, encodes it, and generates the data structure above.\n",
    "All we need to do is a create a list containing our image and a query and then pass it to `mk_msg`.\n",
    "\n",
    "Let's try it out..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0eed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"In brief, what color flowers are in this image?\"\n",
    "msg = [mk_msg(img), mk_msg(q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Chat(model)\n",
    "c([img, q])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc18d6c",
   "metadata": {},
   "source": [
    "# Third Party Providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf7626b",
   "metadata": {},
   "source": [
    "## Azure OpenAI Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80455a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "models_azure = ('gpt-4o', 'gpt-4-32k', 'gpt4-1106-preview', 'gpt-35-turbo', 'gpt-35-turbo-16k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e056a52",
   "metadata": {},
   "source": [
    "Example Azure usage:\n",
    "```\n",
    "azure_endpoint = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "client = Client(models_azure[0], azure_endpoint)\n",
    "chat = Chat(cli=client)\n",
    "chat(\"I'm Faisal\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec4289",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ee5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|eval: false\n",
    "from nbdev.doclinks import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f9715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
