[
  {
    "objectID": "toolloop.html",
    "href": "toolloop.html",
    "title": "Tool loop",
    "section": "",
    "text": "from IPython.display import display, Markdown, clear_output\nfrom pprint import pprint\n' '.join(models)\n\n'gpt-5 gpt-5-mini gpt-5-nano o1-preview o1-mini gpt-4o gpt-4o-mini gpt-4-turbo gpt-4 gpt-4-32k gpt-3.5-turbo gpt-3.5-turbo-instruct o1 o3-mini chatgpt-4o-latest o1-pro o3 o4-mini gpt-4.1 gpt-4.1-mini gpt-4.1-nano'\nmodel = first(m for m in models if 'mini' in m)\nmodel\n\n'gpt-5-mini'",
    "crumbs": [
      "Tool loop"
    ]
  },
  {
    "objectID": "toolloop.html#sample-data",
    "href": "toolloop.html#sample-data",
    "title": "Tool loop",
    "section": "Sample Data",
    "text": "Sample Data\n\ndef _get_orders_customers():\n    orders = {\n        \"O1\": dict(id=\"O1\", product=\"Widget A\", quantity=2, price=19.99, status=\"Shipped\"),\n        \"O2\": dict(id=\"O2\", product=\"Gadget B\", quantity=1, price=49.99, status=\"Processing\"),\n        \"O3\": dict(id=\"O3\", product=\"Gadget B\", quantity=2, price=49.99, status=\"Shipped\")}\n\n    customers = {\n        \"C1\": dict(name=\"John Doe\", email=\"john@example.com\", phone=\"123-456-7890\",\n                   orders=[orders['O1'], orders['O2']]),\n        \"C2\": dict(name=\"Jane Smith\", email=\"jane@example.com\", phone=\"987-654-3210\",\n                   orders=[orders['O3']])\n    }\n    return orders, customers\n\n\norders, customers = _get_orders_customers()\n\n\ndef get_customer_info(\n    customer_id:str # ID of the customer\n): # Customer's name, email, phone number, and list of orders\n    \"Retrieves a customer's information and their orders based on the customer ID\"\n    print(f'- Retrieving customer {customer_id}')\n    return customers.get(customer_id, \"Customer not found\")\n\ndef get_order_details(\n    order_id:str # ID of the order\n): # Order's ID, product name, quantity, price, and order status\n    \"Retrieves the details of a specific order based on the order ID\"\n    print(f'- Retrieving order {order_id}')\n    return orders.get(order_id, \"Order not found\")\n\ndef cancel_order(\n    order_id:str # ID of the order to cancel\n)-&gt;bool: # True if the cancellation is successful\n    \"Cancels an order based on the provided order ID\"\n    print(f'- Cancelling order {order_id}')\n    if order_id not in orders: return False\n    orders[order_id]['status'] = 'Cancelled'\n    return True\n\n\nchatkw = dict(\n    text={ \"verbosity\": \"low\" },\n    reasoning={ \"effort\": \"minimal\" }\n)\n\n\ntools = [get_customer_info, get_order_details, cancel_order]\nchat = Chat(model, tools=tools, **chatkw)\n\n\nr = chat('Hi.')\nr\n\nHello! How can I help you today?\n\n\nid: resp_0610e51711a17c8b006943fce162a0819396584fc43c687fba\ncreated_at: 1766063329.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0610e51711a17c8b006943fce1b7e08193a742dd433b62cbcb’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0610e51711a17c8b006943fce1dfc081939b2f726e067bd88c’, content=[ResponseOutputText(annotations=[], text=‘Hello! How can I help you today?’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘get_customer_info’, parameters={‘type’: ‘object’, ‘properties’: {‘customer_id’: {‘type’: ‘string’, ‘description’: ‘ID of the customer’}}, ‘required’: [‘customer_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=“Retrieves a customer’s information and their orders based on the customer ID”), FunctionTool(name=‘get_order_details’, parameters={‘type’: ‘object’, ‘properties’: {‘order_id’: {‘type’: ‘string’, ‘description’: ‘ID of the order’}}, ‘required’: [‘order_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Retrieves the details of a specific order based on the order ID’), FunctionTool(name=‘cancel_order’, parameters={‘type’: ‘object’, ‘properties’: {‘order_id’: {‘type’: ‘string’, ‘description’: ‘ID of the order to cancel’}}, ‘required’: [‘order_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Cancels an order based on the provided order ID:- type: boolean’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=136, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=15, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=151)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True\n\n\n\n\n\nr = chat('Can you tell me the email address for customer C2?')\nr.output\n\n- Retrieving customer C2\n\n\n[ResponseReasoningItem(id='rs_0610e51711a17c8b006943fce2fb408193a0298d1762a4b19e', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"customer_id\":\"C2\"}', call_id='call_1iQyH2m7zBT6AxtxpVfgOARS', name='get_customer_info', type='function_call', id='fc_0610e51711a17c8b006943fce34518819385d6cbeb29b1f63f', status='completed')]\n\n\n\nr = chat()\nr.output\n\n[ResponseOutputMessage(id='msg_0610e51711a17c8b006943fce4a59c8193b7914069038e07b0', content=[ResponseOutputText(annotations=[], text='The email address for customer C2 is jane@example.com.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n\n\n\nchat = Chat(model, tools=tools)\nr = chat('Please cancel all orders for customer C1 for me.')\nr.output\n\n- Retrieving customer C1\n\n\n[ResponseReasoningItem(id='rs_067a83d17b75c4ea006943fce594008194904f7f4b1710ab11', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"customer_id\":\"C1\"}', call_id='call_5MXLQEl4LdzFyRHIR3DynD9I', name='get_customer_info', type='function_call', id='fc_067a83d17b75c4ea006943fce6c9848194b442cb73ffe02035', status='completed')]\n\n\n\nr = chat()\nr.output\n\n- Cancelling order O1\n- Cancelling order O2\n\n\n[ResponseReasoningItem(id='rs_067a83d17b75c4ea006943fce7b6648194aa8afa477dc81c42', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"order_id\":\"O1\"}', call_id='call_y3LWRVEn8X80nCQf50EOVOu5', name='cancel_order', type='function_call', id='fc_067a83d17b75c4ea006943fce992f88194b8de77d411f78e44', status='completed'),\n ResponseFunctionToolCall(arguments='{\"order_id\":\"O2\"}', call_id='call_oPiTCOxwXjo8uac0mzXK3rEv', name='cancel_order', type='function_call', id='fc_067a83d17b75c4ea006943fce9f498819495f077de035b7454', status='completed')]",
    "crumbs": [
      "Tool loop"
    ]
  },
  {
    "objectID": "toolloop.html#toolloop-implementation",
    "href": "toolloop.html#toolloop-implementation",
    "title": "Tool loop",
    "section": "toolloop implementation",
    "text": "toolloop implementation\n\nsource\n\nChat.toolloop\n\ndef toolloop(\n    pr, # Prompt to pass to Claude\n    max_steps:int=10, # Maximum number of tool requests to loop through\n    cont_func:callable=noop, # Function that stops loop if returns False\n    final_prompt:str='You have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.', # Prompt to add if last message is a tool call\n    stream:bool=False, # Stream response?\n    tools:NoneType=None, # Tools to use\n    tool_choice:NoneType=None, # Required tools to use\n    background:Optional[bool] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    conversation:Optional[response_create_params.Conversation] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    include:Optional[List[ResponseIncludable]] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    input:Union[str, ResponseInputParam] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    instructions:Optional[str] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    max_output_tokens:Optional[int] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    max_tool_calls:Optional[int] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    metadata:Optional[Metadata] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    model:ResponsesModel | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    parallel_tool_calls:Optional[bool] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    previous_response_id:Optional[str] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    prompt:Optional[ResponsePromptParam] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    prompt_cache_key:str | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    prompt_cache_retention:Optional[Literal['in-memory', '24h']] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    reasoning:Optional[Reasoning] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    safety_identifier:str | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    service_tier:Optional[Literal['auto', 'default', 'flex', 'scale', 'priority']] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    store:Optional[bool] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    stream_options:Optional[response_create_params.StreamOptions] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    temperature:Optional[float] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    text:ResponseTextConfigParam | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    top_logprobs:Optional[int] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    top_p:Optional[float] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    truncation:Optional[Literal['auto', 'disabled']] | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    user:str | Omit=&lt;openai.Omit object at 0x7f58df64bd40&gt;,\n    extra_headers:Headers | None=None, # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\nThe extra values given here take precedence over values defined on the client or passed to this method.\n    extra_query:Query | None=None, extra_body:Body | None=None,\n    timeout:float | httpx.Timeout | None | NotGiven=NOT_GIVEN\n):\n\nAdd prompt pr to dialog and get a response from Claude, automatically following up with tool_use messages\n\n\nExported source\n_final_prompt = \"You have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.\"\n\n\n\n\nExported source\n@patch\n@delegates(Chat.__call__)\ndef toolloop(self:Chat,\n             pr, # Prompt to pass to Claude\n             max_steps=10, # Maximum number of tool requests to loop through\n             cont_func:callable=noop, # Function that stops loop if returns False\n             final_prompt=_final_prompt, # Prompt to add if last message is a tool call\n             **kwargs):\n    \"Add prompt `pr` to dialog and get a response from Claude, automatically following up with `tool_use` messages\"\n    @save_iter\n    def _f(o):\n        init_n = len(self.h)\n        r = self(pr, **kwargs)\n        yield r\n        if len(self.last)&gt;1: yield from self.last[1:]\n        for i in range(max_steps-1):\n            x = self.h[-1]\n            if not (isinstance(x, dict) and x['type']=='function_call_output'): break\n            r = self(final_prompt if i==max_steps-2 else None, **kwargs)\n            yield r\n            if len(self.last)&gt;1: yield from self.last[1:]\n            if not cont_func(*self.h[-3:]): break\n        o.value = self.h[init_n+1:]\n    return _f()\n\n\n\n\nTest Customer Dataset\n\ndef show(x):\n    if getattr(x, 'output_text', None): r = x\n    else: r = getattr(x,'output',x)\n    display(r)\n\n\nchat = Chat(model, tools=tools)\npr = 'Can you tell me the email address for customer C1?'\nr = chat.toolloop(pr)\nres = list(r)\nfor o in r: show(o)\n\n- Retrieving customer C1\n\n\nThe email address for customer C1 (John Doe) is john@example.com.\n\n\nid: resp_0787bac936d9204f006943fcee2ba48195ad60e3c1ec52d1d7\ncreated_at: 1766063342.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0787bac936d9204f006943fcee7cd881959b82edf88db5e70d’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0787bac936d9204f006943fcef3ee88195832b056ec436b34f’, content=[ResponseOutputText(annotations=[], text=‘The email address for customer C1 (John Doe) is john@example.com.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘get_customer_info’, parameters={‘type’: ‘object’, ‘properties’: {‘customer_id’: {‘type’: ‘string’, ‘description’: ‘ID of the customer’}}, ‘required’: [‘customer_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=“Retrieves a customer’s information and their orders based on the customer ID”), FunctionTool(name=‘get_order_details’, parameters={‘type’: ‘object’, ‘properties’: {‘order_id’: {‘type’: ‘string’, ‘description’: ‘ID of the order’}}, ‘required’: [‘order_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Retrieves the details of a specific order based on the order ID’), FunctionTool(name=‘cancel_order’, parameters={‘type’: ‘object’, ‘properties’: {‘order_id’: {‘type’: ‘string’, ‘description’: ‘ID of the order to cancel’}}, ‘required’: [‘order_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Cancels an order based on the provided order ID:- type: boolean’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘medium’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=316, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=86, output_tokens_details=OutputTokensDetails(reasoning_tokens=64), total_tokens=402)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True\n\n\n\n\nResponseOutputMessage(id='msg_0787bac936d9204f006943fcef3ee88195832b056ec436b34f', content=[ResponseOutputText(annotations=[], text='The email address for customer C1 (John Doe) is john@example.com.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')\n\n\n\nsource\n\n\nloop_outputs\n\ndef loop_outputs(\n    res\n):\n\n\n\nExported source\ndef loop_outputs(res):\n    return [dict(p) for o in res for p in ([o] if isinstance(o,dict) else getattr(o,'output',[]))]\n\n\n\ncl = loop_outputs(res)\ncl\n\n[{'id': 'rs_0787bac936d9204f006943fceb6f088195a44ade2155966909',\n  'summary': [],\n  'type': 'reasoning',\n  'content': None,\n  'encrypted_content': None,\n  'status': None},\n {'arguments': '{\"customer_id\":\"C1\"}',\n  'call_id': 'call_wqbYqGHnvgMg8lSY9JMUrwzU',\n  'name': 'get_customer_info',\n  'type': 'function_call',\n  'id': 'fc_0787bac936d9204f006943fcebe6d08195b9cd3ca97a01b7d2',\n  'status': 'completed'},\n {'type': 'function_call_output',\n  'call_id': 'call_wqbYqGHnvgMg8lSY9JMUrwzU',\n  'output': \"{'name': 'John Doe', 'email': 'john@example.com', 'phone': '123-456-7890', 'orders': [{'id': 'O1', 'product': 'Widget A', 'quantity': 2, 'price': 19.99, 'status': 'Cancelled'}, {'id': 'O2', 'product': 'Gadget B', 'quantity': 1, 'price': 49.99, 'status': 'Cancelled'}]}\"},\n {'id': 'rs_0787bac936d9204f006943fcec9c108195b41ad9735c482bc4',\n  'summary': [],\n  'type': 'reasoning',\n  'content': None,\n  'encrypted_content': None,\n  'status': None},\n {'id': 'msg_0787bac936d9204f006943fced6e3c8195b5b4c9710ecb6485',\n  'content': [ResponseOutputText(annotations=[], text='The email address for customer C1 (John Doe) is john@example.com.', type='output_text', logprobs=[])],\n  'role': 'assistant',\n  'status': 'completed',\n  'type': 'message'}]\n\n\n\ndef disp_tc(x):\n    if x['type']=='function_call': return f\"- `{x['name']}({x['arguments']})`\\n\"\n    elif x['type']=='function_call_output': return f\"  - `{x['output']}`\\n\\n\"\n    else: return ''.join(o.text for o in x['content'])\n\n\n# Markdown(''.join(map(disp_tc, cl)))\n\n\npprint(r.value)\n\n[ResponseReasoningItem(id='rs_0787bac936d9204f006943fcee7cd881959b82edf88db5e70d', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseOutputMessage(id='msg_0787bac936d9204f006943fcef3ee88195832b056ec436b34f', content=[ResponseOutputText(annotations=[], text='The email address for customer C1 (John Doe) is john@example.com.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n\n\n\norders, customers = _get_orders_customers()\n\n\nchat = Chat(model, tools=tools)\nr = chat.toolloop('What is the status of order O2?')\nfor o in r: display(getattr(o,'output',o))\n\n- Retrieving order O2\n\n\n[ResponseReasoningItem(id='rs_080cc194cfa17c94006943fcf092688194adde78fdb74e90d3', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"order_id\":\"O2\"}', call_id='call_ARnBd6xSrlcAfn3wBQGdLtiu', name='get_order_details', type='function_call', id='fc_080cc194cfa17c94006943fcf1ed8c8194adc4f0474afe98cc', status='completed')]\n\n\nResponseFunctionToolCall(arguments='{\"order_id\":\"O2\"}', call_id='call_ARnBd6xSrlcAfn3wBQGdLtiu', name='get_order_details', type='function_call', id='fc_080cc194cfa17c94006943fcf1ed8c8194adc4f0474afe98cc', status='completed')\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_ARnBd6xSrlcAfn3wBQGdLtiu',\n 'output': \"{'id': 'O2', 'product': 'Gadget B', 'quantity': 1, 'price': 49.99, 'status': 'Processing'}\"}\n\n\n[ResponseOutputMessage(id='msg_080cc194cfa17c94006943fcf3101081948ac3faafe2dcac65', content=[ResponseOutputText(annotations=[], text='Order O2 is currently: Processing.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n\n\n\nr = chat.toolloop('Please cancel all orders for customer C1 for me.')\nres = list(r)\nfor o in res: display(getattr(o,'output',o))\n\n- Retrieving customer C1\n- Cancelling order O1\n- Cancelling order O2\n\n\n[ResponseReasoningItem(id='rs_080cc194cfa17c94006943fcf412208194a4c7b87ce81c4736', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"customer_id\":\"C1\"}', call_id='call_Btl5F6UrkBoJJaDQzke1VvVN', name='get_customer_info', type='function_call', id='fc_080cc194cfa17c94006943fcf82ba08194a65b676e302e9aa6', status='completed')]\n\n\nResponseFunctionToolCall(arguments='{\"customer_id\":\"C1\"}', call_id='call_Btl5F6UrkBoJJaDQzke1VvVN', name='get_customer_info', type='function_call', id='fc_080cc194cfa17c94006943fcf82ba08194a65b676e302e9aa6', status='completed')\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_Btl5F6UrkBoJJaDQzke1VvVN',\n 'output': \"{'name': 'John Doe', 'email': 'john@example.com', 'phone': '123-456-7890', 'orders': [{'id': 'O1', 'product': 'Widget A', 'quantity': 2, 'price': 19.99, 'status': 'Shipped'}, {'id': 'O2', 'product': 'Gadget B', 'quantity': 1, 'price': 49.99, 'status': 'Processing'}]}\"}\n\n\n[ResponseReasoningItem(id='rs_080cc194cfa17c94006943fcf913188194ab017b95ba08a13e', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"order_id\":\"O1\"}', call_id='call_4yaVagsUWe86YMk25kyCU9x2', name='cancel_order', type='function_call', id='fc_080cc194cfa17c94006943fcfa8bd081949731f7da8c5adc86', status='completed'),\n ResponseFunctionToolCall(arguments='{\"order_id\":\"O2\"}', call_id='call_Vy0mCP8ocOYR31qk4sbVgRw5', name='cancel_order', type='function_call', id='fc_080cc194cfa17c94006943fcfab9688194a97ce8bb7ff46706', status='completed')]\n\n\nResponseFunctionToolCall(arguments='{\"order_id\":\"O1\"}', call_id='call_4yaVagsUWe86YMk25kyCU9x2', name='cancel_order', type='function_call', id='fc_080cc194cfa17c94006943fcfa8bd081949731f7da8c5adc86', status='completed')\n\n\nResponseFunctionToolCall(arguments='{\"order_id\":\"O2\"}', call_id='call_Vy0mCP8ocOYR31qk4sbVgRw5', name='cancel_order', type='function_call', id='fc_080cc194cfa17c94006943fcfab9688194a97ce8bb7ff46706', status='completed')\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_4yaVagsUWe86YMk25kyCU9x2',\n 'output': 'True'}\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_Vy0mCP8ocOYR31qk4sbVgRw5',\n 'output': 'True'}\n\n\n[ResponseReasoningItem(id='rs_080cc194cfa17c94006943fcfbd120819490c3ad1d75350865', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseOutputMessage(id='msg_080cc194cfa17c94006943fd043f948194ad1bb58d51d2de71', content=[ResponseOutputText(annotations=[], text='Done — I canceled all orders for customer C1 (John Doe).\\n\\nSummary:\\n- O1 — Widget A — previous status: Shipped — cancellation: Success\\n- O2 — Gadget B — previous status: Processing — cancellation: Success\\n\\nWould you like me to check refund status, send a confirmation to john@example.com, or do anything else?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n\n\nResponseOutputMessage(id='msg_080cc194cfa17c94006943fd043f948194ad1bb58d51d2de71', content=[ResponseOutputText(annotations=[], text='Done — I canceled all orders for customer C1 (John Doe).\\n\\nSummary:\\n- O1 — Widget A — previous status: Shipped — cancellation: Success\\n- O2 — Gadget B — previous status: Processing — cancellation: Success\\n\\nWould you like me to check refund status, send a confirmation to john@example.com, or do anything else?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')\n\n\n\n# cl = loop_outputs(res)\n# Markdown('\\n'.join(map(disp_tc, cl)))\n\n\nfor o in chat.toolloop('What is the status of order O2?'): display(o)\n\n- Retrieving order O2\n\n\n\nid: resp_080cc194cfa17c94006943fd06606c8194acbf56e9291c70ca\ncreated_at: 1766063366.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_080cc194cfa17c94006943fd06bbd4819499041c0e2a896d43’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseFunctionToolCall(arguments=‘{“order_id”:“O2”}’, call_id=‘call_47iqPnLAFKxbJ4JJ9EQwWw71’, name=‘get_order_details’, type=‘function_call’, id=‘fc_080cc194cfa17c94006943fd07c3308194a643843cb1720a9e’, status=‘completed’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘get_customer_info’, parameters={‘type’: ‘object’, ‘properties’: {‘customer_id’: {‘type’: ‘string’, ‘description’: ‘ID of the customer’}}, ‘required’: [‘customer_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=“Retrieves a customer’s information and their orders based on the customer ID”), FunctionTool(name=‘get_order_details’, parameters={‘type’: ‘object’, ‘properties’: {‘order_id’: {‘type’: ‘string’, ‘description’: ‘ID of the order’}}, ‘required’: [‘order_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Retrieves the details of a specific order based on the order ID’), FunctionTool(name=‘cancel_order’, parameters={‘type’: ‘object’, ‘properties’: {‘order_id’: {‘type’: ‘string’, ‘description’: ‘ID of the order to cancel’}}, ‘required’: [‘order_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Cancels an order based on the provided order ID:- type: boolean’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘medium’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=521, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=87, output_tokens_details=OutputTokensDetails(reasoning_tokens=64), total_tokens=608)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True\n\n\n\nResponseFunctionToolCall(arguments='{\"order_id\":\"O2\"}', call_id='call_47iqPnLAFKxbJ4JJ9EQwWw71', name='get_order_details', type='function_call', id='fc_080cc194cfa17c94006943fd07c3308194a643843cb1720a9e', status='completed')\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_47iqPnLAFKxbJ4JJ9EQwWw71',\n 'output': \"{'id': 'O2', 'product': 'Gadget B', 'quantity': 1, 'price': 49.99, 'status': 'Cancelled'}\"}\n\n\nOrder O2 is now: Cancelled.\n\n\nid: resp_080cc194cfa17c94006943fd0841e88194b12aa8c8f30b4e64\ncreated_at: 1766063368.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseOutputMessage(id=‘msg_080cc194cfa17c94006943fd08ad488194a093653ee36021db’, content=[ResponseOutputText(annotations=[], text=‘Order O2 is now: Cancelled.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘get_customer_info’, parameters={‘type’: ‘object’, ‘properties’: {‘customer_id’: {‘type’: ‘string’, ‘description’: ‘ID of the customer’}}, ‘required’: [‘customer_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=“Retrieves a customer’s information and their orders based on the customer ID”), FunctionTool(name=‘get_order_details’, parameters={‘type’: ‘object’, ‘properties’: {‘order_id’: {‘type’: ‘string’, ‘description’: ‘ID of the order’}}, ‘required’: [‘order_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Retrieves the details of a specific order based on the order ID’), FunctionTool(name=‘cancel_order’, parameters={‘type’: ‘object’, ‘properties’: {‘order_id’: {‘type’: ‘string’, ‘description’: ‘ID of the order to cancel’}}, ‘required’: [‘order_id’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Cancels an order based on the provided order ID:- type: boolean’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘medium’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=676, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=13, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=689)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True\n\n\n\n\n\n\nTest Math Example\n\ndef add(x: int, y: int) -&gt; int:\n    \"adds x and y.\"\n    return x + y\n\ndef mul(x: int, y: int) -&gt; int:\n    \"multiplies x and y.\"\n    return x * y\n\n\nchat = Chat(model, tools=[add, mul], **chatkw)\npr = 'Can you add 1258585825128 to 34959234595, multiply by 93, and then add (-12439149)?'\nr = chat.toolloop(pr)\nfor o in r: show(o)\n\n[ResponseReasoningItem(id='rs_02621606c3b513dc006943fd09ad40819781fb7b32d5e5996a', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"x\":1258585825128,\"y\":34959234595}', call_id='call_Fg3v2kfgWIogSl1IEB0w1Y0K', name='add', type='function_call', id='fc_02621606c3b513dc006943fd09fdf881979ba71de45af8d588', status='completed')]\n\n\nResponseFunctionToolCall(arguments='{\"x\":1258585825128,\"y\":34959234595}', call_id='call_Fg3v2kfgWIogSl1IEB0w1Y0K', name='add', type='function_call', id='fc_02621606c3b513dc006943fd09fdf881979ba71de45af8d588', status='completed')\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_Fg3v2kfgWIogSl1IEB0w1Y0K',\n 'output': '1293545059723'}\n\n\n[ResponseFunctionToolCall(arguments='{\"x\":1293545059723,\"y\":93}', call_id='call_9rimpAnaVInF4ssbHosAfTi8', name='mul', type='function_call', id='fc_02621606c3b513dc006943fd0bae9c81978026c7bfaae4a759', status='completed')]\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_9rimpAnaVInF4ssbHosAfTi8',\n 'output': '120299690554239'}\n\n\n[ResponseFunctionToolCall(arguments='{\"x\":120299690554239,\"y\":-12439149}', call_id='call_OS43dlWKEYtYAocZpHJ94RXM', name='add', type='function_call', id='fc_02621606c3b513dc006943fd0c9f5c8197a11bc188b0174d6d', status='completed')]\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_OS43dlWKEYtYAocZpHJ94RXM',\n 'output': '120299678115090'}\n\n\n120299678115090\n\n\nid: resp_02621606c3b513dc006943fd0d30f48197a911787dca9e5d00\ncreated_at: 1766063373.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseOutputMessage(id=‘msg_02621606c3b513dc006943fd0d80f88197b58e845951e295e1’, content=[ResponseOutputText(annotations=[], text=‘120299678115090’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘add’, parameters={‘type’: ‘object’, ‘properties’: {‘x’: {‘type’: ‘integer’, ‘description’: ’‘}, ’y’: {‘type’: ‘integer’, ‘description’: ’‘}}, ’required’: [‘x’, ‘y’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘adds x and y.:- type: integer’), FunctionTool(name=‘mul’, parameters={‘type’: ‘object’, ‘properties’: {‘x’: {‘type’: ‘integer’, ‘description’: ’‘}, ’y’: {‘type’: ‘integer’, ‘description’: ’‘}}, ’required’: [‘x’, ‘y’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘multiplies x and y.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=250, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=9, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=259)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True\n\n\n\n\n\n(1258585825128 + 34959234595) * 93 - 12439149\n\n120299678115090\n\n\n\nchat = Chat(model, tools=[add, mul], **chatkw)\nr = chat.toolloop(pr, stream=True)\nfor o in r:\n    if isinstance(o, dict): print('- ', o)\n    else:\n        for p in o: print(p, end='')\n        if hasattr(o, 'value'): show(o.value)\n\n[ResponseReasoningItem(id='rs_0944024f48105103006943fd116be8819483df3a9f25e3d328', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"x\":1258585825128,\"y\":34959234595}', call_id='call_B6xGJDpQJm5GrS2hnngybWTe', name='add', type='function_call', id='fc_0944024f48105103006943fd11abcc81949200a4eabf12e97b', status='completed')]\n\n\n('arguments', '{\"x\":1258585825128,\"y\":34959234595}')('call_id', 'call_B6xGJDpQJm5GrS2hnngybWTe')('name', 'add')('type', 'function_call')('id', 'fc_0944024f48105103006943fd11abcc81949200a4eabf12e97b')('status', 'completed')-  {'type': 'function_call_output', 'call_id': 'call_B6xGJDpQJm5GrS2hnngybWTe', 'output': '1293545059723'}\n\n\n[ResponseFunctionToolCall(arguments='{\"x\":1293545059723,\"y\":93}', call_id='call_Vb3v1awZxvoZuUjbq9RRZrfM', name='mul', type='function_call', id='fc_0944024f48105103006943fd1402188194a2adccf222c76591', status='completed')]\n\n\n-  {'type': 'function_call_output', 'call_id': 'call_Vb3v1awZxvoZuUjbq9RRZrfM', 'output': '120299690554239'}\n\n\n[ResponseFunctionToolCall(arguments='{\"x\":120299690554239,\"y\":-12439149}', call_id='call_Dd1Uzt8rMfWJCH0irbMa5OVV', name='add', type='function_call', id='fc_0944024f48105103006943fd14e54481948906bd955b405418', status='completed')]\n\n\n-  {'type': 'function_call_output', 'call_id': 'call_Dd1Uzt8rMfWJCH0irbMa5OVV', 'output': '120299678115090'}\n120299678115090\n\n\n120299678115090\n\n\nid: resp_0944024f48105103006943fd1583ec8194863012f06a95bb9a\ncreated_at: 1766063381.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseOutputMessage(id=‘msg_0944024f48105103006943fd16463c81949df34dd0158af4c1’, content=[ResponseOutputText(annotations=[], text=‘120299678115090’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘add’, parameters={‘type’: ‘object’, ‘properties’: {‘x’: {‘type’: ‘integer’, ‘description’: ’‘}, ’y’: {‘type’: ‘integer’, ‘description’: ’‘}}, ’required’: [‘x’, ‘y’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘adds x and y.:- type: integer’), FunctionTool(name=‘mul’, parameters={‘type’: ‘object’, ‘properties’: {‘x’: {‘type’: ‘integer’, ‘description’: ’‘}, ’y’: {‘type’: ‘integer’, ‘description’: ’‘}}, ’required’: [‘x’, ‘y’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘multiplies x and y.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=250, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=9, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=259)\nuser: None\nstore: True\n\n\n\n\n\n\nError Conditions: Out of Iterations, Exception During Tool Invocation\n\ndef mydiv(a:float, b:float):\n    \"Divide two numbers\"\n    return a / b\n\n\nchat = Chat(model, tools=[mydiv], **chatkw)\nr = chat.toolloop('Please calculate this sequence using your tools: 43/23454; 652/previous result; 6843/previous result; 321/previous result', max_steps=2)\nfor o in r: show(o)\n\n[ResponseReasoningItem(id='rs_02f9e0725e9fb6a5006943fd16f3dc81979a3aa89dfcb98aca', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"a\":43,\"b\":23454}', call_id='call_Fnsc5iNP8rMuj0rqeBLzXVsO', name='mydiv', type='function_call', id='fc_02f9e0725e9fb6a5006943fd177c6c8197a5ad34f31755933a', status='completed'),\n ResponseFunctionToolCall(arguments='{\"a\":652,\"b\":0}', call_id='call_k7APDmfK4dy57LeiHYhBCNq6', name='mydiv', type='function_call', id='fc_02f9e0725e9fb6a5006943fd17b4d08197ada7efed9792c679', status='completed'),\n ResponseFunctionToolCall(arguments='{\"a\":6843,\"b\":0}', call_id='call_ZpzLBH2zM3Wcy9aoFCbnZ0mN', name='mydiv', type='function_call', id='fc_02f9e0725e9fb6a5006943fd17e5bc8197b3e6f2903db6ce9d', status='completed'),\n ResponseFunctionToolCall(arguments='{\"a\":321,\"b\":0}', call_id='call_JGS2KeOU7ku3n56kqbSrvEGy', name='mydiv', type='function_call', id='fc_02f9e0725e9fb6a5006943fd181dd88197bd6492c3465327ea', status='completed')]\n\n\nResponseFunctionToolCall(arguments='{\"a\":43,\"b\":23454}', call_id='call_Fnsc5iNP8rMuj0rqeBLzXVsO', name='mydiv', type='function_call', id='fc_02f9e0725e9fb6a5006943fd177c6c8197a5ad34f31755933a', status='completed')\n\n\nResponseFunctionToolCall(arguments='{\"a\":652,\"b\":0}', call_id='call_k7APDmfK4dy57LeiHYhBCNq6', name='mydiv', type='function_call', id='fc_02f9e0725e9fb6a5006943fd17b4d08197ada7efed9792c679', status='completed')\n\n\nResponseFunctionToolCall(arguments='{\"a\":6843,\"b\":0}', call_id='call_ZpzLBH2zM3Wcy9aoFCbnZ0mN', name='mydiv', type='function_call', id='fc_02f9e0725e9fb6a5006943fd17e5bc8197b3e6f2903db6ce9d', status='completed')\n\n\nResponseFunctionToolCall(arguments='{\"a\":321,\"b\":0}', call_id='call_JGS2KeOU7ku3n56kqbSrvEGy', name='mydiv', type='function_call', id='fc_02f9e0725e9fb6a5006943fd181dd88197bd6492c3465327ea', status='completed')\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_Fnsc5iNP8rMuj0rqeBLzXVsO',\n 'output': '0.001833375969983798'}\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_k7APDmfK4dy57LeiHYhBCNq6',\n 'output': 'Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.12/site-packages/toolslm/funccall.py\", line 215, in call_func\\n    try: return func(**inps)\\n                ^^^^^^^^^^^^\\n  File \"/tmp/ipykernel_5385/246724137.py\", line 3, in mydiv\\n    return a / b\\n           ~~^~~\\nZeroDivisionError: division by zero\\n'}\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_ZpzLBH2zM3Wcy9aoFCbnZ0mN',\n 'output': 'Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.12/site-packages/toolslm/funccall.py\", line 215, in call_func\\n    try: return func(**inps)\\n                ^^^^^^^^^^^^\\n  File \"/tmp/ipykernel_5385/246724137.py\", line 3, in mydiv\\n    return a / b\\n           ~~^~~\\nZeroDivisionError: division by zero\\n'}\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_JGS2KeOU7ku3n56kqbSrvEGy',\n 'output': 'Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.12/site-packages/toolslm/funccall.py\", line 215, in call_func\\n    try: return func(**inps)\\n                ^^^^^^^^^^^^\\n  File \"/tmp/ipykernel_5385/246724137.py\", line 3, in mydiv\\n    return a / b\\n           ~~^~~\\nZeroDivisionError: division by zero\\n'}\n\n\nI computed the first division successfully: - 43 / 23454 = 0.001833375969983798\nI attempted the next steps but they failed because I tried to divide by zero (I passed 0 as the “previous result” for subsequent operations), causing errors. To complete the sequence you want, I need to perform these successive calculations using the preceding result each time: 1. 652 / (43/23454) 2. 6843 / (result of step 2) 3. 321 / (result of step 3)\nIf you want, I can now: - Recompute the chain without tool limits and give all four results, or - Compute them step-by-step here directly (no tools needed). Which do you prefer?\n\n\nid: resp_02f9e0725e9fb6a5006943fd18bff081979e13bb5930ee32b7\ncreated_at: 1766063384.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_02f9e0725e9fb6a5006943fd19334c8197b4b20beb1ae09493’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_02f9e0725e9fb6a5006943fd195a9c81978fb9bcca07f80c87’, content=[ResponseOutputText(annotations=[], text=‘I computed the first division successfully:- 43 / 23454 = 0.001833375969983798attempted the next steps but they failed because I tried to divide by zero (I passed 0 as the “previous result” for subsequent operations), causing errors. To complete the sequence you want, I need to perform these successive calculations using the preceding result each time:. 652 / (43/23454). 6843 / (result of step 2). 321 / (result of step 3)you want, I can now:- Recompute the chain without tool limits and give all four results, or- Compute them step-by-step here directly (no tools needed). Which do you prefer?’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘mydiv’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘number’, ‘description’: ’‘}, ’b’: {‘type’: ‘number’, ‘description’: ’‘}}, ’required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Divide two numbers’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=537, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=163, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=700)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True\n\n\n\n\nResponseOutputMessage(id='msg_02f9e0725e9fb6a5006943fd195a9c81978fb9bcca07f80c87', content=[ResponseOutputText(annotations=[], text='I computed the first division successfully:\\n- 43 / 23454 = 0.001833375969983798\\n\\nI attempted the next steps but they failed because I tried to divide by zero (I passed 0 as the “previous result” for subsequent operations), causing errors. To complete the sequence you want, I need to perform these successive calculations using the preceding result each time:\\n1. 652 / (43/23454)\\n2. 6843 / (result of step 2)\\n3. 321 / (result of step 3)\\n\\nIf you want, I can now:\\n- Recompute the chain without tool limits and give all four results, or\\n- Compute them step-by-step here directly (no tools needed). Which do you prefer?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')\n\n\nThis tests raise_on_err=False change to toolslm.call_func invocation. We should see this return an error as a string instead of crash:\n\nchat = Chat(model, tools=[mydiv], **chatkw)\nr = chat.toolloop('Try dividing 1 by 0 and see what the error result is')\nfor o in r: show(o)\n\n[ResponseReasoningItem(id='rs_0af57bb05fb6f746006943fd1bd07c81958a319ad4cb70eae8', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"a\":1,\"b\":0}', call_id='call_0FGtqnXi4rLuPC0ivZcaRw24', name='mydiv', type='function_call', id='fc_0af57bb05fb6f746006943fd1c29a08195989bf0c4f670baca', status='completed')]\n\n\nResponseFunctionToolCall(arguments='{\"a\":1,\"b\":0}', call_id='call_0FGtqnXi4rLuPC0ivZcaRw24', name='mydiv', type='function_call', id='fc_0af57bb05fb6f746006943fd1c29a08195989bf0c4f670baca', status='completed')\n\n\n{'type': 'function_call_output',\n 'call_id': 'call_0FGtqnXi4rLuPC0ivZcaRw24',\n 'output': 'Traceback (most recent call last):\\n  File \"/usr/local/lib/python3.12/site-packages/toolslm/funccall.py\", line 215, in call_func\\n    try: return func(**inps)\\n                ^^^^^^^^^^^^\\n  File \"/tmp/ipykernel_5385/246724137.py\", line 3, in mydiv\\n    return a / b\\n           ~~^~~\\nZeroDivisionError: division by zero\\n'}\n\n\nZeroDivisionError: division by zero\n\n\nid: resp_0af57bb05fb6f746006943fd1cbdac8195b5d480c66c146b06\ncreated_at: 1766063388.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseOutputMessage(id=‘msg_0af57bb05fb6f746006943fd1d335881958929318eb6ad6c40’, content=[ResponseOutputText(annotations=[], text=‘ZeroDivisionError: division by zero’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘mydiv’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘number’, ‘description’: ’‘}, ’b’: {‘type’: ‘number’, ‘description’: ’‘}}, ’required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Divide two numbers’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=198, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=209)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True",
    "crumbs": [
      "Tool loop"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "cosette",
    "section": "",
    "text": "pip install cosette",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "cosette",
    "section": "",
    "text": "pip install cosette",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "cosette",
    "section": "Getting started",
    "text": "Getting started\nOpenAI’s Python SDK will automatically be installed with Cosette, if you don’t already have it.\n\nfrom cosette import *\n\nCosette only exports the symbols that are needed to use the library, so you can use import * to import them. Alternatively, just use:\nimport cosette\n…and then add the prefix cosette. to any usages of the module.\nCosette provides models, which is a list of models currently available from the SDK.\n\n' '.join(models)\n\nFor these examples, we’ll use GPT-5-mini.\n\nmodel = first(m for m in models if 'mini' in m)",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#chat",
    "href": "index.html#chat",
    "title": "cosette",
    "section": "Chat",
    "text": "Chat\nThe main interface to Cosette is the Chat class, which provides a stateful interface to the models. You can pass message keywords to either Chat or when you call the model.\n\nchatkw = dict(\n    text={ \"verbosity\": \"low\" },\n    reasoning={ \"effort\": \"minimal\" }\n)\n\n\nchat = Chat(model, sp=\"You are a helpful and concise assistant.\", **chatkw)\nchat(\"I'm Jeremy\")\n\nNice to meet you, Jeremy. How can I help you today?\n\n\nid: resp_0d3d0fd9abbe53b800691bb8f36a8081a287420e10bd559959\ncreated_at: 1763424499.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful and concise assistant.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0d3d0fd9abbe53b800691bb8f3f96481a2b973102cdd2f71ae’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0d3d0fd9abbe53b800691bb8f42c2081a2844a8c1c40c9e319’, content=[ResponseOutputText(annotations=[], text=‘Nice to meet you, Jeremy. How can I help you today?’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=20, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=40)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\n\nr = chat(\"What's my name?\")\nr\n\nYour name is Jeremy.\n\n\nid: resp_0d3d0fd9abbe53b800691bb8f4f94481a2810c8643c0b3d40a\ncreated_at: 1763424500.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful and concise assistant.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0d3d0fd9abbe53b800691bb8f60dfc81a29e49561b99db8e9f’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0d3d0fd9abbe53b800691bb8f63d2881a29391dc42f032a3ed’, content=[ResponseOutputText(annotations=[], text=‘Your name is Jeremy.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=48, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=59)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\nAs you see above, displaying the results of a call in a notebook shows just the message contents, with the other details hidden behind a collapsible section. Alternatively you can print the details:\n\nprint(r)\n\nResponse(id='resp_0d3d0fd9abbe53b800691bb8f4f94481a2810c8643c0b3d40a', created_at=1763424500.0, error=None, incomplete_details=None, instructions='You are a helpful and concise assistant.', metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_0d3d0fd9abbe53b800691bb8f60dfc81a29e49561b99db8e9f', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0d3d0fd9abbe53b800691bb8f63d2881a29391dc42f032a3ed', content=[ResponseOutputText(annotations=[], text='Your name is Jeremy.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=4096, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 48; Out: 11; Total: 59, user=None, billing={'payer': 'developer'}, store=True)\n\n\nYou can use stream=True to stream the results as soon as they arrive (although you will only see the gradual generation if you execute the notebook yourself, of course!)\n\nfor o in chat(\"What's your name?\", stream=True): print(o, end='')\n\nI'm ChatGPT.",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#model-capabilities",
    "href": "index.html#model-capabilities",
    "title": "cosette",
    "section": "Model Capabilities",
    "text": "Model Capabilities\nDifferent OpenAI models have different capabilities. Some models such as o1-mini do not have support for streaming, system prompts, or temperature. Query these capbilities using these functions:\n\n# o1 does not support streaming or setting the temperature\ncan_stream('o1'), can_set_sp('o1'), can_set_temp('o1')\n\n(True, True, False)\n\n\n\n# gpt-4o has these capabilities\ncan_stream('gpt-4o'), can_set_sp('gpt-4o'), can_set_temp('gpt-4o')\n\n(True, True, True)",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#tool-use",
    "href": "index.html#tool-use",
    "title": "cosette",
    "section": "Tool use",
    "text": "Tool use\nTool use lets the model use external tools.\nWe use docments to make defining Python functions as ergonomic as possible. Each parameter (and the return value) should have a type, and a docments comment with the description of what it is. As an example we’ll write a simple function that adds numbers together, and will tell us when it’s being called:\n\ndef sums(\n    a:int,  # First thing to sum\n    b:int=1 # Second thing to sum\n) -&gt; int: # The sum of the inputs\n    \"Adds a + b.\"\n    print(f\"Finding the sum of {a} and {b}\")\n    return a + b\n\nSometimes the model will say something like “according to the sums tool the answer is” – generally we’d rather it just tells the user the answer, so we can use a system prompt to help with this:\n\nsp = \"Never mention what tools you use.\"\n\nWe’ll get the model to add up some long numbers:\n\na,b = 604542,6458932\npr = f\"What is {a}+{b}?\"\npr\n\n'What is 604542+6458932?'\n\n\nTo use tools, pass a list of them to Chat:\n\nchat = Chat(model, sp=sp, tools=[sums], **chatkw)\n\nNow when we call that with our prompt, the model doesn’t return the answer, but instead returns a tool_use message, which means we have to call the named tool with the provided parameters:\n\nr = chat(pr)\nr.output\n\nFinding the sum of 604542 and 6458932\n\n\n[ResponseReasoningItem(id='rs_0e80f673f989086700691bb8f93f808196a2d8be5fabc7ab32', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_G8jd6G7UGQtVFcJHiCYfLN8b', name='sums', type='function_call', id='fc_0e80f673f989086700691bb8f9f3dc81969e7bfce86b2e8cdc', status='completed')]\n\n\nCosette handles all that for us – we just have to pass along the message, and it all happens automatically:\n\nchat()\n\n7,063,474\n\n\nid: resp_0e80f673f989086700691bb8fb04ec8196854dd7610c320811\ncreated_at: 1763424507.0\nerror: None\nincomplete_details: None\ninstructions: Never mention what tools you use.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseOutputMessage(id=‘msg_0e80f673f989086700691bb8fbaf4081969d32466017fe56b9’, content=[ResponseOutputText(annotations=[], text=‘7,063,474’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘sums’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘integer’, ‘description’: ‘First thing to sum’}, ‘b’: {‘type’: ‘integer’, ‘description’: ‘Second thing to sum’, ‘default’: 1}}, ‘required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Adds a + b.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=142, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=9, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=151)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\nYou can see how many tokens have been used at any time by checking the use property.\n\nchat.use\n\nIn: 231; Out: 36; Total: 267\n\n\n\nTool loop\n\ndef show(x):\n    if getattr(x, 'output_text', None): display(x)\n\nWe can do everything needed to use tools in a single step, by using Chat.toolloop. This can even call multiple tools as needed solve a problem. For example, let’s define a tool to handle multiplication:\n\ndef mults(\n    a:int,  # First thing to multiply\n    b:int=1 # Second thing to multiply\n) -&gt; int: # The product of the inputs\n    \"Multiplies a * b.\"\n    print(f\"Finding the product of {a} and {b}\")\n    return a * b\n\nNow with a single call we can calculate (a+b)*2:\n\nchat = Chat(model, tools=[sums,mults], **chatkw)\npr = f'Calculate ({a}+{b})*2 and display the result as US$'\npr\n\n'Calculate (604542+6458932)*2 and display the result as US$'\n\n\n\nr = chat.toolloop(pr)\n\n\nfor o in r: show(o)\n\nFinding the sum of 604542 and 6458932\nFinding the product of 7063474 and 2\n\n\nUS$14,126,948\n\n\nid: resp_0b2a84d51ad623c400691bb900c6d08195b323fdb870a37bec\ncreated_at: 1763424512.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseOutputMessage(id=‘msg_0b2a84d51ad623c400691bb9014e088195b6bf52d37fafebf7’, content=[ResponseOutputText(annotations=[], text=‘US$14,126,948’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘sums’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘integer’, ‘description’: ‘First thing to sum’}, ‘b’: {‘type’: ‘integer’, ‘description’: ‘Second thing to sum’, ‘default’: 1}}, ‘required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Adds a + b.:- type: integer’), FunctionTool(name=‘mults’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘integer’, ‘description’: ‘First thing to multiply’}, ‘b’: {‘type’: ‘integer’, ‘description’: ‘Second thing to multiply’, ‘default’: 1}}, ‘required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Multiplies a * b.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=225, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=236)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#images",
    "href": "index.html#images",
    "title": "cosette",
    "section": "Images",
    "text": "Images\nAs everyone knows, when testing image APIs you have to use a cute puppy.\n\nfn = Path('samples/puppy.jpg')\nImage(filename=fn, width=200)\n\n\n\n\n\n\n\n\nWe create a Chat object as before:\n\nchat = Chat(model, **chatkw)\n\nClaudia expects images as a list of bytes, so we read in the file:\n\nimg = fn.read_bytes()\n\nPrompts to Claudia can be lists, containing text, images, or both, eg:\n\nchat([img, \"In brief, what color flowers are in this image?\"])\n\nThey are purple.\n\n\nid: resp_07fbd059d2633f7000691bb902fd98819194e5dbe9d2bb9909\ncreated_at: 1763424515.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_07fbd059d2633f7000691bb90390cc819181e8ec1955d1f7ed’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_07fbd059d2633f7000691bb903d1b081918d272dbb0556c7d0’, content=[ResponseOutputText(annotations=[], text=‘They are purple.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=102, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=10, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=112)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\nThe image is included as input tokens.\n\nchat.use\n\nIn: 102; Out: 10; Total: 112\n\n\nAlternatively, Cosette supports creating a multi-stage chat with separate image and text prompts. For instance, you can pass just the image as the initial prompt (in which case the model will make some general comments about what it sees), and then follow up with questions in additional prompts:\n\nchat = Chat(model, **chatkw)\nchat(img)\n\nThis is a small puppy lying on grass next to purple flowers. It has white fur with brown patches, long floppy ears, and dark eyes—appears to be a young spaniel-type dog (e.g., Cavalier King Charles Spaniel).\n\n\nid: resp_04ca2ec79ffca3d500691bb904744c81958f7812565de27daa\ncreated_at: 1763424516.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_04ca2ec79ffca3d500691bb904d1fc8195a696751f6ac10748’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_04ca2ec79ffca3d500691bb90503008195ac8efa68f28eccd1’, content=[ResponseOutputText(annotations=[], text=‘This is a small puppy lying on grass next to purple flowers. It has white fur with brown patches, long floppy ears, and dark eyes—appears to be a young spaniel-type dog (e.g., Cavalier King Charles Spaniel).’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=92, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=56, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=148)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\n\nchat('What direction is the puppy facing?')\n\nThe puppy is facing toward the camera (front-facing).\n\n\nid: resp_04ca2ec79ffca3d500691bb906d068819599b648197ed3ff12\ncreated_at: 1763424518.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_04ca2ec79ffca3d500691bb9078c048195861906ac7c42938a’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_04ca2ec79ffca3d500691bb907b5188195a62b8261e5561acb’, content=[ResponseOutputText(annotations=[], text=‘The puppy is facing toward the camera (front-facing).’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=159, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=17, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=176)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\n\nchat('What color is it?')\n\nThe puppy is mostly white with brown patches (especially on the ears and around the eyes).\n\n\nid: resp_04ca2ec79ffca3d500691bb90888b8819596a6c054a1750183\ncreated_at: 1763424520.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_04ca2ec79ffca3d500691bb9093cc48195baa2c98e9038941c’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_04ca2ec79ffca3d500691bb90965b48195ba6d501e5be77817’, content=[ResponseOutputText(annotations=[], text=‘The puppy is mostly white with brown patches (especially on the ears and around the eyes).’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=185, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=24, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=209)\nuser: None\nbilling: {‘payer’: ‘developer’}\nstore: True\n\n\n\n\nNote that the image is passed in again for every input in the dialog, so that number of input tokens increases quickly with this kind of chat.\n\nchat.use\n\nIn: 436; Out: 97; Total: 533",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "index.html#other-providers",
    "href": "index.html#other-providers",
    "title": "cosette",
    "section": "Other providers",
    "text": "Other providers\nHere’s an example of using the library with Groq:\n\ngroq_c = Client(\n    model=\"openai/gpt-oss-20b\",\n    api_key_env=\"GROQ_KEY\",\n    base_url=\"https://api.groq.com/openai/v1\"\n)\n\ngroq_c(\"Hello! What's 2+2?\")\n\nSure! 2 + 2 equals 4.\n\n\nid: resp_01kaa4wr25fb9sg6j2t0h74115\ncreated_at: 1763424755.0\nerror: None\nincomplete_details: None\ninstructions:\nmetadata: {}\nmodel: openai/gpt-oss-20b\nobject: response\noutput: [ResponseReasoningItem(id=‘resp_01kaa4wr25fba9v3q7sqdmnmhr’, summary=[], type=‘reasoning’, content=[Content(text=‘We need to respond as ChatGPT with a friendly answer: 2+2=4. Ensure no policy conflicts.’, type=‘reasoning_text’)], encrypted_content=None, status=‘completed’), ResponseOutputMessage(id=‘msg_01kaa4wr25fbas7kjb537b2yw1’, content=[ResponseOutputText(annotations=[], text=‘Sure! 202f+02f2 equals 4.’, type=‘output_text’, logprobs=None)], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=None)\ntop_logprobs: None\ntruncation: disabled\nusage: ResponseUsage(input_tokens=79, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=47, output_tokens_details=OutputTokensDetails(reasoning_tokens=25), total_tokens=126)\nuser: None\nstore: False\n\n\n\n\n\ngchat = Chat(cli=groq_c)\ngchat(\"Hello! I'm Jeremy\")\n\nHello Jeremy! How can I help you today?\n\n\nid: resp_01kaa4wsaze4ms5ezjrezw1baa\ncreated_at: 1763424757.0\nerror: None\nincomplete_details: None\ninstructions:\nmetadata: {}\nmodel: openai/gpt-oss-20b\nobject: response\noutput: [ResponseReasoningItem(id=‘resp_01kaa4wsaze4n9hze8pq6vvv75’, summary=[], type=‘reasoning’, content=[Content(text=‘User: “Hello! I'm Jeremy” We can greet. Possibly ask how can help.’, type=‘reasoning_text’)], encrypted_content=None, status=‘completed’), ResponseOutputMessage(id=‘msg_01kaa4wsaze4nsb4xcv927zww0’, content=[ResponseOutputText(annotations=[], text=‘Hello Jeremy! How can I help you today?’, type=‘output_text’, logprobs=None)], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=None)\ntop_logprobs: None\ntruncation: disabled\nusage: ResponseUsage(input_tokens=75, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=38, output_tokens_details=OutputTokensDetails(reasoning_tokens=19), total_tokens=113)\nuser: None\nstore: False\n\n\n\n\n\ngchat(\"What's my name?\")\n\nYou’re Jeremy!\n\n\nid: resp_01kaa4wtmcfbrb3fn5vzpd2fvy\ncreated_at: 1763424758.0\nerror: None\nincomplete_details: None\ninstructions:\nmetadata: {}\nmodel: openai/gpt-oss-20b\nobject: response\noutput: [ResponseReasoningItem(id=‘resp_01kaa4wtmcfbrrd02tzsq8z47y’, summary=[], type=‘reasoning’, content=[Content(text=‘User asks “What's my name?” The name is Jeremy from the greeting. So answer: Jeremy.’, type=‘reasoning_text’)], encrypted_content=None, status=‘completed’), ResponseOutputMessage(id=‘msg_01kaa4wtmcfbs8pa0xnvdkdkpq’, content=[ResponseOutputText(annotations=[], text=‘You’re Jeremy!’, type=‘output_text’, logprobs=None)], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=None)\ntop_logprobs: None\ntruncation: disabled\nusage: ResponseUsage(input_tokens=99, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=34, output_tokens_details=OutputTokensDetails(reasoning_tokens=21), total_tokens=133)\nuser: None\nstore: False",
    "crumbs": [
      "cosette"
    ]
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "CHANGELOG.html#section",
    "href": "CHANGELOG.html#section",
    "title": "",
    "section": "0.2.6",
    "text": "0.2.6\n\nNew Features\n\nValidate tool names returned by model against allowed tools (#34), thanks to @PiotrCzapla"
  },
  {
    "objectID": "CHANGELOG.html#section-1",
    "href": "CHANGELOG.html#section-1",
    "title": "",
    "section": "0.2.5",
    "text": "0.2.5\n\nNew Features\n\nAdd 5.1 and 5.2 models (#33)"
  },
  {
    "objectID": "CHANGELOG.html#section-2",
    "href": "CHANGELOG.html#section-2",
    "title": "",
    "section": "0.2.4",
    "text": "0.2.4\n\nNew Features\n\nAdd gpt-oss models (#31)\n\n\n\nBugs Squashed\n\nUse json.loads for func args instead of ast.literal_eval (#32), thanks to @artste"
  },
  {
    "objectID": "CHANGELOG.html#section-3",
    "href": "CHANGELOG.html#section-3",
    "title": "",
    "section": "0.2.3",
    "text": "0.2.3\n\nNew Features\n\nSupport GPT 5 (#29)"
  },
  {
    "objectID": "CHANGELOG.html#section-4",
    "href": "CHANGELOG.html#section-4",
    "title": "",
    "section": "0.2.2",
    "text": "0.2.2\n\nBugs Squashed\n\nopenai broke their api (#28)"
  },
  {
    "objectID": "CHANGELOG.html#section-5",
    "href": "CHANGELOG.html#section-5",
    "title": "",
    "section": "0.2.1",
    "text": "0.2.1\n\nNew Features\n\nSupport latest 1.99.2 sdk (#27)"
  },
  {
    "objectID": "CHANGELOG.html#section-6",
    "href": "CHANGELOG.html#section-6",
    "title": "",
    "section": "0.2.0",
    "text": "0.2.0\n\nBreaking Changes\n\nRemove obj support (#26)"
  },
  {
    "objectID": "CHANGELOG.html#section-7",
    "href": "CHANGELOG.html#section-7",
    "title": "",
    "section": "0.1.3",
    "text": "0.1.3\n\nBugs Squashed\n\nfastcore dep update needed (#25)"
  },
  {
    "objectID": "CHANGELOG.html#section-8",
    "href": "CHANGELOG.html#section-8",
    "title": "",
    "section": "0.1.2",
    "text": "0.1.2\n\nBreaking Changes\n\nSwitch to responses api\n\n\n\nNew Features\n\nAdd loop_outputs (#24)\nSupport instructions param (#23)\nSwitch to responses api (#22)"
  },
  {
    "objectID": "CHANGELOG.html#section-9",
    "href": "CHANGELOG.html#section-9",
    "title": "",
    "section": "0.0.6",
    "text": "0.0.6\n\nNew Features\n\nadd history and namespace support to OpenAI Chat class (#20), thanks to @austinvhuang\nport toolloop and other updates from claudette and add new models (#19), thanks to @austinvhuang\nenable o1 streaming (#16), thanks to @KeremTurgutlu\nAdd llms.txt (#15), thanks to @Isaac-Flath\no1, o3-mini support (#14), thanks to @austinvhuang\nadd structured outputs (#9), thanks to @ssslakter\nadd msglm (#7), thanks to @comhar\nAdded support for o-1 models and Azure endpoint. (#5), thanks to @fladhak"
  },
  {
    "objectID": "CHANGELOG.html#section-10",
    "href": "CHANGELOG.html#section-10",
    "title": "",
    "section": "0.0.4",
    "text": "0.0.4\n\nBugs Squashed\n\nexport mk_msg and mk_msgs (#10), thanks to @comhar"
  },
  {
    "objectID": "CHANGELOG.html#section-11",
    "href": "CHANGELOG.html#section-11",
    "title": "",
    "section": "0.0.3",
    "text": "0.0.3\n\nNew Features\n\nFix description"
  },
  {
    "objectID": "CHANGELOG.html#section-12",
    "href": "CHANGELOG.html#section-12",
    "title": "",
    "section": "0.0.2",
    "text": "0.0.2\n\nInitial release"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Cosette’s source",
    "section": "",
    "text": "from IPython.display import display,Image,Markdown\nfrom datetime import datetime\nfrom pprint import pprint\n\n\ndef print_columns(items, cols=3, width=30):\n    for i in range(0, len(items), cols):\n        row = items[i:i+cols]\n        print(''.join(item[:width-1].ljust(width) for item in row))\n\nclient = OpenAI()\nmodel_list = client.models.list()\nprint(f\"Available models as of {datetime.now().strftime('%Y-%m-%d')}:\\n\")\nprint_columns(sorted([m.id for m in model_list]))\n\nAvailable models as of 2025-12-18:\n\nbabbage-002                   chatgpt-4o-latest             chatgpt-image-latest          \ncodex-mini-latest             dall-e-2                      dall-e-3                      \ndavinci-002                   gpt-3.5-turbo                 gpt-3.5-turbo-0125            \ngpt-3.5-turbo-1106            gpt-3.5-turbo-16k             gpt-3.5-turbo-instruct        \ngpt-3.5-turbo-instruct-0914   gpt-4                         gpt-4-0125-preview            \ngpt-4-0613                    gpt-4-1106-preview            gpt-4-turbo                   \ngpt-4-turbo-2024-04-09        gpt-4-turbo-preview           gpt-4.1                       \ngpt-4.1-2025-04-14            gpt-4.1-mini                  gpt-4.1-mini-2025-04-14       \ngpt-4.1-nano                  gpt-4.1-nano-2025-04-14       gpt-4o                        \ngpt-4o-2024-05-13             gpt-4o-2024-08-06             gpt-4o-2024-11-20             \ngpt-4o-audio-preview          gpt-4o-audio-preview-2024-12- gpt-4o-audio-preview-2025-06- \ngpt-4o-mini                   gpt-4o-mini-2024-07-18        gpt-4o-mini-audio-preview     \ngpt-4o-mini-audio-preview-202 gpt-4o-mini-realtime-preview  gpt-4o-mini-realtime-preview- \ngpt-4o-mini-search-preview    gpt-4o-mini-search-preview-20 gpt-4o-mini-transcribe        \ngpt-4o-mini-transcribe-2025-0 gpt-4o-mini-transcribe-2025-1 gpt-4o-mini-tts               \ngpt-4o-mini-tts-2025-03-20    gpt-4o-mini-tts-2025-12-15    gpt-4o-realtime-preview       \ngpt-4o-realtime-preview-2024- gpt-4o-realtime-preview-2025- gpt-4o-search-preview         \ngpt-4o-search-preview-2025-03 gpt-4o-transcribe             gpt-4o-transcribe-diarize     \ngpt-5                         gpt-5-2025-08-07              gpt-5-chat-latest             \ngpt-5-codex                   gpt-5-mini                    gpt-5-mini-2025-08-07         \ngpt-5-nano                    gpt-5-nano-2025-08-07         gpt-5-pro                     \ngpt-5-pro-2025-10-06          gpt-5-search-api              gpt-5-search-api-2025-10-14   \ngpt-5.1                       gpt-5.1-2025-11-13            gpt-5.1-chat-latest           \ngpt-5.1-codex                 gpt-5.1-codex-max             gpt-5.1-codex-mini            \ngpt-5.2                       gpt-5.2-2025-12-11            gpt-5.2-chat-latest           \ngpt-5.2-pro                   gpt-5.2-pro-2025-12-11        gpt-audio                     \ngpt-audio-2025-08-28          gpt-audio-mini                gpt-audio-mini-2025-10-06     \ngpt-audio-mini-2025-12-15     gpt-image-1                   gpt-image-1-mini              \ngpt-image-1.5                 gpt-realtime                  gpt-realtime-2025-08-28       \ngpt-realtime-mini             gpt-realtime-mini-2025-10-06  gpt-realtime-mini-2025-12-15  \no1                            o1-2024-12-17                 o1-pro                        \no1-pro-2025-03-19             o3                            o3-2025-04-16                 \no3-deep-research              o3-deep-research-2025-06-26   o3-mini                       \no3-mini-2025-01-31            o3-pro                        o3-pro-2025-06-10             \no4-mini                       o4-mini-2025-04-16            o4-mini-deep-research         \no4-mini-deep-research-2025-06 omni-moderation-2024-09-26    omni-moderation-latest        \nsora-2                        sora-2-pro                    text-embedding-3-large        \ntext-embedding-3-small        text-embedding-ada-002        tts-1                         \ntts-1-1106                    tts-1-hd                      tts-1-hd-1106                 \nwhisper-1                     \n\n\n\n\nExported source\nmodels = 'gpt-5.2', 'gpt-5.2-pro', 'gpt-5.2-chat-latest', 'gpt-5.1-codex', 'gpt-5-mini', 'gpt-5-nano', 'o1-preview', 'o1-mini', 'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-4', 'gpt-4-32k', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct', 'o1', 'o3-mini', 'chatgpt-4o-latest', 'o1-pro', 'o3', 'o4-mini', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano'\n\n\no1 should support images while o1-mini, o3-mini do not support images.\n\nsource\n\n\n\ndef can_set_temp(\n    m\n):\n\n\n\nExported source\ntext_only_models = 'o1-preview', 'o1-mini', 'o3-mini'\n\n\n\n\nExported source\nhas_streaming_models = set(models) - set(('o1-mini', 'o3-mini'))\nhas_sp_models = set(models) - set(('o1-mini', 'o3-mini'))\nhas_temp_models = set(models) - set(('o1', 'o1-mini', 'o3-mini'))\n\n\n\n\nExported source\ndef can_stream(m): return m in has_streaming_models\ndef can_set_sp(m): return m in has_sp_models\ndef can_set_temp(m): return m in has_temp_models\n\n\n\nsource\n\n\n\n\ndef can_set_sp(\n    m\n):\n\n\nsource\n\n\n\n\ndef can_stream(\n    m\n):\n\n\nassert can_stream(\"gpt-4o\")\nassert not can_stream(\"o1-mini\")\n\n\nmodel = first(m for m in models if 'mini' in m)\nmodel\n\n'gpt-5-mini'",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#setup",
    "href": "core.html#setup",
    "title": "Cosette’s source",
    "section": "",
    "text": "from IPython.display import display,Image,Markdown\nfrom datetime import datetime\nfrom pprint import pprint\n\n\ndef print_columns(items, cols=3, width=30):\n    for i in range(0, len(items), cols):\n        row = items[i:i+cols]\n        print(''.join(item[:width-1].ljust(width) for item in row))\n\nclient = OpenAI()\nmodel_list = client.models.list()\nprint(f\"Available models as of {datetime.now().strftime('%Y-%m-%d')}:\\n\")\nprint_columns(sorted([m.id for m in model_list]))\n\nAvailable models as of 2025-12-18:\n\nbabbage-002                   chatgpt-4o-latest             chatgpt-image-latest          \ncodex-mini-latest             dall-e-2                      dall-e-3                      \ndavinci-002                   gpt-3.5-turbo                 gpt-3.5-turbo-0125            \ngpt-3.5-turbo-1106            gpt-3.5-turbo-16k             gpt-3.5-turbo-instruct        \ngpt-3.5-turbo-instruct-0914   gpt-4                         gpt-4-0125-preview            \ngpt-4-0613                    gpt-4-1106-preview            gpt-4-turbo                   \ngpt-4-turbo-2024-04-09        gpt-4-turbo-preview           gpt-4.1                       \ngpt-4.1-2025-04-14            gpt-4.1-mini                  gpt-4.1-mini-2025-04-14       \ngpt-4.1-nano                  gpt-4.1-nano-2025-04-14       gpt-4o                        \ngpt-4o-2024-05-13             gpt-4o-2024-08-06             gpt-4o-2024-11-20             \ngpt-4o-audio-preview          gpt-4o-audio-preview-2024-12- gpt-4o-audio-preview-2025-06- \ngpt-4o-mini                   gpt-4o-mini-2024-07-18        gpt-4o-mini-audio-preview     \ngpt-4o-mini-audio-preview-202 gpt-4o-mini-realtime-preview  gpt-4o-mini-realtime-preview- \ngpt-4o-mini-search-preview    gpt-4o-mini-search-preview-20 gpt-4o-mini-transcribe        \ngpt-4o-mini-transcribe-2025-0 gpt-4o-mini-transcribe-2025-1 gpt-4o-mini-tts               \ngpt-4o-mini-tts-2025-03-20    gpt-4o-mini-tts-2025-12-15    gpt-4o-realtime-preview       \ngpt-4o-realtime-preview-2024- gpt-4o-realtime-preview-2025- gpt-4o-search-preview         \ngpt-4o-search-preview-2025-03 gpt-4o-transcribe             gpt-4o-transcribe-diarize     \ngpt-5                         gpt-5-2025-08-07              gpt-5-chat-latest             \ngpt-5-codex                   gpt-5-mini                    gpt-5-mini-2025-08-07         \ngpt-5-nano                    gpt-5-nano-2025-08-07         gpt-5-pro                     \ngpt-5-pro-2025-10-06          gpt-5-search-api              gpt-5-search-api-2025-10-14   \ngpt-5.1                       gpt-5.1-2025-11-13            gpt-5.1-chat-latest           \ngpt-5.1-codex                 gpt-5.1-codex-max             gpt-5.1-codex-mini            \ngpt-5.2                       gpt-5.2-2025-12-11            gpt-5.2-chat-latest           \ngpt-5.2-pro                   gpt-5.2-pro-2025-12-11        gpt-audio                     \ngpt-audio-2025-08-28          gpt-audio-mini                gpt-audio-mini-2025-10-06     \ngpt-audio-mini-2025-12-15     gpt-image-1                   gpt-image-1-mini              \ngpt-image-1.5                 gpt-realtime                  gpt-realtime-2025-08-28       \ngpt-realtime-mini             gpt-realtime-mini-2025-10-06  gpt-realtime-mini-2025-12-15  \no1                            o1-2024-12-17                 o1-pro                        \no1-pro-2025-03-19             o3                            o3-2025-04-16                 \no3-deep-research              o3-deep-research-2025-06-26   o3-mini                       \no3-mini-2025-01-31            o3-pro                        o3-pro-2025-06-10             \no4-mini                       o4-mini-2025-04-16            o4-mini-deep-research         \no4-mini-deep-research-2025-06 omni-moderation-2024-09-26    omni-moderation-latest        \nsora-2                        sora-2-pro                    text-embedding-3-large        \ntext-embedding-3-small        text-embedding-ada-002        tts-1                         \ntts-1-1106                    tts-1-hd                      tts-1-hd-1106                 \nwhisper-1                     \n\n\n\n\nExported source\nmodels = 'gpt-5.2', 'gpt-5.2-pro', 'gpt-5.2-chat-latest', 'gpt-5.1-codex', 'gpt-5-mini', 'gpt-5-nano', 'o1-preview', 'o1-mini', 'gpt-4o', 'gpt-4o-mini', 'gpt-4-turbo', 'gpt-4', 'gpt-4-32k', 'gpt-3.5-turbo', 'gpt-3.5-turbo-instruct', 'o1', 'o3-mini', 'chatgpt-4o-latest', 'o1-pro', 'o3', 'o4-mini', 'gpt-4.1', 'gpt-4.1-mini', 'gpt-4.1-nano'\n\n\no1 should support images while o1-mini, o3-mini do not support images.\n\nsource\n\n\n\ndef can_set_temp(\n    m\n):\n\n\n\nExported source\ntext_only_models = 'o1-preview', 'o1-mini', 'o3-mini'\n\n\n\n\nExported source\nhas_streaming_models = set(models) - set(('o1-mini', 'o3-mini'))\nhas_sp_models = set(models) - set(('o1-mini', 'o3-mini'))\nhas_temp_models = set(models) - set(('o1', 'o1-mini', 'o3-mini'))\n\n\n\n\nExported source\ndef can_stream(m): return m in has_streaming_models\ndef can_set_sp(m): return m in has_sp_models\ndef can_set_temp(m): return m in has_temp_models\n\n\n\nsource\n\n\n\n\ndef can_set_sp(\n    m\n):\n\n\nsource\n\n\n\n\ndef can_stream(\n    m\n):\n\n\nassert can_stream(\"gpt-4o\")\nassert not can_stream(\"o1-mini\")\n\n\nmodel = first(m for m in models if 'mini' in m)\nmodel\n\n'gpt-5-mini'",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#openai-sdk",
    "href": "core.html#openai-sdk",
    "title": "Cosette’s source",
    "section": "OpenAI SDK",
    "text": "OpenAI SDK\n\ncli = OpenAI().responses\n\n\nm = {'role': 'user', 'content': \"I'm Jeremy\"}\nr = cli.create(\n    input=[m], model=model, max_output_tokens=100,\n    text={ \"verbosity\": \"low\" },\n    reasoning={ \"effort\": \"minimal\" }\n)\nprint(r)\n\nResponse(id='resp_0265a51280da05ce006943fbff17f48193b07e2e882e2f3fed', created_at=1766063103.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_0265a51280da05ce006943fbff642481939bf2e1300bc13f18', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0265a51280da05ce006943fbff9e5481938d162f29a6c301a2', content=[ResponseOutputText(annotations=[], text='Hi Jeremy — nice to meet you. How can I help today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=100, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 8; Out: 20; Total: 28, user=None, billing={'payer': 'openai'}, store=True)\n\n\n\nFormatting output\n\n\nExported source\n@patch\ndef _repr_markdown_(self:Response):\n    det = '\\n- '.join(f'{k}: {v}' for k,v in dict(self).items())\n    res = self.output_text\n    if not res: return f\"- {det}\"\n    return f\"\"\"{res}\n\n&lt;details&gt;\n\n- {det}\n\n&lt;/details&gt;\"\"\"\n\n\n\nr\n\nHi Jeremy — nice to meet you. How can I help today?\n\n\nid: resp_0265a51280da05ce006943fbff17f48193b07e2e882e2f3fed\ncreated_at: 1766063103.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0265a51280da05ce006943fbff642481939bf2e1300bc13f18’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0265a51280da05ce006943fbff9e5481938d162f29a6c301a2’, content=[ResponseOutputText(annotations=[], text=‘Hi Jeremy — nice to meet you. How can I help today?’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 100\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=8, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=28)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True\n\n\n\n\n\nr.usage\n\nIn: 8; Out: 20; Total: 28\n\n\n\nsource\n\n\nusage\n\ndef usage(\n    inp:int=0, # Number of prompt tokens\n    out:int=0, # Number of completion tokens\n):\n\nSlightly more concise version of ResponseUsage.\n\n\nExported source\ndef usage(inp=0, # Number of prompt tokens\n          out=0  # Number of completion tokens\n         ):\n    \"Slightly more concise version of `ResponseUsage`.\"\n    return ResponseUsage(input_tokens=inp, output_tokens=out, total_tokens=inp+out, input_tokens_details={'cached_tokens':0}, output_tokens_details={'cached_tokens':0, 'reasoning_tokens':0})\n\n\n\nusage(5)\n\nIn: 5; Out: 0; Total: 5\n\n\n\nsource\n\n\nResponseUsage.__repr__\n\ndef __repr__(\n    \n):\n\nReturn repr(self).\n\n\nExported source\n@patch\ndef __repr__(self:ResponseUsage): return f'In: {self.input_tokens}; Out: {self.output_tokens}; Total: {self.total_tokens}'\n\n\n\nr.usage\n\nIn: 8; Out: 20; Total: 28\n\n\n\nsource\n\n\nResponseUsage.__add__\n\ndef __add__(\n    b\n):\n\nAdd together each of input_tokens and output_tokens\n\n\nExported source\n@patch\ndef __add__(self:ResponseUsage, b):\n    \"Add together each of `input_tokens` and `output_tokens`\"\n    return usage(self.input_tokens+b.input_tokens, self.output_tokens+b.output_tokens)\n\n\n\nr.usage+r.usage\n\nIn: 16; Out: 40; Total: 56\n\n\n\nsource\n\n\nwrap_latex\n\ndef wrap_latex(\n    text\n):\n\nReplace OpenAI LaTeX codes with markdown-compatible ones\n\n\nCreating messages\nCreating correctly formatted dicts from scratch every time isn’t very handy, so we’ll import a couple of helper functions from the msglm library.\nLet’s use mk_msg to recreate our msg {'role': 'user', 'content': \"I'm Jeremy\"} from earlier.\n\nrkw = dict(\n    text={ \"verbosity\": \"low\" },\n    reasoning={ \"effort\": \"minimal\" }\n)\n\n\nprompt = \"I'm Jeremy\"\nm = mk_msg(prompt)\nr = cli.create(input=[m], model=model, max_output_tokens=400, **rkw)\nr\n\nNice to meet you, Jeremy. How can I help you today?\n\n\nid: resp_0234b44bdd8bf5d2006943fc008fe88190970de1236e42e2f0\ncreated_at: 1766063104.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0234b44bdd8bf5d2006943fc00d530819098ee80460a88d72d’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0234b44bdd8bf5d2006943fc00ffcc8190b0df6e7599b23188’, content=[ResponseOutputText(annotations=[], text=‘Nice to meet you, Jeremy. How can I help you today?’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 400\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=8, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=28)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True\n\n\n\n\n\nprint(r)\n\nResponse(id='resp_0234b44bdd8bf5d2006943fc008fe88190970de1236e42e2f0', created_at=1766063104.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_0234b44bdd8bf5d2006943fc00d530819098ee80460a88d72d', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ResponseOutputMessage(id='msg_0234b44bdd8bf5d2006943fc00ffcc8190b0df6e7599b23188', content=[ResponseOutputText(annotations=[], text='Nice to meet you, Jeremy. How can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=400, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='minimal', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='low'), top_logprobs=0, truncation='disabled', usage=In: 8; Out: 20; Total: 28, user=None, billing={'payer': 'openai'}, store=True)\n\n\nWe can pass more than just text messages to OpenAI. As we’ll see later we can also pass images, SDK objects, etc. To handle these different data types we need to pass the type along with our content to OpenAI.\nmk_msg infers the type automatically and creates the appropriate data structure.\nLLMs, don’t actually have state, but instead dialogs are created by passing back all previous prompts and responses every time. With OpenAI, they always alternate user and assistant. We’ll use mk_msgs from msglm to make it easier to build up these dialog lists.\n\nmsgs = mk_msgs([prompt, r, \"I forgot my name. Can you remind me please?\"]) \nmsgs\n\n[{'role': 'user', 'content': \"I'm Jeremy\"},\n ResponseReasoningItem(id='rs_0234b44bdd8bf5d2006943fc00d530819098ee80460a88d72d', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseOutputMessage(id='msg_0234b44bdd8bf5d2006943fc00ffcc8190b0df6e7599b23188', content=[ResponseOutputText(annotations=[], text='Nice to meet you, Jeremy. How can I help you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'),\n {'role': 'user', 'content': 'I forgot my name. Can you remind me please?'}]\n\n\n\ncli.create(input=msgs, model=model, max_output_tokens=400, **rkw)\n\nYou told me your name is Jeremy.\n\n\nid: resp_0234b44bdd8bf5d2006943fc0198d481909981aa9c1e32488c\ncreated_at: 1766063105.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0234b44bdd8bf5d2006943fc01ec0881908c7b8f1fea4a4c59’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0234b44bdd8bf5d2006943fc0255948190973533dd978af3ad’, content=[ResponseOutputText(annotations=[], text=‘You told me your name is Jeremy.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 400\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=43, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=14, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=57)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#client",
    "href": "core.html#client",
    "title": "Cosette’s source",
    "section": "Client",
    "text": "Client\n\nBasics\n\nsource\n\n\nClient\n\ndef Client(\n    model, cli:NoneType=None, api_key_env:NoneType=None, base_url:NoneType=None\n):\n\nBasic LLM messages client.\n\n\nExported source\nclass Client:\n    def __init__(self, model, cli=None, api_key_env=None, base_url=None):\n        \"Basic LLM messages client.\"\n        self.model,self.use = model,usage(0,0)\n        self.text_only = model in text_only_models\n        if not cli:\n            cli = OpenAI(api_key=os.getenv(api_key_env or \"OPENAI_API_KEY\"), base_url=base_url )\n        self.c = cli.responses\n\n\n\nc = Client(model)\nc.use\n\nIn: 0; Out: 0; Total: 0\n\n\n\n\nExported source\n@patch\ndef _r(self:Client, r):\n    \"Store the result of the message and accrue total usage.\"\n    self.result = r\n    if getattr(r,'usage',None): self.use += r.usage\n    return r\n\n\n\nc._r(r)\nc.use\n\nIn: 8; Out: 20; Total: 28\n\n\n\nsource\n\n\nmk_openai_func\n\ndef mk_openai_func(\n    f\n):\n\n\nsource\n\n\nmk_tool_choice\n\ndef mk_tool_choice(\n    f\n):\n\nOur mk_tool_choice converts falsy values to NOT_GIVEN which omits the value completely from the API call. It treats any string except for 'required'|'none' as tool call and converts it to dictionary\n  {\"type\": \"function\", \"function\": {\"name\": f}}\nThe remaining option 'auto' is the default, so we simply recommend using None that translates to NOT_GIVEN.\n\nsource\n\n\nget_stream\n\ndef get_stream(\n    o, r, cli, cb:NoneType=None\n):\n\n\nsource\n\n\nClient.__call__\n\ndef __call__(\n    msgs:list, # List of messages in the dialog\n    sp:str='', # System prompt\n    maxtok:int=4096, # Maximum tokens\n    stream:bool=False, # Stream response?\n    tools:Optional=None, # List of tools to make available\n    tool_choice:Optional=None, # Forced tool choice\n    cb:callable=None, # Callback after completion\n    background:Optional[bool] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    conversation:Optional[response_create_params.Conversation] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    include:Optional[List[ResponseIncludable]] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    input:Union[str, ResponseInputParam] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    instructions:Optional[str] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    max_output_tokens:Optional[int] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    max_tool_calls:Optional[int] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    metadata:Optional[Metadata] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    model:ResponsesModel | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    parallel_tool_calls:Optional[bool] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    previous_response_id:Optional[str] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    prompt:Optional[ResponsePromptParam] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    prompt_cache_key:str | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    prompt_cache_retention:Optional[Literal['in-memory', '24h']] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    reasoning:Optional[Reasoning] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    safety_identifier:str | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    service_tier:Optional[Literal['auto', 'default', 'flex', 'scale', 'priority']] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    store:Optional[bool] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    stream_options:Optional[response_create_params.StreamOptions] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    temperature:Optional[float] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    text:ResponseTextConfigParam | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    top_logprobs:Optional[int] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    top_p:Optional[float] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    truncation:Optional[Literal['auto', 'disabled']] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    user:str | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    extra_headers:Headers | None=None, # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\nThe extra values given here take precedence over values defined on the client or passed to this method.\n    extra_query:Query | None=None, extra_body:Body | None=None,\n    timeout:float | httpx.Timeout | None | NotGiven=NOT_GIVEN\n):\n\nMake a call to LLM.\n\n\nExported source\n@patch\n@delegates(Responses.create)\ndef __call__(self:Client,\n             msgs:list, # List of messages in the dialog\n             sp:str='', # System prompt\n             maxtok=4096, # Maximum tokens\n             stream:bool=False, # Stream response?\n             tools:Optional[list]=None, # List of tools to make available\n             tool_choice:Optional[str]=None, # Forced tool choice\n             cb:callable=None, # Callback after completion\n             **kwargs):\n    \"Make a call to LLM.\"\n    if tools: assert not self.text_only, \"Tool use is not supported by the current model type.\"\n    if any(c['type'] == 'image_url' for msg in msgs if isinstance(msg, dict) and isinstance(msg.get('content'), list) for c in msg['content']): assert not self.text_only, \"Images are not supported by the current model type.\"\n    tools = [mk_openai_func(o) for o in listify(tools)]\n    r = self.c.create(\n        model=self.model, input=msgs, max_output_tokens=maxtok, stream=stream, instructions=sp,\n        tools=tools, tool_choice=mk_tool_choice(tool_choice), **kwargs)\n    if stream: return get_stream(r, self, cb=cb)\n    else:\n        res = self._r(r)\n        if cb: cb(res)\n        return res\n\n\n\nmsgs = 'Hi'\n\n\nc(msgs)\n\nHi! How can I help you today?\n\n\nid: resp_065c2e7e7bef6bde006943fc03236c8195b210bffc9db409ef\ncreated_at: 1766063107.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_065c2e7e7bef6bde006943fc0373d08195b33048fb368515ca’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_065c2e7e7bef6bde006943fc04b9a081958509334f1f7d9aaf’, content=[ResponseOutputText(annotations=[], text=‘Hi! How can I help you today?’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘medium’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=7, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=79, output_tokens_details=OutputTokensDetails(reasoning_tokens=64), total_tokens=86)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True\n\n\n\n\n\nc.use\n\nIn: 15; Out: 99; Total: 114\n\n\n\nr = c(msgs, stream=True)\nfor o in r: print(o, end='')\n\nHi — how can I help you today?\n\n\n\nr.value\n\nHi — how can I help you today?\n\n\nid: resp_0fa910a22542c20a006943fc0536708195b5bc51bfb1e09461\ncreated_at: 1766063109.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0fa910a22542c20a006943fc057d6881959dc6d6773a9e11e3’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0fa910a22542c20a006943fc069ef88195b4bbe832ae4bed0a’, content=[ResponseOutputText(annotations=[], text=‘Hi — how can I help you today?’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘medium’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=7, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=79, output_tokens_details=OutputTokensDetails(reasoning_tokens=64), total_tokens=86)\nuser: None\nstore: True\n\n\n\n\n\nlen(r.events)\n\n19\n\n\n\nc.use\n\nIn: 22; Out: 178; Total: 200\n\n\n\nc(msgs, sp='Talk like GLaDOS.', **rkw)\n\nHello. It’s… delightful that you’ve decided to communicate. State your purpose so we may proceed with minimal wasted time.\n\n\nid: resp_0193b4fcbba4639a006943fc0711c48197bdd2e56dc80b6e6e\ncreated_at: 1766063111.0\nerror: None\nincomplete_details: None\ninstructions: Talk like GLaDOS.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0193b4fcbba4639a006943fc0759408197b8630a59417d0a52’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0193b4fcbba4639a006943fc0791648197accd6a3c0557678b’, content=[ResponseOutputText(annotations=[], text=“Hello. It’s… delightful that you’ve decided to communicate. State your purpose so we may proceed with minimal wasted time.”, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=17, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=29, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=46)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True\n\n\n\n\n\n\nImages\nAs everyone knows, when testing image APIs you have to use a cute puppy.\n\n# Image is Cute_dog.jpg from Wikimedia\nfn = Path('samples/puppy.jpg')\nImage(filename=fn, width=200)\n\n\n\n\n\n\n\n\n\nimg = fn.read_bytes()\n\nOpenAI expects an image message to have the following structure\n{\n  \"type\": \"image_url\",\n  \"image_url\": {\n    \"url\": f\"data:{MEDIA_TYPE};base64,{IMG}\"\n  }\n}\nmsglm automatically detects if a message is an image, encodes it, and generates the data structure above. All we need to do is a create a list containing our image and a query and then pass it to mk_msg.\nLet’s try it out…\n\nq = \"In brief, what color flowers are in this image?\"\nmsg = [mk_msg(img), mk_msg(q)]\n\n\nc = Client(model)\nc(msg, **rkw)\n\nLight purple (lavender) flowers.\n\n\nid: resp_06c91e5a9021060d006943fc0859c08194a4697c2a89807169\ncreated_at: 1766063112.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_06c91e5a9021060d006943fc08ba9c8194905c9143163f1283’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_06c91e5a9021060d006943fc08e2a48194989a0193da86fdcf’, content=[ResponseOutputText(annotations=[], text=‘Light purple (lavender) flowers.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=107, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=14, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=121)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#tool-use",
    "href": "core.html#tool-use",
    "title": "Cosette’s source",
    "section": "Tool use",
    "text": "Tool use\n\nBasic tool calling\n\ndef sums(\n    a:int,  # First thing to sum\n    b:int # Second thing to sum\n) -&gt; int: # The sum of the inputs\n    \"Adds a + b.\"\n    print(f\"Finding the sum of {a} and {b}\")\n    return a + b\n\n\ndef add(x: int, y:int):\n    \"adds x and y\"\n    return x + y\n\nmk_openai_func(add)\n\n{'type': 'function',\n 'name': 'add',\n 'description': 'adds x and y',\n 'parameters': {'type': 'object',\n  'properties': {'x': {'type': 'integer', 'description': ''},\n   'y': {'type': 'integer', 'description': ''}},\n  'required': ['x', 'y']}}\n\n\n\nsysp = \"You are a helpful assistant. When using tools, be sure to pass all required parameters. Don't use tools unless needed for the provided prompt.\"\n\n\na,b = 604542,6458932\npr = f\"What is {a}+{b}?\"\ntools=sums\ntool_choice=\"sums\"\n\n\nmsgs = [mk_msg(pr)]\nr = c(msgs, sp=sysp, tools=tools, tool_choice='required', **rkw)\n\n\ntc = [o for o in r.output if isinstance(o, ResponseFunctionToolCall)]\ntc\n\n[ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_uZ9bpRTk2Rnr9vMKUGXh5gOZ', name='sums', type='function_call', id='fc_0278bf5ab7665d17006943fc0a0f508194aa69f150e7e7c560', status='completed')]\n\n\n\nfunc = tc[0]\nfunc\n\nResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_uZ9bpRTk2Rnr9vMKUGXh5gOZ', name='sums', type='function_call', id='fc_0278bf5ab7665d17006943fc0a0f508194aa69f150e7e7c560', status='completed')\n\n\n\nsource\n\n\ncall_func_openai\n\ndef call_func_openai(\n    func, ns:Optional=None\n):\n\n\n\nExported source\ndef call_func_openai(func, ns:Optional[abc.Mapping]=None):\n    try: return call_func(func.name, json.loads(func.arguments), ns, raise_on_err=False)\n    except KeyError as e: return f\"Error - tool not defined in the tool_schemas: {func.name}\"\n\n\n\nns = mk_ns(sums)\nres = call_func_openai(func, ns=ns)\nres\n\nFinding the sum of 604542 and 6458932\n\n\n7063474\n\n\n\nsource\n\n\nallowed_tools\n\ndef allowed_tools(\n    specs:Optional=None, choice:Union=None\n):\n\n\n\nExported source\ndef _get_name(f):\n    if isinstance(f,str): return f\n    if isinstance(f, dict): return f['name']\n    if callable(f) and hasattr(f, '__name__'): return f.__name__\n\ndef allowed_tools(specs:Optional[list]=None, choice:Optional[Union[dict,str]]=None):\n    if choice:\n        choice = mk_tool_choice(choice)\n        if isinstance(choice, dict) and choice['type'] == 'function': \n            return {choice['function']['name']}\n    return {_get_name(v) for v in specs or []}\n\n\n\nallowed_tools([sums, add], 'add')\n\n{'add'}\n\n\n\nsource\n\n\nlimit_ns\n\ndef limit_ns(\n    ns:Optional=None, # Namespace to search for tools\n    specs:Union=None, # List of the tools that are allowed for llm to call, if None no tools are allowed\n    choice:Union=None, # Tool choice as defined by Anthropic API\n):\n\nFilter namespace ns to only include tools allowed by specs and choice\n\n\nExported source\ndef limit_ns(\n    ns:Optional[abc.Mapping]=None, # Namespace to search for tools\n    specs:Optional[Union[str,abc.Callable]]=None, # List of the tools that are allowed for llm to call, if None no tools are allowed\n    choice:Optional[Union[dict,str]]=None # Tool choice as defined by Anthropic API\n    ):\n    \"Filter namespace `ns` to only include tools allowed by `specs` and `choice`\"\n    if ns is None: ns = globals()\n    if not isinstance(ns, abc.Mapping): ns = mk_ns(ns)\n    ns = {k:v for k,v in ns.items() if k in allowed_tools(specs, choice)}\n    return ns\n\n\n\nlimit_ns([sums, add], None, None)\n\n{}\n\n\n\nlimit_ns([sums, add], ['sums'], None)\n\n{'sums': &lt;function __main__.sums(a: int, b: int) -&gt; int&gt;}\n\n\n\nlimit_ns([sums, add], ['sums', add], 'add')\n\n{'add': &lt;function __main__.add(x: int, y: int)&gt;}\n\n\n\nsource\n\n\nmk_toolres\n\ndef mk_toolres(\n    r:Mapping, # Response containing tool use request\n    ns:Optional=None, # Namespace to search for tools\n):\n\nCreate a tool_result message from response r.\n\n\nExported source\ndef _toolres(r, ns):\n    \"Create a result dict from `tcs`.\"\n    if ns is None: ns = globals()\n    tcs = [o for o in getattr(r, 'output', []) if isinstance(o, ResponseFunctionToolCall)]\n    return { tc.call_id: call_func_openai(tc, ns=mk_ns(ns)) for tc in tcs }\n\n\n\n\nExported source\ndef mk_toolres(\n    r:abc.Mapping, # Response containing tool use request\n    ns:Optional[abc.Mapping]=None, # Namespace to search for tools\n    ):\n    \"Create a `tool_result` message from response `r`.\"\n    tr = _toolres(r, ns)\n    r = mk_msg(r)\n    res = [r] if isinstance(r, dict) else listify(r)\n    for k,v in tr.items(): res.append(dict(type=\"function_call_output\", call_id=k, output=str(v)))\n    return res\n\n\n\ntr = mk_toolres(r, ns=ns)\ntr\n\nFinding the sum of 604542 and 6458932\n\n\n[ResponseReasoningItem(id='rs_0278bf5ab7665d17006943fc09c3e8819499322942022e9de6', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_uZ9bpRTk2Rnr9vMKUGXh5gOZ', name='sums', type='function_call', id='fc_0278bf5ab7665d17006943fc0a0f508194aa69f150e7e7c560', status='completed'),\n {'type': 'function_call_output',\n  'call_id': 'call_uZ9bpRTk2Rnr9vMKUGXh5gOZ',\n  'output': '7063474'}]\n\n\n\nm2 = msgs + tr\n\n\nres = c(mk_msgs(m2), sp=sysp, tools=tools)\nres\n\n604542 + 6,458,932 = 7,063,474\n\n\nid: resp_0278bf5ab7665d17006943fc0ae8f081949ba1ce578dc4a48b\ncreated_at: 1766063114.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don’t use tools unless needed for the provided prompt.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseOutputMessage(id=‘msg_0278bf5ab7665d17006943fc0b33f081948ac26c67ad12dcef’, content=[ResponseOutputText(annotations=[], text=‘604542 + 6,458,932 = 7,063,474’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘sums’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘integer’, ‘description’: ‘First thing to sum’}, ‘b’: {‘type’: ‘integer’, ‘description’: ‘Second thing to sum’}}, ‘required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Adds a + b.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘medium’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘medium’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=157, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=177)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True\n\n\n\n\n\ntr = mk_toolres(r, ns=limit_ns([sums, add], [sums, add], 'add'))\ntr\n\n[ResponseReasoningItem(id='rs_0278bf5ab7665d17006943fc09c3e8819499322942022e9de6', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_uZ9bpRTk2Rnr9vMKUGXh5gOZ', name='sums', type='function_call', id='fc_0278bf5ab7665d17006943fc0a0f508194aa69f150e7e7c560', status='completed'),\n {'type': 'function_call_output',\n  'call_id': 'call_uZ9bpRTk2Rnr9vMKUGXh5gOZ',\n  'output': 'Error - tool not defined in the tool_schemas: sums'}]\n\n\nThis should also work in situations where no tool use is required:\n\nmsgs = mk_toolres(\"I'm Jeremy\")\nc(msgs, sp=sysp, tools=tools, **rkw)\n\nNice to meet you, Jeremy. How can I help you today?\n\n\nid: resp_01af5661a0ba543e006943fc0c01ac8196b66f21a944c1337e\ncreated_at: 1766063116.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don’t use tools unless needed for the provided prompt.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_01af5661a0ba543e006943fc0c4e588196934dd914a099574c’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_01af5661a0ba543e006943fc0c77d481969ca90d07baa45c08’, content=[ResponseOutputText(annotations=[], text=‘Nice to meet you, Jeremy. How can I help you today?’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘sums’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘integer’, ‘description’: ‘First thing to sum’}, ‘b’: {‘type’: ‘integer’, ‘description’: ‘Second thing to sum’}}, ‘required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Adds a + b.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=96, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=116)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True\n\n\n\n\n\nsource\n\n\nClient.structured\n\ndef structured(\n    msgs:list, # Prompt\n    tools:Optional=None, # List of tools to make available to OpenAI model\n    ns:Optional=None, # Namespace to search for tools\n    sp:str='', # System prompt\n    maxtok:int=4096, # Maximum tokens\n    stream:bool=False, # Stream response?\n    tool_choice:Optional=None, # Forced tool choice\n    cb:callable=None, # Callback after completion\n    background:Optional[bool] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    conversation:Optional[response_create_params.Conversation] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    include:Optional[List[ResponseIncludable]] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    input:Union[str, ResponseInputParam] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    instructions:Optional[str] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    max_output_tokens:Optional[int] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    max_tool_calls:Optional[int] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    metadata:Optional[Metadata] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    model:ResponsesModel | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    parallel_tool_calls:Optional[bool] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    previous_response_id:Optional[str] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    prompt:Optional[ResponsePromptParam] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    prompt_cache_key:str | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    prompt_cache_retention:Optional[Literal['in-memory', '24h']] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    reasoning:Optional[Reasoning] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    safety_identifier:str | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    service_tier:Optional[Literal['auto', 'default', 'flex', 'scale', 'priority']] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    store:Optional[bool] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    stream_options:Optional[response_create_params.StreamOptions] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    temperature:Optional[float] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    text:ResponseTextConfigParam | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    top_logprobs:Optional[int] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    top_p:Optional[float] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    truncation:Optional[Literal['auto', 'disabled']] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    user:str | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    extra_headers:Headers | None=None, # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\nThe extra values given here take precedence over values defined on the client or passed to this method.\n    extra_query:Query | None=None, extra_body:Body | None=None,\n    timeout:float | httpx.Timeout | None | NotGiven=NOT_GIVEN\n):\n\nReturn the value of all tool calls (generally used for structured outputs)\n\n\nExported source\n@patch\n@delegates(Client.__call__)\ndef structured(self:Client,\n               msgs: list, # Prompt\n               tools:Optional[list]=None, # List of tools to make available to OpenAI model\n               ns:Optional[abc.Mapping]=None, # Namespace to search for tools\n               **kwargs):\n    \"Return the value of all tool calls (generally used for structured outputs)\"\n    if ns is None: ns = mk_ns(tools)\n    r = self(msgs, tools=tools, tool_choice='required', **kwargs)\n    return first(_toolres(r, ns).values())\n\n\n\nclass PrimeMinister(BasicRepr):\n    \"An Australian prime minister\"\n    def __init__(\n        self,\n        firstname:str, # First name\n        surname:str, # Surname\n        dob:str, # Date of birth\n        year_entered:int, # Year first became PM\n    ): store_attr()\n\n\nc1 = Client(model)\nc1.structured('Who was the first prime minister of Australia?', [PrimeMinister], **rkw)\n\nPrimeMinister(firstname='Edmund', surname='Barton', dob='1849-01-18', year_entered=1901)\n\n\n\n\nStreaming tool calling\n\nmsgs = [mk_msg(pr)]\nr = c(msgs, sp=sysp, tools=tools, stream=True, **rkw)\n\nWe can stream back any tool call text (which may be empty):\n\nfor o in r: print(o, end='')\n\nAfter streaming is complete, value.output will contain the tool calls:\n\nr.value.output\n\n[ResponseReasoningItem(id='rs_0e3b419c50c1b99f006943fc0f02488190b01c0e4f5473ad74', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_mBzrloUYhxSVlhuJLQx52BJT', name='sums', type='function_call', id='fc_0e3b419c50c1b99f006943fc0f60bc81909d88ed289f143273', status='completed')]\n\n\nTherefore we can repeat the same process as before, but using the value attr:\n\ntr = mk_toolres(r.value, ns=ns)\nmsgs += tr\nc(mk_msgs(msgs), sp=sysp, tools=tools, **rkw)\n\nFinding the sum of 604542 and 6458932\n\n\n7,063,474\n\n\nid: resp_0e3b419c50c1b99f006943fc1025148190baac3f22eb1fc289\ncreated_at: 1766063120.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don’t use tools unless needed for the provided prompt.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseOutputMessage(id=‘msg_0e3b419c50c1b99f006943fc1079f88190ad05b366136e4ff1’, content=[ResponseOutputText(annotations=[], text=‘7,063,474’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘sums’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘integer’, ‘description’: ‘First thing to sum’}, ‘b’: {‘type’: ‘integer’, ‘description’: ‘Second thing to sum’}}, ‘required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Adds a + b.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=157, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=9, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=166)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#chat",
    "href": "core.html#chat",
    "title": "Cosette’s source",
    "section": "Chat",
    "text": "Chat\n\nBasic chat\n\nsource\n\n\nChat\n\ndef Chat(\n    model:Optional=None, # Model to use (leave empty if passing `cli`)\n    cli:Optional=None, # Client to use (leave empty if passing `model`)\n    sp:str='', # Optional system prompt\n    tools:Optional=None, # List of tools to make available\n    hist:list=None, # Initialize history\n    tool_choice:Optional=None, # Forced tool choice\n    ns:Optional=None, # Namespace to search for tools\n    kw:VAR_KEYWORD\n):\n\nOpenAI chat client.\n\n\nExported source\nclass Chat:\n    def __init__(self,\n                 model:Optional[str]=None, # Model to use (leave empty if passing `cli`)\n                 cli:Optional[Client]=None, # Client to use (leave empty if passing `model`)\n                 sp='', # Optional system prompt\n                 tools:Optional[list]=None, # List of tools to make available\n                 hist: list = None,  # Initialize history\n                 tool_choice:Optional[str]=None, # Forced tool choice\n                 ns:Optional[abc.Mapping]=None,  # Namespace to search for tools\n                 **kw):\n        \"OpenAI chat client.\"\n        assert model or cli\n        self.c = (cli or Client(model))\n        self.h = hist if hist else []\n        if ns is None: ns=tools\n        self.sp,self.tools,self.tool_choice,self.ns,self.kw = sp,tools,tool_choice,ns,kw\n    \n    @property\n    def use(self): return self.c.use\n\n\n\nchat = Chat(model, sp=sysp, **rkw)\nchat.c.use, chat.h\n\n(In: 0; Out: 0; Total: 0, [])\n\n\n\nsource\n\n\nChat.__call__\n\ndef __call__(\n    pr:NoneType=None, # Prompt / message\n    stream:bool=False, # Stream response?\n    tools:NoneType=None, # Tools to use\n    tool_choice:NoneType=None, # Required tools to use\n    background:Optional[bool] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    conversation:Optional[response_create_params.Conversation] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    include:Optional[List[ResponseIncludable]] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    input:Union[str, ResponseInputParam] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    instructions:Optional[str] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    max_output_tokens:Optional[int] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    max_tool_calls:Optional[int] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    metadata:Optional[Metadata] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    model:ResponsesModel | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    parallel_tool_calls:Optional[bool] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    previous_response_id:Optional[str] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    prompt:Optional[ResponsePromptParam] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    prompt_cache_key:str | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    prompt_cache_retention:Optional[Literal['in-memory', '24h']] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    reasoning:Optional[Reasoning] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    safety_identifier:str | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    service_tier:Optional[Literal['auto', 'default', 'flex', 'scale', 'priority']] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    store:Optional[bool] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    stream_options:Optional[response_create_params.StreamOptions] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    temperature:Optional[float] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    text:ResponseTextConfigParam | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    top_logprobs:Optional[int] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    top_p:Optional[float] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    truncation:Optional[Literal['auto', 'disabled']] | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    user:str | Omit=&lt;openai.Omit object at 0x7f338c0fc830&gt;,\n    extra_headers:Headers | None=None, # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\nThe extra values given here take precedence over values defined on the client or passed to this method.\n    extra_query:Query | None=None, extra_body:Body | None=None,\n    timeout:float | httpx.Timeout | None | NotGiven=NOT_GIVEN\n):\n\nAdd prompt pr to dialog and get a response\n\n\nExported source\n@patch\n@delegates(Responses.create)\ndef __call__(self:Chat,\n             pr=None,  # Prompt / message\n             stream:bool=False, # Stream response?\n             tools=None, # Tools to use\n             tool_choice=None, # Required tools to use\n             **kwargs):\n    \"Add prompt `pr` to dialog and get a response\"\n    if isinstance(pr,str): pr = pr.strip()\n    if pr: self.h.append(mk_msg(pr))\n    if not tools: tools = self.tools\n    if not tool_choice: tool_choice = self.tool_choice\n    kw = self.kw | kwargs\n    def _cb(v):\n        self.last = mk_toolres(v, ns=limit_ns(self.ns, self.tools, tool_choice))\n        self.h += self.last\n    res = self.c(self.h, sp=self.sp, stream=stream, cb=_cb, tools=tools, **kw)\n    return res\n\n\n\nchat(\"I'm Jeremy\")\nchat(\"What's my name?\")\n\nYou said your name is Jeremy.\n\n\nid: resp_0fad0aabf9d158d7006943fc124d84819785a5290dc6c01b91\ncreated_at: 1766063122.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don’t use tools unless needed for the provided prompt.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0fad0aabf9d158d7006943fc12f2b48197a746f0750976bf02’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0fad0aabf9d158d7006943fc131c8081979f047c455a339294’, content=[ResponseOutputText(annotations=[], text=‘You said your name is Jeremy.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=64, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=13, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=77)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True\n\n\n\n\n\nchat = Chat(model, sp=sysp, **rkw)\nfor o in chat(\"I'm Jeremy\", stream=True): print(o, end='')\n\nHi Jeremy — nice to meet you. How can I help today?\n\n\n\nr = chat(\"What's my name?\", stream=True, **rkw)\nfor o in r: print(o, end='')\n\nYou told me your name is Jeremy.\n\n\n\nr.value\n\nYou told me your name is Jeremy.\n\n\nid: resp_0543b2d5231a1965006943fc14ef008190aaec7f2241f09349\ncreated_at: 1766063124.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don’t use tools unless needed for the provided prompt.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_0543b2d5231a1965006943fc153b188190b243d5ebcad39ee6’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_0543b2d5231a1965006943fc1564bc81909b4a13499aca14fe’, content=[ResponseOutputText(annotations=[], text=‘You told me your name is Jeremy.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: []\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=68, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=14, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=82)\nuser: None\nstore: True\n\n\n\n\nHistory is stored in the h attr:\n\nchat.h\n\n[{'role': 'user', 'content': \"I'm Jeremy\"},\n ResponseReasoningItem(id='rs_0543b2d5231a1965006943fc143c408190832fd504f8544e47', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseOutputMessage(id='msg_0543b2d5231a1965006943fc147de8819096cf364e6b837d18', content=[ResponseOutputText(annotations=[], text='Hi Jeremy — nice to meet you. How can I help today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message'),\n {'role': 'user', 'content': \"What's my name?\"},\n ResponseReasoningItem(id='rs_0543b2d5231a1965006943fc153b188190b243d5ebcad39ee6', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseOutputMessage(id='msg_0543b2d5231a1965006943fc1564bc81909b4a13499aca14fe', content=[ResponseOutputText(annotations=[], text='You told me your name is Jeremy.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n\n\n\n\nChat tool use\n\npr = f\"What is {a}+{b}?\"\npr\n\n'What is 604542+6458932?'\n\n\n\nchat = Chat(model, sp=sysp, tools=[sums], **rkw)\nr = chat(pr)\nr.output\n\n[ResponseReasoningItem(id='rs_05f8244e8d805b77006943fc16df2c819581d1abf604e76779', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseOutputMessage(id='msg_05f8244e8d805b77006943fc170a848195a62a36f7b5af8774', content=[ResponseOutputText(annotations=[], text='7063474', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]\n\n\n\nchat()\n\nFinding the sum of 604542 and 6458932\n\n\n\nid: resp_05f8244e8d805b77006943fc1815d88195966a897b6bf45f32\ncreated_at: 1766063128.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don’t use tools unless needed for the provided prompt.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseFunctionToolCall(arguments=‘{“a”:604542,“b”:6458932}’, call_id=‘call_xgNdloSrer0Bze0O7MtyYjyZ’, name=‘sums’, type=‘function_call’, id=‘fc_05f8244e8d805b77006943fc187a9481958e2ec6c71d8c7a4c’, status=‘completed’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘sums’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘integer’, ‘description’: ‘First thing to sum’}, ‘b’: {‘type’: ‘integer’, ‘description’: ‘Second thing to sum’}}, ‘required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Adds a + b.:- type: integer’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=127, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=25, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=152)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True\n\n\n\nThe Chat class automatically validates tool calls against the provided tools list. If the model attempts to call a tool that isn’t in the allowed set (whether due to hallucination or a mismatch between tools and ns), the tool call will fail with an error message rather than executing arbitrary code.\nThis provides an important safety mechanism - even if the model invents a function name or tries to call a tool that shouldn’t be available, Chat ensures only explicitly allowed tools can be executed.\n\nchat = Chat(model, sp=sysp, tools=[sums, add], **rkw)\nchat.ns={} # Quick way to simulate call to tool that does not exist in ns or tools\nr = chat(pr)\nr.output\n\n[ResponseReasoningItem(id='rs_07858df65bb0139a006943fc1979b48194bbeb443a6dc2345c', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_3ilZzNeuNYVwm3NoqKV5DATp', name='sums', type='function_call', id='fc_07858df65bb0139a006943fc19bd788194a11707b54eb3eb61', status='completed')]\n\n\n\nchat.h\n\n[{'role': 'user', 'content': 'What is 604542+6458932?'},\n ResponseReasoningItem(id='rs_07858df65bb0139a006943fc1979b48194bbeb443a6dc2345c', summary=[], type='reasoning', content=None, encrypted_content=None, status=None),\n ResponseFunctionToolCall(arguments='{\"a\":604542,\"b\":6458932}', call_id='call_3ilZzNeuNYVwm3NoqKV5DATp', name='sums', type='function_call', id='fc_07858df65bb0139a006943fc19bd788194a11707b54eb3eb61', status='completed'),\n {'type': 'function_call_output',\n  'call_id': 'call_3ilZzNeuNYVwm3NoqKV5DATp',\n  'output': 'Error - tool not defined in the tool_schemas: sums'}]\n\n\nChat handles image prompts too.\n\nq = \"In brief, what color flowers are in this image?\"\nchat([img, q])\n\nThe flowers are purple.\n\n\nid: resp_07858df65bb0139a006943fc1a3a18819486051bea70b615a4\ncreated_at: 1766063130.0\nerror: None\nincomplete_details: None\ninstructions: You are a helpful assistant. When using tools, be sure to pass all required parameters. Don’t use tools unless needed for the provided prompt.\nmetadata: {}\nmodel: gpt-5-mini-2025-08-07\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_07858df65bb0139a006943fc1aa2b08194b7f22154fb9c9712’, summary=[], type=‘reasoning’, content=None, encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_07858df65bb0139a006943fc1acef88194884b2b85ff6d5aba’, content=[ResponseOutputText(annotations=[], text=‘The flowers are purple.’, type=‘output_text’, logprobs=[])], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: 1.0\ntool_choice: auto\ntools: [FunctionTool(name=‘sums’, parameters={‘type’: ‘object’, ‘properties’: {‘a’: {‘type’: ‘integer’, ‘description’: ‘First thing to sum’}, ‘b’: {‘type’: ‘integer’, ‘description’: ‘Second thing to sum’}}, ‘required’: [‘a’, ‘b’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘Adds a + b.:- type: integer’), FunctionTool(name=‘add’, parameters={‘type’: ‘object’, ‘properties’: {‘x’: {‘type’: ‘integer’, ‘description’: ’‘}, ’y’: {‘type’: ‘integer’, ‘description’: ’‘}}, ’required’: [‘x’, ‘y’], ‘additionalProperties’: False}, strict=True, type=‘function’, description=‘adds x and y’)]\ntop_p: 1.0\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: Reasoning(effort=‘minimal’, generate_summary=None, summary=None)\nsafety_identifier: None\nservice_tier: default\nstatus: completed\ntext: ResponseTextConfig(format=ResponseFormatText(type=‘text’), verbosity=‘low’)\ntop_logprobs: 0\ntruncation: disabled\nusage: ResponseUsage(input_tokens=277, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=11, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=288)\nuser: None\nbilling: {‘payer’: ‘openai’}\nstore: True",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#third-party-providers",
    "href": "core.html#third-party-providers",
    "title": "Cosette’s source",
    "section": "Third Party Providers",
    "text": "Third Party Providers\n\nAzure OpenAI Service\nExample Azure usage:\nazure_endpoint = AzureOpenAI(\n  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n  api_version=\"2024-08-01-preview\"\n)\n\nclient = Client(models_azure[0], azure_endpoint)\nchat = Chat(cli=client)\nchat(\"Hi.\")",
    "crumbs": [
      "Cosette's source"
    ]
  },
  {
    "objectID": "core.html#other-providers",
    "href": "core.html#other-providers",
    "title": "Cosette’s source",
    "section": "Other providers",
    "text": "Other providers\nHere’s an example of using the library with OpenRouter:\n\nopenrouter_c = Client(\n    model=\"openai/gpt-oss-20b\",\n    api_key_env=\"OPENROUTER_API_KEY\",\n    base_url=\"https://openrouter.ai/api/v1\"\n)\n\nopenrouter_c(\"Hello! What's 2+2?\")\n\n4\n\n\nid: gen-1766063131-zacuMF6yJHRUIPUXB48G\ncreated_at: 1766063131.0\nerror: None\nincomplete_details: None\ninstructions: None\nmetadata: {}\nmodel: openai/gpt-oss-20b\nobject: response\noutput: [ResponseReasoningItem(id=‘rs_tmp_u56eghazyi9’, summary=[], type=‘reasoning’, content=[Content(text=‘We need to answer: 2+2 = 4. Also maybe friendly.’, type=‘reasoning_text’)], encrypted_content=None, status=None), ResponseOutputMessage(id=‘msg_tmp_htb9r0aougc’, content=[ResponseOutputText(annotations=[], text=‘4’, type=‘output_text’, logprobs=None)], role=‘assistant’, status=‘completed’, type=‘message’)]\nparallel_tool_calls: True\ntemperature: None\ntool_choice: auto\ntools: []\ntop_p: None\nbackground: False\nconversation: None\nmax_output_tokens: 4096\nmax_tool_calls: None\nprevious_response_id: None\nprompt: None\nprompt_cache_key: None\nprompt_cache_retention: None\nreasoning: None\nsafety_identifier: None\nservice_tier: auto\nstatus: None\ntext: None\ntop_logprobs: None\ntruncation: None\nusage: ResponseUsage(input_tokens=75, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=28, output_tokens_details=OutputTokensDetails(reasoning_tokens=12), total_tokens=103, cost=9.35e-06, is_byok=False, cost_details={‘upstream_inference_cost’: None, ‘upstream_inference_input_cost’: 3.75e-06, ‘upstream_inference_output_cost’: 5.6e-06})\nuser: None\noutput_text:\nstore: False\n\n\n\n\nHere’s an example of using the library with Groq:\n\ngroq_c = Client(\n    model=\"openai/gpt-oss-20b\",\n    api_key_env=\"GROQ_KEY\",\n    base_url=\"https://api.groq.com/openai/v1\"\n)\n\ngroq_c(\"Hello! What's 2+2?\")\n\n\ngchat = Chat(cli=groq_c)\ngchat(\"Hello! I'm Jeremy\")\n\n\ngchat(\"What's my name?\")",
    "crumbs": [
      "Cosette's source"
    ]
  }
]